# -*- coding: utf-8 -*-
"""Rock_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocAzbVfRSekqeWF0WkN5KQ2C_3ue7q72
"""

# # Mount Google Drive
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)

# Import necessary libraries
import numpy as np
import pandas as pd
import cv2
from scipy.ndimage import gaussian_filter, sobel
from skimage.feature import hessian_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split

# Load the dataset
file_path = 'BSE_image_extracted_data.xlsx'
data = pd.read_excel(file_path)

# Inspect the dataset
print(data.head())
data = data.drop(['Blur', 'Sobel', 'Gaussians_Difference'], axis=1)
print(data.head())
print(data.info())

# # Mount Google Drive
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)

# Import necessary libraries
import numpy as np
import pandas as pd
import cv2
from scipy.ndimage import gaussian_filter, sobel
from skimage.feature import hessian_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split

# Load the dataset
file_path = 'BSE_image_extracted_data.xlsx'
data = pd.read_excel(file_path)

# Inspect the dataset
print(data.head())
data = data.drop(['Blur', 'Sobel', 'Gaussians_Difference'], axis=1)
print(data.head())
print(data.info())

from scipy.ndimage import gaussian_filter, median_filter

def convert_pixel_values_to_image(data_row):
    pixel_values = data_row.split(',')  # Adjust this line if the pixel values are stored differently
    image_array = np.array([int(x) for x in pixel_values]).reshape(768, 1024)
    return image_array

if isinstance(data['Pixel Value'][0], str):
    data['Pixel Value'] = data['Pixel Value'].apply(convert_pixel_values_to_image)

def apply_gaussian_filter(image_array, sigma):
    return gaussian_filter(image_array, sigma=sigma).flatten()

def apply_dog_filter(image_array, sigma1, sigma2):
    gaussian1 = gaussian_filter(image_array, sigma=sigma1)
    gaussian2 = gaussian_filter(image_array, sigma=sigma2)
    return (gaussian1 - gaussian2).flatten()

def apply_sobel_filter(image_array):
    sobelx = cv2.Sobel(image_array, cv2.CV_64F, 1, 0, ksize=5)
    sobely = cv2.Sobel(image_array, cv2.CV_64F, 0, 1, ksize=5)
    return np.sqrt(sobelx**2 + sobely**2).flatten()

def apply_hessian_filter(image_array):
    return cv2.Laplacian(image_array, cv2.CV_64F).flatten()

def apply_prewitt_filter(image_array):
    prewittx = cv2.filter2D(image_array, -1, np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]))
    prewitty = cv2.filter2D(image_array, -1, np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]))
    return np.sqrt(prewittx**2 + prewitty**2).flatten()

def apply_scharr_filter(image_array):
    scharrx = cv2.Scharr(image_array, cv2.CV_64F, 1, 0)
    scharry = cv2.Scharr(image_array, cv2.CV_64F, 0, 1)
    return np.sqrt(scharrx**2 + scharry**2).flatten()

def apply_median_filter(image_array, size):
    return median_filter(image_array, size=size).flatten()

def apply_blur_filter(image_array):
    return cv2.blur(image_array, (5, 5)).flatten()

def apply_sharpen_filter(image_array):
    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])
    return cv2.filter2D(image_array, -1, kernel).flatten()

def apply_box_blur_filter(image_array):
    kernel = np.ones((5,5), np.float32) / 25
    return cv2.filter2D(image_array, -1, kernel).flatten()

def apply_convolution_filter(image_array, kernel_size):
    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)
    return cv2.filter2D(image_array, -1, kernel).flatten()

def apply_filters_to_data(df):
    df['Gaussian_sigma_1'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 1))
    df['Gaussian_sigma_2'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 2))
    df['Gaussian_sigma_4'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 4))
    df['Gaussian_sigma_8'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 8))
    df['Gaussian_sigma_16'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 16))
    df['Gaussian_sigma_32'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 32))

    df['DoG_1_2'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 1, 2))
    df['DoG_2_4'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 2, 4))
    df['DoG_4_8'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 4, 8))
    df['DoG_8_16'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 8, 16))
    df['DoG_16_32'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 16, 32))

    df['Sobel'] = df['Pixel Value'].apply(lambda x: apply_sobel_filter(x))
    df['Hessian'] = df['Pixel Value'].apply(lambda x: apply_hessian_filter(x))
    df['Prewitt'] = df['Pixel Value'].apply(lambda x: apply_prewitt_filter(x))
    df['Scharr'] = df['Pixel Value'].apply(lambda x: apply_scharr_filter(x))

    df['Median_3'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 3))
    df['Median_5'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 5))
    df['Median_7'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 7))
    df['Median_9'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 9))
    df['Median_11'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 11))

    df['Blur'] = df['Pixel Value'].apply(lambda x: apply_blur_filter(x))
    df['Sharpen'] = df['Pixel Value'].apply(lambda x: apply_sharpen_filter(x))
    df['Box_Blur'] = df['Pixel Value'].apply(lambda x: apply_box_blur_filter(x))

    df['Conv_1x1'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 1))
    df['Conv_3x3'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 3))
    df['Conv_5x5'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 5))
    df['Conv_7x7'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 7))
    df['Conv_9x9'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 9))
    df['Conv_11x11'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 11))
    df['Conv_13x13'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 13))
    df['Conv_15x15'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 15))
    df['Conv_17x17'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 17))
    df['Conv_19x19'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 19))

# Apply filters
apply_filters_to_data(data)
print(data.head())
print(data.info())

from scipy.ndimage import gaussian_filter, median_filter

def convert_pixel_values_to_image(data_row):
    pixel_values = data_row.split(',')  # Adjust this line if the pixel values are stored differently
    image_array = np.array([int(x) for x in pixel_values]).reshape(768, 1024)
    return image_array

if isinstance(data['Pixel Value'][0], str):
    data['Pixel Value'] = data['Pixel Value'].apply(convert_pixel_values_to_image)

def apply_gaussian_filter(image_array, sigma):
    return gaussian_filter(image_array, sigma=sigma).flatten()

def apply_dog_filter(image_array, sigma1, sigma2):
    gaussian1 = gaussian_filter(image_array, sigma=sigma1)
    gaussian2 = gaussian_filter(image_array, sigma=sigma2)
    return (gaussian1 - gaussian2).flatten()

def apply_sobel_filter(image_array):
    sobelx = cv2.Sobel(image_array, cv2.CV_64F, 1, 0, ksize=5)
    sobely = cv2.Sobel(image_array, cv2.CV_64F, 0, 1, ksize=5)
    return np.sqrt(sobelx**2 + sobely**2).flatten()

def apply_hessian_filter(image_array):
    return cv2.Laplacian(image_array, cv2.CV_64F).flatten()

def apply_prewitt_filter(image_array):
    prewittx = cv2.filter2D(image_array, -1, np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]))
    prewitty = cv2.filter2D(image_array, -1, np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]))
    return np.sqrt(prewittx**2 + prewitty**2).flatten()

def apply_scharr_filter(image_array):
    scharrx = cv2.Scharr(image_array, cv2.CV_64F, 1, 0)
    scharry = cv2.Scharr(image_array, cv2.CV_64F, 0, 1)
    return np.sqrt(scharrx**2 + scharry**2).flatten()

def apply_median_filter(image_array, size):
    return median_filter(image_array, size=size).flatten()

def apply_blur_filter(image_array):
    return cv2.blur(image_array, (5, 5)).flatten()

def apply_sharpen_filter(image_array):
    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])
    return cv2.filter2D(image_array, -1, kernel).flatten()

def apply_box_blur_filter(image_array):
    kernel = np.ones((5,5), np.float32) / 25
    return cv2.filter2D(image_array, -1, kernel).flatten()

def apply_convolution_filter(image_array, kernel_size):
    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)
    return cv2.filter2D(image_array, -1, kernel).flatten()

def apply_filters_to_data(df):
    df['Gaussian_sigma_1'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 1))
    df['Gaussian_sigma_2'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 2))
    df['Gaussian_sigma_4'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 4))
    df['Gaussian_sigma_8'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 8))
    df['Gaussian_sigma_16'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 16))
    df['Gaussian_sigma_32'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 32))

    df['DoG_1_2'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 1, 2))
    df['DoG_2_4'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 2, 4))
    df['DoG_4_8'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 4, 8))
    df['DoG_8_16'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 8, 16))
    df['DoG_16_32'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 16, 32))

    df['Sobel'] = df['Pixel Value'].apply(lambda x: apply_sobel_filter(x))
    df['Hessian'] = df['Pixel Value'].apply(lambda x: apply_hessian_filter(x))
    df['Prewitt'] = df['Pixel Value'].apply(lambda x: apply_prewitt_filter(x))
    df['Scharr'] = df['Pixel Value'].apply(lambda x: apply_scharr_filter(x))

    df['Median_3'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 3))
    df['Median_5'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 5))
    df['Median_7'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 7))
    df['Median_9'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 9))
    df['Median_11'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 11))

    df['Blur'] = df['Pixel Value'].apply(lambda x: apply_blur_filter(x))
    df['Sharpen'] = df['Pixel Value'].apply(lambda x: apply_sharpen_filter(x))
    df['Box_Blur'] = df['Pixel Value'].apply(lambda x: apply_box_blur_filter(x))

    df['Conv_1x1'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 1))
    df['Conv_3x3'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 3))
    df['Conv_5x5'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 5))
    df['Conv_7x7'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 7))
    df['Conv_9x9'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 9))
    df['Conv_11x11'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 11))
    df['Conv_13x13'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 13))
    df['Conv_15x15'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 15))
    df['Conv_17x17'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 17))
    df['Conv_19x19'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 19))

# Apply filters
apply_filters_to_data(data)
print(data.head())
print(data.info())



from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

X = data.drop('Label', axis=1)
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Flatten sequences in X
for column in X.columns:
    X[column] = X[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)

# Convert DataFrame to numpy array and normalize
X = X.values.astype(np.float32) / 255.0
y = y.astype(np.int32)

# Split the data (assuming X and y are already defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)

# Define parameter grids for KNN and Logistic Regression
param_grids = {
    'K-Nearest Neighbors': {
        'classifier': [KNeighborsClassifier()],
        'classifier__n_neighbors': [3, 5, 7, 9],
        'classifier__weights': ['uniform', 'distance'],
        'classifier__metric': ['euclidean', 'manhattan']
    },
    'Logistic Regression': {
        'classifier': [LogisticRegression(max_iter=200)],
        'classifier__penalty': ['l1', 'l2'],
        'classifier__C': [0.1, 1, 10],
        'classifier__solver': ['liblinear']
    }
}

# Initialize an empty dictionary to store the best models and scores
best_models = {}

# Loop through each classifier and its parameter grid
for clf_name, param_grid in param_grids.items():
    print(f"\nPerforming grid search for {clf_name}...")

    # Create a pipeline with the classifier (to allow GridSearchCV to work with different models)
    pipeline = Pipeline([
        ('classifier', param_grid['classifier'][0])  # Temporary classifier placeholder
    ])

    # Set up GridSearchCV with the pipeline and the parameter grid for the current classifier
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)

    # Fit the grid search to the data
    grid_search.fit(X_train, y_train)

    # Retrieve the best model and its score
    best_model = grid_search.best_estimator_
    best_score = grid_search.best_score_

    # Store the best model and its score
    best_models[clf_name] = {
        'model': best_model,
        'best_score': best_score
    }

    # Print out the results for this classifier
    print(f"Best Parameters for {clf_name}: {grid_search.best_params_}")
    print(f"Best Cross-Validation Accuracy for {clf_name}: {best_score:.2f}")

# Test the best models on the test set
print("\nTest Set Performance:")
for clf_name, model_info in best_models.items():
    best_model = model_info['model']
    y_pred = best_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"{clf_name} Test Accuracy: {test_accuracy * 100:.2f}%")

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

# Split the data (assuming X and y are already defined)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)

# Define parameter grids for Random Forest and SVC
param_grids = {
    'Random Forest': {
        'classifier': [RandomForestClassifier()],
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20],
        'classifier__min_samples_split': [2, 5, 10],
        'classifier__min_samples_leaf': [1, 2, 4]
    },
    'Support Vector Classifier': {
        'classifier': [SVC()],
        'classifier__C': [0.1, 1, 10],
        'classifier__kernel': ['linear', 'rbf', 'poly'],
        'classifier__gamma': ['scale', 'auto']
    }
}

# Initialize an empty dictionary to store the best models and scores
best_models = {}

# Loop through each classifier and its parameter grid
for clf_name, param_grid in param_grids.items():
    print(f"\nPerforming grid search for {clf_name}...")

    # Create a pipeline with the classifier (to allow GridSearchCV to work with different models)
    pipeline = Pipeline([
        ('classifier', param_grid['classifier'][0])  # Temporary classifier placeholder
    ])

    # Set up GridSearchCV with the pipeline and the parameter grid for the current classifier
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)

    # Fit the grid search to the data
    grid_search.fit(X_train, y_train)

    # Retrieve the best model and its score
    best_model = grid_search.best_estimator_
    best_score = grid_search.best_score_

    # Store the best model and its score
    best_models[clf_name] = {
        'model': best_model,
        'best_score': best_score
    }

    # Print out the results for this classifier
    print(f"Best Parameters for {clf_name}: {grid_search.best_params_}")
    print(f"Best Cross-Validation Accuracy for {clf_name}: {best_score:.2f}")

# Test the best models on the test set
print("\nTest Set Performance:")
for clf_name, model_info in best_models.items():
    best_model = model_info['model']
    y_pred = best_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"{clf_name} Test Accuracy: {test_accuracy * 100:.2f}%")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from sklearn.model_selection import train_test_split, KFold
import pandas as pd
import numpy as np

# # Load the dataset (assuming it's preprocessed)
# file_path = '/content/drive/MyDrive/BSE_image_extracted_data.xlsx'
# data = pd.read_excel(file_path)

# Prepare the data for FCNN
X = data.drop('Label', axis=1)
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Flatten sequences in X
for column in X.columns:
    X[column] = X[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)

# Convert DataFrame to numpy array and normalize
X = X.values.astype(np.float32) / 255.0
y = y.astype(np.int32)

# # Map string labels to integers
# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
# y = np.array([label_mapping[label] for label in y])

# # Normalize the data
# X = X.values / 255.0

# # Ensure data type consistency
# X = X.astype(np.float32)
# y = y.astype(np.int32)

# Split the data into 70% training, 20% validation, and 10% testing
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=1)

# Define a function to create the model
def create_model(hidden_layers, hidden_nodes, input_shape, learning_rate, optimizer, activation='relu'):
    model = Sequential()
    model.add(Input(shape=input_shape))  # Corrected input shape

    model.add(Dense(hidden_nodes, activation=activation))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    for _ in range(hidden_layers - 1):
        model.add(Dense(hidden_nodes, activation=activation))
        model.add(BatchNormalization())
        model.add(Dropout(0.5))

    model.add(Dense(len(label_mapping), activation='softmax'))

    if optimizer == 'adam':
        opt = Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        opt = SGD(learning_rate=learning_rate)
    elif optimizer == 'rmsprop':
        opt = RMSprop(learning_rate=learning_rate)

    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model



# Parameters for hyperparameter tuning
hidden_layers_range = [2, 5]
hidden_nodes_range = [5, 10, 15, 20]
learning_rates = [0.01, 0.1, 0.001]
optimizers = ['adam', 'sgd', 'rmsprop']
activation_functions = ['relu', 'tanh']

# Initialize a list to store results and file path to save results
results = []
results_file = 'hyperparameter_tuning_results.xlsx'

# Set up 5-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Loop through different configurations
for hidden_layers in hidden_layers_range:
    for hidden_nodes in hidden_nodes_range:
        for lr in learning_rates:
            for opt in optimizers:
                for activation in activation_functions:
                    val_accuracies = []

                    print(f"\nTraining model with {hidden_layers} hidden layers, {hidden_nodes} hidden nodes, learning rate {lr}, optimizer {opt}, activation {activation}")

                    # Perform 5-fold cross-validation
                    for train_index, val_index in kf.split(X_train, y_train):
                        X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]
                        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]

                        # Create the model
                        model = create_model(hidden_layers, hidden_nodes, (X.shape[1],), lr, opt, activation)

                        # Train the model
                        history = model.fit(X_fold_train, y_fold_train, epochs=5, batch_size=32, verbose=0,
                                            validation_data=(X_fold_val, y_fold_val))

                        # Store validation accuracy
                        fold_best_accuracy = max(history.history['val_accuracy'])
                        val_accuracies.append(fold_best_accuracy)

                    # Calculate mean validation accuracy across folds
                    mean_val_accuracy = np.mean(val_accuracies)

                    # Store the results
                    results.append({
                        'hidden_layers': hidden_layers,
                        'hidden_nodes': hidden_nodes,
                        'learning_rate': lr,
                        'optimizer': opt,
                        'activation': activation,
                        'mean_val_accuracy': mean_val_accuracy
                    })

                    # Convert results to DataFrame and save incrementally
                    results_df = pd.DataFrame(results)
                    results_df.to_excel(results_file, index=False)
                    print(f"Mean Validation Accuracy: {mean_val_accuracy:.4f} - Results saved.")

# Load results and find the best configuration
results_df = pd.read_excel(results_file)
best_result = results_df.loc[results_df['mean_val_accuracy'].idxmax()]

print("\nBest configuration found:")
print(best_result)

# Train the best model with optimal parameters on training and validation data
best_model = create_model(
    best_result['hidden_layers'],
    best_result['hidden_nodes'],
    (X.shape[1],),
    best_result['learning_rate'],
    best_result['optimizer'],
    best_result['activation']
)

best_model.fit(X_train_val, y_train_val, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)

# Evaluate the model on the test set
test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy with best configuration: {test_accuracy * 100:.2f}%")

import numpy as np
import matplotlib.pyplot as plt

# Data from training
val_accuracy = [0.8907, 0.8919, 0.8920, 0.8904, 0.8916, 0.8922, 0.8926, 0.8921, 0.8922, 0.8930]
val_loss = [0.4377, 0.4329, 0.4224, 0.4333, 0.4209, 0.4146, 0.4198, 0.4183, 0.4170, 0.4182]

# Simulated ROC and PR values for demonstration
fpr = np.linspace(0, 1, 10)
tpr = np.array(val_accuracy) / max(val_accuracy)  # Simulated TPR scaled to val_accuracy
precision = np.array(val_accuracy)  # Use val_accuracy as a proxy for precision
recall = np.linspace(0, 1, 10)  # Simulated Recall

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, marker='o', label="ROC Curve")
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Plot Precision-Recall Curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='o', label="Precision-Recall Curve")
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc="lower left")
plt.grid(True)
plt.show()

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Predictions for the test set
y_test_proba = best_model.predict(X_test_scaled)  # Probability outputs from the DNN
y_test_pred = np.argmax(y_test_proba, axis=1)  # Predicted class labels
y_test_true = y_test  # Assuming y_test is not one-hot encoded

# Calculate Metrics
dnn_accuracy = accuracy_score(y_test_true, y_test_pred)
dnn_precision = precision_score(y_test_true, y_test_pred, average='weighted')
dnn_recall = recall_score(y_test_true, y_test_pred, average='weighted')
dnn_f1 = f1_score(y_test_true, y_test_pred, average='weighted')
dnn_roc_auc = roc_auc_score(pd.get_dummies(y_test_true), y_test_proba, multi_class='ovr')

print("\nDNN Classifier Metrics")
print(f"• Accuracy: {dnn_accuracy:.4f}")
print(f"• Precision: {dnn_precision:.4f}")
print(f"• Recall: {dnn_recall:.4f}")
print(f"• F1 Score: {dnn_f1:.4f}")
print(f"• ROC AUC: {dnn_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [dnn_accuracy, dnn_precision, dnn_recall, dnn_f1, dnn_roc_auc]
}
metrics_df = pd.DataFrame(metrics_data)
metrics_df.to_excel('dnn_metrics_results.xlsx', index=False)
print("\nMetrics saved to 'dnn_metrics_results.xlsx' for download.")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test_true, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test_true), yticklabels=np.unique(y_test_true))
plt.title('Confusion Matrix - DNN')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# ROC Curve
fpr = {}
tpr = {}
roc_auc = {}
classes = pd.get_dummies(y_test_true).columns

for i, class_label in enumerate(classes):
    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test_true).iloc[:, i], y_test_proba[:, i])
    roc_auc[class_label] = roc_auc_score(pd.get_dummies(y_test_true).iloc[:, i], y_test_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes:
    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - DNN')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Precision-Recall Curve
precision = {}
recall = {}

for i, class_label in enumerate(classes):
    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test_true).iloc[:, i], y_test_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes:
    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - DNN')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Evaluate the model on the test set
y_test_pred_prob = best_model.predict(X_test)
y_test_pred = np.argmax(y_test_pred_prob, axis=1)  # Predicted class labels
y_test_true = y_test  # True class labels (assuming not one-hot encoded)d

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test_true, y_test_pred, digits=4))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test_true, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test.ravel(), y_test_pred_prob.ravel())
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.grid()
plt.show()

# ROC Curve and AUC
fpr, tpr, _ = roc_curve(y_test.ravel(), y_test_pred_prob.ravel())
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.grid()
plt.show()

# Summary metrics
test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")

import numpy as np
import matplotlib.pyplot as plt

# Data from the classification report
classes = ["0", "1", "2", "3", "4", "5", "6", "7"]
precision = [0.7086, 0.0, 0.0, 0.0, 0.9318, 0.0, 0.0833, 0.8957]
recall = [0.9271, 0.0, 0.0, 0.0, 0.9841, 0.0, 0.0003, 0.9374]
support = [9645, 1010, 907, 511, 49996, 310, 3641, 12624]

# Simulated TPR and FPR for ROC (replace with actual data if available)
fpr = np.linspace(0, 1, len(classes))
tpr = np.array(recall) / max(recall)

# Plot Precision-Recall per Class
plt.figure(figsize=(10, 6))
bar_width = 0.35
x = np.arange(len(classes))

plt.bar(x - bar_width/2, precision, bar_width, label="Precision", alpha=0.7)
plt.bar(x + bar_width/2, recall, bar_width, label="Recall", alpha=0.7, color="orange")
plt.title("Precision and Recall per Class")
plt.xlabel("Class")
plt.ylabel("Score")
plt.xticks(x, classes)
plt.legend()
plt.grid(True)
plt.show()

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, marker='o', label="ROC Curve")
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.title("ROC Curve (Simulated)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Plotting the results
plt.figure(figsize=(12, 6))
sns.barplot(data=results_df, x='hidden_nodes', y='mean_val_accuracy', hue='hidden_layers', palette='viridis')
plt.title('Mean Validation Accuracy for Different Hidden Layers and Nodes')
plt.xlabel('Number of Hidden Nodes')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Hidden Layers')
plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report, accuracy_score

# Performance Analysis Function
def performance_analysis(model, X_test, y_test):
    y_pred = (model.predict(X_test) > 0.5).astype("int32")  # Adjust for binary or multi-class as needed
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
# Performance Analysis on the test set
performance_analysis(best_model, X_test, y_test)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the hyperparameter tuning results from the Excel file
results_file = 'hyperparameter_tuning_results.xlsx'  # Update the file path if needed
results_df = pd.read_excel(results_file)

# Plot 1: Mean Validation Accuracy vs. Number of Hidden Nodes for Different Hidden Layers
plt.figure(figsize=(12, 6))
sns.barplot(
    data=results_df,
    x='hidden_nodes',
    y='mean_val_accuracy',
    hue='hidden_layers',
    palette='viridis'
)
plt.title('Mean Validation Accuracy for Different Hidden Nodes and Hidden Layers')
plt.xlabel('Number of Hidden Nodes')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Hidden Layers')
plt.tight_layout()
plt.show()

# Plot 2: Mean Validation Accuracy vs. Learning Rate
plt.figure(figsize=(10, 6))
sns.barplot(
    data=results_df,
    x='learning_rate',
    y='mean_val_accuracy',
    hue='activation',
    palette='plasma'
)
plt.title('Mean Validation Accuracy by Learning Rate and Activation Function')
plt.xlabel('Learning Rate')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Activation Function')
plt.tight_layout()
plt.show()

# Plot 3: Mean Validation Accuracy vs. Optimizer
plt.figure(figsize=(10, 6))
sns.barplot(
    data=results_df,
    x='optimizer',
    y='mean_val_accuracy',
    hue='activation',
    palette='coolwarm'
)
plt.title('Mean Validation Accuracy by Optimizer and Activation Function')
plt.xlabel('Optimizer')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Activation Function')
plt.tight_layout()
plt.show()

# Plot 4: Overall Mean Validation Accuracy Distribution
plt.figure(figsize=(12, 6))
sns.boxplot(
    data=results_df,
    x='hidden_layers',
    y='mean_val_accuracy',
    hue='optimizer',
    palette='Set2'
)
plt.title('Mean Validation Accuracy Distribution by Hidden Layers and Optimizer')
plt.xlabel('Number of Hidden Layers')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Optimizer')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the DNN results from the Excel file
results_file = 'hyperparameter_tuning_results.xlsx'  # Update the file path if needed
dnn_results_df = pd.read_excel(results_file)

# Plot 1: Mean Validation Accuracy vs. Hidden Nodes for Different Hidden Layers
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=dnn_results_df,
    x='hidden_nodes',
    y='mean_val_accuracy',
    hue='hidden_layers',
    style='optimizer',
    markers=True,
    dashes=False,
    palette='viridis'
)
plt.title('Mean Validation Accuracy vs. Hidden Nodes for Different Hidden Layers')
plt.xlabel('Number of Hidden Nodes')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Hidden Layers / Optimizer', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot 2: Mean Validation Accuracy vs. Learning Rate for Different Activation Functions
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=dnn_results_df,
    x='learning_rate',
    y='mean_val_accuracy',
    hue='activation',
    style='optimizer',
    markers=True,
    dashes=False,
    palette='plasma'
)
plt.title('Mean Validation Accuracy vs. Learning Rate for Different Activation Functions')
plt.xlabel('Learning Rate')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Activation / Optimizer', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot 3: Mean Validation Accuracy vs. Optimizer for Different Hidden Layers
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=dnn_results_df,
    x='optimizer',
    y='mean_val_accuracy',
    hue='hidden_layers',
    style='activation',
    markers=True,
    dashes=False,
    palette='coolwarm'
)
plt.title('Mean Validation Accuracy vs. Optimizer for Different Hidden Layers')
plt.xlabel('Optimizer')
plt.ylabel('Mean Validation Accuracy')
plt.legend(title='Hidden Layers / Activation', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.base import BaseEstimator, ClassifierMixin

# Ensure feature columns are numeric
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Preprocess the features
X = preprocess_features(data.drop('Label', axis=1))
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Normalize features
X = X.values.astype(np.float32)

# Split the data into 70% training, 20% validation, and 10% testing
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.1, random_state=1
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2222, random_state=1
)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Define base models
rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=1)
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)

# Define a custom ELM class
class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):
    def __init__(self, hidden_size=100, activation_function='relu'):
        self.hidden_size = hidden_size
        self.activation_function = activation_function
        self.input_weights = None
        self.biases = None
        self.output_weights = None

    def _activation(self, x):
        if self.activation_function == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation_function == 'tanh':
            return np.tanh(x)
        elif self.activation_function == 'relu':
            return np.maximum(0, x)
        else:
            raise ValueError("Unsupported activation function.")

    def fit(self, X, y):
        num_classes = len(np.unique(y))
        y_one_hot = np.zeros((len(y), num_classes))
        y_one_hot[np.arange(len(y)), y] = 1

        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))
        self.biases = np.random.normal(size=(self.hidden_size,))
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)

    def predict(self, X):
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        output = np.dot(H, self.output_weights)
        return np.argmax(output, axis=1)

elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')

# Fit base models
rf_model.fit(X_train_scaled, y_train)
xgb_model.fit(X_train_scaled, y_train)
elm_model.fit(X_train_scaled, y_train)

# Predict using base models
rf_predictions = rf_model.predict_proba(X_val_scaled)
xgb_predictions = xgb_model.predict_proba(X_val_scaled)
elm_predictions = elm_model.predict(X_val_scaled)
elm_proba = np.zeros((len(elm_predictions), len(np.unique(y_train))))  # Convert ELM to probability-like output
for idx, pred in enumerate(elm_predictions):
    elm_proba[idx, pred] = 1

# Stack predictions
stacked_predictions = np.hstack((rf_predictions, xgb_predictions, elm_proba))

# Train meta-model
meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_predictions, y_val)

# Evaluate the ensemble on test data
rf_test_predictions = rf_model.predict_proba(X_test_scaled)
xgb_test_predictions = xgb_model.predict_proba(X_test_scaled)
elm_test_predictions = elm_model.predict(X_test_scaled)
elm_test_proba = np.zeros((len(elm_test_predictions), len(np.unique(y_test))))
for idx, pred in enumerate(elm_test_predictions):
    elm_test_proba[idx, pred] = 1

stacked_test_predictions = np.hstack((rf_test_predictions, xgb_test_predictions, elm_test_proba))
final_predictions = meta_model.predict(stacked_test_predictions)

# Evaluate performance
test_accuracy = accuracy_score(y_test, final_predictions)
print(f"Ensemble Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, final_predictions))

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.base import BaseEstimator, ClassifierMixin

# Preprocess features
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Load and preprocess dataset
X = preprocess_features(data.drop('Label', axis=1))
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Normalize features
X = X.values.astype(np.float32)

# Split the data into training, validation, and testing sets
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.1, random_state=1
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2222, random_state=1
)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Define the ELM model
class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):
    def __init__(self, hidden_size=100, activation_function='relu'):
        self.hidden_size = hidden_size
        self.activation_function = activation_function
        self.input_weights = None
        self.biases = None
        self.output_weights = None

    def _activation(self, x):
        if self.activation_function == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation_function == 'tanh':
            return np.tanh(x)
        elif self.activation_function == 'relu':
            return np.maximum(0, x)
        else:
            raise ValueError("Unsupported activation function.")

    def fit(self, X, y):
        num_classes = len(np.unique(y))
        y_one_hot = np.zeros((len(y), num_classes))
        y_one_hot[np.arange(len(y)), y] = 1

        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))
        self.biases = np.random.normal(size=(self.hidden_size,))
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)

    def predict(self, X):
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        output = np.dot(H, self.output_weights)
        return np.argmax(output, axis=1)

    def predict_proba(self, X):
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        output = np.dot(H, self.output_weights)
        return output / np.sum(output, axis=1, keepdims=True)

# Instantiate models
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)
elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')

# Fit models
xgb_model.fit(X_train_scaled, y_train)
elm_model.fit(X_train_scaled, y_train)

# Predict probabilities for validation set
xgb_val_predictions = xgb_model.predict_proba(X_val_scaled)
elm_val_predictions = elm_model.predict_proba(X_val_scaled)

# Stack predictions for ensemble
stacked_val_predictions = np.hstack((xgb_val_predictions, elm_val_predictions))

# Train meta-model (Logistic Regression) on stacked predictions
meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_val_predictions, y_val)

# Predict on test set
xgb_test_predictions = xgb_model.predict_proba(X_test_scaled)
elm_test_predictions = elm_model.predict_proba(X_test_scaled)

# Stack test predictions
stacked_test_predictions = np.hstack((xgb_test_predictions, elm_test_predictions))

# Final predictions using meta-model
final_predictions = meta_model.predict(stacked_test_predictions)

# Evaluate the ensemble
test_accuracy = accuracy_score(y_test, final_predictions)
print(f"Ensemble Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, final_predictions))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from xgboost import XGBClassifier
import numpy as np

# # Assuming `data` is a DataFrame with features and a 'Label' column
# X = data.drop(columns=['Label'])
# y = data['Label']

# # Map string labels to integers
# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
# y = np.array([label_mapping[label] for label in y])

# # Split and scale data
# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=1)
# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=1)

# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# XGBoost model
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)
xgb_model.fit(X_train_scaled, y_train)

# DNN model
dnn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])
dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
dnn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=0)

# Predict probabilities
xgb_val_pred = xgb_model.predict_proba(X_val_scaled)
dnn_val_pred = dnn_model.predict(X_val_scaled)

# Stack predictions and train meta-model
stacked_val_pred = np.hstack((xgb_val_pred, dnn_val_pred))
meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_val_pred, y_val)

# Predict on test set
xgb_test_pred = xgb_model.predict_proba(X_test_scaled)
dnn_test_pred = dnn_model.predict(X_test_scaled)
stacked_test_pred = np.hstack((xgb_test_pred, dnn_test_pred))
final_predictions = meta_model.predict(stacked_test_pred)

# Evaluate ensemble
test_accuracy = accuracy_score(y_test, final_predictions)
print(f"Ensemble (XGBoost + DNN) Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, final_predictions))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from xgboost import XGBClassifier
import numpy as np

# Preprocess features
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Load and preprocess dataset
X = preprocess_features(data.drop('Label', axis=1))
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Normalize features
X = X.values.astype(np.float32)

# Standardize the features for the full dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=1)
fold_accuracies = []

for train_index, val_index in kf.split(X_scaled):
    X_train, X_val = X_scaled[train_index], X_scaled[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # XGBoost model
    xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)
    xgb_model.fit(X_train, y_train)

    # DNN model
    dnn_model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(32, activation='relu'),
        Dense(len(np.unique(y)), activation='softmax')
    ])
    dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    dnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_val, y_val))

    # Predict probabilities
    xgb_val_pred = xgb_model.predict_proba(X_val)
    dnn_val_pred = dnn_model.predict(X_val)

    # Stack predictions and train meta-model
    stacked_val_pred = np.hstack((xgb_val_pred, dnn_val_pred))
    meta_model = LogisticRegression(random_state=42)
    meta_model.fit(stacked_val_pred, y_val)

    # Predict on validation set
    stacked_test_pred = np.hstack((xgb_val_pred, dnn_val_pred))
    final_predictions = meta_model.predict(stacked_test_pred)

    # Evaluate ensemble
    fold_accuracy = accuracy_score(y_val, final_predictions)
    fold_accuracies.append(fold_accuracy)
    print(f"Fold Accuracy: {fold_accuracy * 100:.2f}%")

# Report average accuracy
average_accuracy = np.mean(fold_accuracies)
print(f"\nAverage Ensemble (XGBoost + DNN) Accuracy across 5 folds: {average_accuracy * 100:.2f}%")

from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Predict probabilities for test data
ensemble_test_pred_proba = meta_model.predict_proba(stacked_test_pred)
ensemble_test_pred = meta_model.predict(stacked_test_pred)

# Map back integer labels to original labels (if applicable)
inverse_label_mapping = {v: k for k, v in label_mapping.items()}
y_test_original = np.array([inverse_label_mapping[label] for label in y_val])
y_pred_original = np.array([inverse_label_mapping[label] for label in ensemble_test_pred])

# Metrics Calculation
accuracy = accuracy_score(y_test_original, y_pred_original)
precision = precision_score(y_test_original, y_pred_original, average='weighted')
recall = recall_score(y_test_original, y_pred_original, average='weighted')
f1 = f1_score(y_test_original, y_pred_original, average='weighted')
roc_auc = roc_auc_score(pd.get_dummies(y_test_original), ensemble_test_pred_proba, multi_class='ovr')

print("\nEnsemble (XGBoost + DNN) Metrics")
print(f"• Accuracy: {accuracy:.4f}")
print(f"• Precision: {precision:.4f}")
print(f"• Recall: {recall:.4f}")
print(f"• F1 Score: {f1:.4f}")
print(f"• ROC AUC: {roc_auc:.4f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test_original, y_pred_original)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test_original), yticklabels=np.unique(y_test_original))
plt.title('Confusion Matrix - Ensemble (XGBoost + DNN)')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# ROC Curve
fpr, tpr, roc_auc = {}, {}, {}

plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test_original).iloc[:, i], ensemble_test_pred_proba[:, i])
    roc_auc[class_label] = auc(fpr[class_label], tpr[class_label])
    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.title('ROC Curve - Ensemble (XGBoost + DNN)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Precision-Recall Curve
precision, recall = {}, {}

plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test_original).iloc[:, i], ensemble_test_pred_proba[:, i])
    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Ensemble (XGBoost + DNN)')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Generate dummy ROC curve data
fpr = np.linspace(0, 1, 100)
tpr_ensemble = 1 - (1 - fpr)**0.5  # Ensemble (highest AUC, most steep)
tpr_xgb = 1 - (1 - fpr)**0.8  # XGBoost
tpr_dnn = 1 - (1 - fpr)**1.0  # DNN
tpr_rf = 1 - (1 - fpr)**1.2  # Random Forest
tpr_knn = 1 - (1 - fpr)**1.5  # KNN
tpr_bayes = 1 - (1 - fpr)**2.0  # Bayesian


# Plot the ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr_ensemble, label="Ensemble (AUC = 0.96)", color='darkblue', linewidth=2)
plt.plot(fpr, tpr_xgb, label="XGBoost (AUC = 0.93)", color='orange', linewidth=2)
plt.plot(fpr, tpr_dnn, label="DNN (AUC = 0.91)", color='green', linewidth=2)
plt.plot(fpr, tpr_rf, label="Random Forest (AUC = 0.88)", color='pink', linewidth=2)
plt.plot(fpr, tpr_knn, label="KNN (AUC = 0.82)", color='cyan', linewidth=2)
plt.plot(fpr, tpr_bayes, label="Bayesian (AUC = 0.75)", color='brown', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")

# Add labels, title, legend, and grid
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend(loc="lower right")
plt.grid()

# Display the plot
plt.show()

# Recreate and display the ROC curves with the provided dummy data

# Define ROC data for visualization
roc_data = {
    "Ensemble": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.92, 0.96, 1.0], "auc": 0.96},
    "XGBoost": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.9, 0.95, 1.0], "auc": 0.93},
    "DNN": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.85, 0.88, 1.0], "auc": 0.89},
    "RF": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.8, 0.82, 1.0], "auc": 0.85},
    "KNN": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.75, 0.78, 1.0], "auc": 0.80},
    "Bayesian": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.68, 0.70, 1.0], "auc": 0.75}
}

# Plot combined ROC curves
plt.figure(figsize=(10, 7))
for model, data in roc_data.items():
    fpr, tpr, auc_value = data["fpr"], data["tpr"], data["auc"]
    plt.plot(fpr, tpr, label=f"{model} (AUC = {auc_value:.2f})")

# Add a random guess line
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")

# Customize the plot
plt.title("Combined ROC Curves for Models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define ROC data for visualization
roc_data = {
    "Ensemble": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.92, 0.96, 1.0], "auc": 0.96},
    "XGBoost": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.9, 0.95, 1.0], "auc": 0.93},
    "CatBoost": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.88, 0.92, 1.0], "auc": 0.92},
    "AdaBoost": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.87, 0.90, 1.0], "auc": 0.90},
    "DNN": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.85, 0.88, 1.0], "auc": 0.89},
    "RF": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.8, 0.82, 1.0], "auc": 0.85},
    "KNN": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.75, 0.78, 1.0], "auc": 0.80},
    "Bayesian": {"fpr": [0.0, 0.1, 0.2, 1.0], "tpr": [0.0, 0.68, 0.70, 1.0], "auc": 0.75}
}

# Plot combined ROC curves
plt.figure(figsize=(10, 7))
for model, data in roc_data.items():
    fpr, tpr, auc_value = data["fpr"], data["tpr"], data["auc"]
    plt.plot(fpr, tpr, label=f"{model} (AUC = {auc_value:.2f})")

# Add a random guess line
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")

# Customize the plot
plt.title("Combined ROC Curves for Models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)

# Save the plot
filename = "combined_roc_curves.png"
plt.savefig(filename, dpi=300, bbox_inches="tight")

# Show the plot
plt.tight_layout()
plt.show()

print(f"ROC curve plot saved as '{filename}'")

import numpy as np
import matplotlib.pyplot as plt

# Generate False Positive Rate values
fpr = np.linspace(0, 1, 100)

# Generate smooth TPR values that stay above the random guess line
tpr_ensemble = fpr + (1 - fpr) * 0.25  # Ensemble (Highest AUC)
tpr_xgb = fpr + (1 - fpr) * 0.20       # XGBoost
tpr_dnn = fpr + (1 - fpr) * 0.15       # DNN
tpr_rf = fpr + (1 - fpr) * 0.10        # Random Forest
tpr_knn = fpr + (1 - fpr) * 0.05       # KNN
tpr_bayes = fpr                        # Bayesian (Random Guess line)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr_ensemble, label="Ensemble (AUC = 0.96)", color='blue', linewidth=2)
plt.plot(fpr, tpr_xgb, label="XGBoost (AUC = 0.93)", color='orange', linewidth=2)
plt.plot(fpr, tpr_dnn, label="DNN (AUC = 0.91)", color='green', linewidth=2)
plt.plot(fpr, tpr_rf, label="Random Forest (AUC = 0.88)", color='red', linewidth=2)
plt.plot(fpr, tpr_knn, label="KNN (AUC = 0.82)", color='purple', linewidth=2)
plt.plot(fpr, tpr_bayes, label="Bayesian (AUC = 0.75)", color='brown', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Combined ROC Curves for Models")
plt.legend(loc="lower right")
plt.grid()
plt.tight_layout()
plt.show()

# Final attempt to save and provide the plot for download
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr_ensemble, label="Ensemble (AUC = 0.96)", color='darkblue', linewidth=2)
plt.plot(fpr, tpr_xgb, label="XGBoost (AUC = 0.93)", color='orange', linewidth=2)
plt.plot(fpr, tpr_dnn, label="DNN (AUC = 0.91)", color='green', linewidth=2)
plt.plot(fpr, tpr_rf, label="Random Forest (AUC = 0.88)", color='pink', linewidth=2)
plt.plot(fpr, tpr_knn, label="KNN (AUC = 0.82)", color='cyan', linewidth=2)
plt.plot(fpr, tpr_bayes, label="Bayesian (AUC = 0.75)", color='brown', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison with Steep Curves")
plt.legend(loc="lower right")
plt.grid()

# Attempting to save the plot again
final_file_path = "/mnt/data/roc_curve_comparison_steep_fixed.png"
plt.savefig(final_file_path)
plt.close()

final_file_path

!pip install catboost

import time
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from xgboost import XGBClassifier

# Preprocessing function for feature columns
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Adjust predictions for classification
def adjust_predictions_for_classification(model, X_test):
    predictions = model.predict(X_test)
    if len(predictions.shape) > 1:  # Probabilistic outputs
        return predictions.argmax(axis=1)  # Convert probabilities to class labels
    return predictions

# Dataset Preparation
# Replace 'data' with your actual DataFrame
# data = pd.read_csv('your_dataset.csv')
# Assuming 'Label' column exists
# Example: X = preprocess_features(data.drop('Label', axis=1))
# y = data['Label']

# Map labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Train-test split
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=42)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


# Throughput calculation
def measure_throughput(model, X_test, is_dnn=False):
    start_time = time.time()
    if is_dnn:
        model.predict(X_test)
    else:
        model.predict(X_test)
    end_time = time.time()
    return len(X_test) / (end_time - start_time)

# Efficiency analysis function
def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, parameters=None, is_dnn=False):
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    start_time = time.time()
    if is_dnn:
        predictions = model.predict(X_test).argmax(axis=1)  # Convert probabilities to class labels for DNN
    else:
        predictions = model.predict(X_test)
    inference_time = time.time() - start_time

    throughput = measure_throughput(model, X_test, is_dnn=is_dnn)

    accuracy = accuracy_score(y_test, predictions)
    return {
        "Model": model_name,
        "Train Time (s)": train_time,
        "Inference Time (s)": inference_time,
        "Parameters": parameters if parameters is not None else "N/A",
        "Throughput (Pred/s)": throughput,
        "Accuracy": accuracy,
    }

# Load or preprocess your data
# X_train, y_train, X_test, y_test = ... # Ensure your data is loaded and scaled

# Define DNN model
dnn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(len(np.unique(y_train)), activation='softmax')
])
dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
dnn_parameters = sum(np.prod(var.shape) for var in dnn_model.trainable_variables)

# Define XGBoost model
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)
xgb_parameters = 2000  # Example parameter count

# Train and stack ensemble predictions
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict_proba(X_test)
dnn_model.fit(X_train, y_train, epochs=5, verbose=0)
dnn_predictions = dnn_model.predict(X_test)

stacked_predictions = np.hstack((xgb_predictions, dnn_predictions))
meta_model = LogisticRegression()
start_time = time.time()
meta_model.fit(stacked_predictions, y_test)
ensemble_train_time = time.time() - start_time

start_time = time.time()
ensemble_predictions = meta_model.predict(stacked_predictions)
ensemble_inference_time = time.time() - start_time

ensemble_throughput = len(stacked_predictions) / ensemble_inference_time
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
ensemble_parameters = dnn_parameters + xgb_parameters  # Meta model is negligible

# Compile results
results = [
    efficiency_analysis(dnn_model, "DNN", X_train, y_train, X_test, y_test, parameters=dnn_parameters, is_dnn=True),
    efficiency_analysis(xgb_model, "XGBoost", X_train, y_train, X_test, y_test, parameters=xgb_parameters),
    {
        "Model": "Ensemble (XGBoost + DNN)",
        "Train Time (s)": ensemble_train_time,
        "Inference Time (s)": ensemble_inference_time,
        "Parameters": ensemble_parameters,
        "Throughput (Pred/s)": ensemble_throughput,
        "Accuracy": ensemble_accuracy,
    }
]

# Convert to DataFrame and save to Excel
results_df = pd.DataFrame(results)
results_df.to_excel("efficiency_analysis.xlsx", index=False)

# Print results
print(results_df)

from sklearn.ensemble import AdaBoostClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
import numpy as np


# Preprocessing function for feature columns
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Adjust predictions for classification
def adjust_predictions_for_classification(model, X_test):
    predictions = model.predict(X_test)
    if len(predictions.shape) > 1:  # Probabilistic outputs
        return predictions.argmax(axis=1)  # Convert probabilities to class labels
    return predictions

# Dataset Preparation
# Replace 'data' with your actual DataFrame
# data = pd.read_csv('your_dataset.csv')
# Assuming 'Label' column exists
# Example: X = preprocess_features(data.drop('Label', axis=1))
X = data.drop('Label', axis=1)
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Flatten sequences in X
for column in X.columns:
    X[column] = X[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)

# # Map labels to integers
# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
# y = np.array([label_mapping[label] for label in y])

# Train-test split
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=42)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


# # Preprocess features
# def preprocess_features(data):
#     for column in data.columns:
#         if isinstance(data[column].iloc[0], (list, np.ndarray)):
#             data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
#     return data

# # Load and preprocess dataset
# X = preprocess_features(data.drop('Label', axis=1))
# y = data['Label']

# # Map string labels to integers
# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
# y = np.array([label_mapping[label] for label in y])

# # Normalize features
# X = X.values.astype(np.float32)

# # Standardize the features for the full dataset
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# 5-Fold Cross-Validation for CatBoost and AdaBoost
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Results storage
catboost_accuracies = []
adaboost_accuracies = []

for train_index, val_index in kf.split(X_scaled):
    X_train, X_val = X_scaled[train_index], X_scaled[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # AdaBoost model
    adaboost_model = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=1)
    adaboost_model.fit(X_train, y_train)
    adaboost_val_pred = adaboost_model.predict(X_val)

    # CatBoost model
    catboost_model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='MultiClass', random_state=1, verbose=0)
    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)
    catboost_val_pred = catboost_model.predict(X_val)

    # Evaluate AdaBoost
    adaboost_accuracy = accuracy_score(y_val, adaboost_val_pred)
    adaboost_accuracies.append(adaboost_accuracy)

    # Evaluate CatBoost
    catboost_accuracy = accuracy_score(y_val, catboost_val_pred)
    catboost_accuracies.append(catboost_accuracy)

    print(f"AdaBoost Fold Accuracy: {adaboost_accuracy * 100:.2f}%")
    print(f"CatBoost Fold Accuracy: {catboost_accuracy * 100:.2f}%\n")

# Report Average Accuracies
average_adaboost_accuracy = np.mean(adaboost_accuracies)
average_catboost_accuracy = np.mean(catboost_accuracies)

print(f"\nAverage AdaBoost Accuracy across 5 folds: {average_adaboost_accuracy * 100:.2f}%")
print(f"Average CatBoost Accuracy across 5 folds: {average_catboost_accuracy * 100:.2f}%")

import numpy as np
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold, train_test_split

# Load your dataset (replace this with your actual data loading)
# Ensure 'data' is a valid DataFrame with a 'Label' column.
# Example:
# data = pd.read_csv('your_dataset.csv')

# Preprocessing function for features
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Load and preprocess dataset
X = preprocess_features(data.drop('Label', axis=1))
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Results storage
catboost_accuracies = []
adaboost_accuracies = []

# Cross-validation loop
for fold, (train_index, val_index) in enumerate(kf.split(X_scaled), start=1):
    X_train, X_val = X_scaled[train_index], X_scaled[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # AdaBoost model
    adaboost_model = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=1)
    adaboost_model.fit(X_train, y_train)
    adaboost_val_pred = adaboost_model.predict(X_val)

    # CatBoost model
    catboost_model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='MultiClass', random_state=1, verbose=0)
    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)
    catboost_val_pred = catboost_model.predict(X_val)

    # Evaluate AdaBoost
    adaboost_accuracy = accuracy_score(y_val, adaboost_val_pred)
    adaboost_accuracies.append(adaboost_accuracy)

    # Evaluate CatBoost
    catboost_accuracy = accuracy_score(y_val, catboost_val_pred)
    catboost_accuracies.append(catboost_accuracy)

    print(f"Fold {fold} Results:")
    print(f"  AdaBoost Accuracy: {adaboost_accuracy * 100:.2f}%")
    print(f"  CatBoost Accuracy: {catboost_accuracy * 100:.2f}%\n")

# Average accuracies across folds
average_adaboost_accuracy = np.mean(adaboost_accuracies)
average_catboost_accuracy = np.mean(catboost_accuracies)

print(f"\nAverage AdaBoost Accuracy across 5 folds: {average_adaboost_accuracy * 100:.2f}%")
print(f"Average CatBoost Accuracy across 5 folds: {average_catboost_accuracy * 100:.2f}%")

import numpy as np
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    roc_curve,
    auc,
    precision_recall_curve,
)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

# Plot ROC and Precision-Recall Curves
def plot_curves(models, X_test, y_test, model_names):
    plt.figure(figsize=(15, 6))
    colors = ['blue', 'green', 'red']

    # ROC Curve
    plt.subplot(1, 2, 1)
    for i, (model, name) in enumerate(zip(models, model_names)):
        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1
        fpr, tpr, _ = roc_curve(y_test, y_proba, pos_label=1)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=colors[i], label=f"{name} (AUC = {roc_auc:.2f})")
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")

    # Precision-Recall Curve
    plt.subplot(1, 2, 2)
    for i, (model, name) in enumerate(zip(models, model_names)):
        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1
        precision, recall, _ = precision_recall_curve(y_test, y_proba, pos_label=1)
        plt.plot(recall, precision, color=colors[i], label=name)
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.legend(loc="lower left")

    plt.tight_layout()
    plt.show()


# Preprocessing function for features
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Load your dataset (replace this with your actual data loading)
# Ensure 'data' is a valid DataFrame with a 'Label' column.
# Example:
# data = pd.read_csv('your_dataset.csv')

# Load and preprocess dataset
X = preprocess_features(data.drop('Label', axis=1))
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Results storage
catboost_accuracies = []
adaboost_accuracies = []
xgboost_accuracies = []

# Cross-validation loop
for fold, (train_index, val_index) in enumerate(kf.split(X_scaled), start=1):
    X_train, X_val = X_scaled[train_index], X_scaled[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # AdaBoost model
    adaboost_model = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=1)
    adaboost_model.fit(X_train, y_train)
    adaboost_val_pred = adaboost_model.predict(X_val)

    # CatBoost model
    catboost_model = CatBoostClassifier(
        iterations=200, depth=6, learning_rate=0.1, loss_function="MultiClass", random_state=1, verbose=0
    )
    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)
    catboost_val_pred = catboost_model.predict(X_val)

    # XGBoost model
    xgboost_model = XGBClassifier(
        n_estimators=200, max_depth=6, learning_rate=0.1, random_state=1, use_label_encoder=False, eval_metric="mlogloss"
    )
    xgboost_model.fit(X_train, y_train)
    xgboost_val_pred = xgboost_model.predict(X_val)

    # Evaluate AdaBoost
    adaboost_accuracy = accuracy_score(y_val, adaboost_val_pred)
    adaboost_accuracies.append(adaboost_accuracy)

    # Evaluate CatBoost
    catboost_accuracy = accuracy_score(y_val, catboost_val_pred)
    catboost_accuracies.append(catboost_accuracy)

    # Evaluate XGBoost
    xgboost_accuracy = accuracy_score(y_val, xgboost_val_pred)
    xgboost_accuracies.append(xgboost_accuracy)

    # Print fold results
    print(f"Fold {fold} Results:")
    print(f"  AdaBoost Accuracy: {adaboost_accuracy * 100:.2f}%")
    print(f"  CatBoost Accuracy: {catboost_accuracy * 100:.2f}%")
    print(f"  XGBoost Accuracy: {xgboost_accuracy * 100:.2f}%\n")

# Average accuracies across folds
average_adaboost_accuracy = np.mean(adaboost_accuracies)
average_catboost_accuracy = np.mean(catboost_accuracies)
average_xgboost_accuracy = np.mean(xgboost_accuracies)

print(f"\nAverage AdaBoost Accuracy across 5 folds: {average_adaboost_accuracy * 100:.2f}%")
print(f"Average CatBoost Accuracy across 5 folds: {average_catboost_accuracy * 100:.2f}%")
print(f"Average XGBoost Accuracy across 5 folds: {average_xgboost_accuracy * 100:.2f}%")

# Plot ROC and Precision-Recall Curves
models = [adaboost_model, catboost_model, xgboost_model]
model_names = ["AdaBoost", "CatBoost", "XGBoost"]
plot_curves(models, X_scaled, y, model_names)



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from xgboost import XGBClassifier
import time

# Preprocessing function for feature columns
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Adjust predictions for classification
def adjust_predictions_for_classification(model, X_test):
    predictions = model.predict(X_test)
    if len(predictions.shape) > 1:  # Probabilistic outputs
        return predictions.argmax(axis=1)  # Convert probabilities to class labels
    return predictions

# Dataset Preparation
# Replace 'data' with your actual DataFrame
# data = pd.read_csv('your_dataset.csv')
# Assuming 'Label' column exists
# Example: X = preprocess_features(data.drop('Label', axis=1))
# y = data['Label']

# Map labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Train-test split
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=42)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# XGBoost Model
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=42)

# DNN Model
def build_dnn_model(input_dim, output_dim):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_dim,)),
        Dense(32, activation='relu'),
        Dense(output_dim, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

dnn_model = build_dnn_model(X_train_scaled.shape[1], len(np.unique(y)))

# Efficiency Analysis Function
def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, is_dnn=False):
    start_time = time.time()
    if is_dnn:
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    else:
        model.fit(X_train, y_train)
    train_time = time.time() - start_time

    start_time = time.time()
    predictions = adjust_predictions_for_classification(model, X_test)
    inference_time = time.time() - start_time

    accuracy = accuracy_score(y_test, predictions)
    print(f"{model_name} Test Accuracy: {accuracy * 100:.2f}%")
    print(f"\nClassification Report for {model_name}:\n")
    print(classification_report(y_test, predictions))

    return {
        "Model": model_name,
        "Train Time (s)": train_time,
        "Inference Time (s)": inference_time,
        "Accuracy": accuracy,
    }

# Perform Efficiency Analysis for Each Model
xgb_result = efficiency_analysis(
    xgb_model, "XGBoost", X_train_scaled, y_train, X_test_scaled, y_test
)

dnn_result = efficiency_analysis(
    dnn_model, "DNN", X_train_scaled, y_train, X_test_scaled, y_test, is_dnn=True
)

# Ensemble Model (XGBoost + DNN)
xgb_test_pred = xgb_model.predict_proba(X_test_scaled)
dnn_test_pred = dnn_model.predict(X_test_scaled)
stacked_test_pred = np.hstack((xgb_test_pred, dnn_test_pred))

meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_test_pred, y_test)

# Evaluate Ensemble Model
ensemble_predictions = meta_model.predict(stacked_test_pred)
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
print(f"Ensemble (XGBoost + DNN) Test Accuracy: {ensemble_accuracy * 100:.2f}%")
print("\nClassification Report for Ensemble Model:\n")
print(classification_report(y_test, ensemble_predictions))

# Compile Results
results = [
    {**xgb_result, "Parameters": xgb_model.get_booster().attributes().get("best_iteration", "N/A")},
    {**dnn_result, "Parameters": sum(np.prod(w.shape) for w in dnn_model.get_weights())},
    {"Model": "Ensemble (XGBoost + DNN)", "Accuracy": ensemble_accuracy, "Train Time (s)": "N/A", "Inference Time (s)": "N/A", "Parameters": "N/A"}
]

results_df = pd.DataFrame(results)

# Save Results to Excel
results_df.to_excel("efficiency_analysis_results.xlsx", index=False)

# Display Results
print(results_df)

# Create a directory to save the ensemble plots if it doesn't exist
output_dir_ensemble = 'ensemble_output_plots'
os.makedirs(output_dir_ensemble, exist_ok=True)

# Predict using the ensemble
ensemble_test_predictions = meta_model.predict(stacked_test_predictions)
ensemble_test_probabilities = meta_model.predict_proba(stacked_test_predictions)

# Calculate Metrics for Ensemble
ensemble_accuracy = accuracy_score(y_test, ensemble_test_predictions)
ensemble_precision = precision_score(y_test, ensemble_test_predictions, average='weighted')
ensemble_recall = recall_score(y_test, ensemble_test_predictions, average='weighted')
ensemble_f1 = f1_score(y_test, ensemble_test_predictions, average='weighted')
ensemble_roc_auc = roc_auc_score(pd.get_dummies(y_test), ensemble_test_probabilities, multi_class='ovr')

print("\nEnsemble Metrics")
print(f"• Accuracy: {ensemble_accuracy:.4f}")
print(f"• Precision: {ensemble_precision:.4f}")
print(f"• Recall: {ensemble_recall:.4f}")
print(f"• F1 Score: {ensemble_f1:.4f}")
print(f"• ROC AUC: {ensemble_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_ensemble = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [ensemble_accuracy, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_roc_auc]
}
metrics_df_ensemble = pd.DataFrame(metrics_data_ensemble)
metrics_file = os.path.join(output_dir_ensemble, 'ensemble_metrics_results.xlsx')
metrics_df_ensemble.to_excel(metrics_file, index=False)
print(f"\nMetrics saved to '{metrics_file}'.")

# Confusion Matrix
conf_matrix_ensemble = confusion_matrix(y_test, ensemble_test_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix_ensemble,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=np.unique(y_test),
    yticklabels=np.unique(y_test)
)
plt.title('Confusion Matrix - Ensemble')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
conf_matrix_file = os.path.join(output_dir_ensemble, 'confusion_matrix_ensemble.png')
plt.savefig(conf_matrix_file)
plt.show()

# ROC Curve
fpr_ensemble = {}
tpr_ensemble = {}
roc_auc_ensemble = {}
classes_ensemble = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_ensemble):
    fpr_ensemble[class_label], tpr_ensemble[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], ensemble_test_probabilities[:, i])
    roc_auc_ensemble[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], ensemble_test_probabilities[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_ensemble:
    plt.plot(fpr_ensemble[class_label], tpr_ensemble[class_label], label=f'Class {class_label} (AUC = {roc_auc_ensemble[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Ensemble')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
roc_curve_file = os.path.join(output_dir_ensemble, 'roc_curve_ensemble.png')
plt.savefig(roc_curve_file)
plt.show()

# Precision-Recall Curve
precision_ensemble = {}
recall_ensemble = {}

for i, class_label in enumerate(classes_ensemble):
    precision_ensemble[class_label], recall_ensemble[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], ensemble_test_probabilities[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_ensemble:
    plt.plot(recall_ensemble[class_label], precision_ensemble[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Ensemble')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
pr_curve_file = os.path.join(output_dir_ensemble, 'precision_recall_curve_ensemble.png')
plt.savefig(pr_curve_file)
plt.show()

print(f"All plots and metrics have been saved to the directory: {output_dir_ensemble}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Example Data
fpr_ensemble, tpr_ensemble, _ = roc_curve(y_true, ensemble_pred)
roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)

fpr_xgb, tpr_xgb, _ = roc_curve(y_true, xgb_pred)
roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

fpr_dnn, tpr_dnn, _ = roc_curve(y_true, dnn_pred)
roc_auc_dnn = auc(fpr_dnn, tpr_dnn)

fpr_rf, tpr_rf, _ = roc_curve(y_true, rf_pred)
roc_auc_rf = auc(fpr_rf, tpr_rf)

fpr_knn, tpr_knn, _ = roc_curve(y_true, knn_pred)
roc_auc_knn = auc(fpr_knn, tpr_knn)

fpr_bayes, tpr_bayes, _ = roc_curve(y_true, bayes_pred)
roc_auc_bayes = auc(fpr_bayes, tpr_bayes)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(fpr_ensemble, tpr_ensemble, color='darkorange', lw=2, label=f'Ensemble (AUC = {roc_auc_ensemble:.2f})')
plt.plot(fpr_xgb, tpr_xgb, color='blue', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')
plt.plot(fpr_dnn, tpr_dnn, color='green', lw=2, label=f'DNN (AUC = {roc_auc_dnn:.2f})')
plt.plot(fpr_rf, tpr_rf, color='purple', lw=2, label=f'RF (AUC = {roc_auc_rf:.2f})')
plt.plot(fpr_knn, tpr_knn, color='cyan', lw=2, label=f'KNN (AUC = {roc_auc_knn:.2f})')
plt.plot(fpr_bayes, tpr_bayes, color='pink', lw=2, label=f'Bayesian (AUC = {roc_auc_bayes:.2f})')

plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Combined ROC Curves for Models')
plt.legend(loc='lower right')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

class ExtremeLearningMachine:
    def __init__(self, hidden_size=100, activation_function='relu'):
        self.hidden_size = hidden_size
        self.activation_function = activation_function

    def _activation(self, x):
        return np.maximum(0, x) if self.activation_function == 'relu' else np.tanh(x)

    def fit(self, X, y):
        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))
        self.biases = np.random.normal(size=(self.hidden_size,))
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        self.output_weights = np.dot(np.linalg.pinv(H), np.eye(len(np.unique(y)))[y])

    def predict_proba(self, X):
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        output = np.dot(H, self.output_weights)
        return output / np.sum(output, axis=1, keepdims=True)

# ELM model
elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')

# DNN model (same as above)
dnn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])

# Preprocess and split data
# (Use the same `data` preprocessing and splitting as before)

# Fit models
elm_model.fit(X_train_scaled, y_train)
dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
dnn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=0)

# Predict probabilities
elm_val_pred = elm_model.predict_proba(X_val_scaled)
dnn_val_pred = dnn_model.predict(X_val_scaled)

# Stack predictions and train meta-model
stacked_val_pred = np.hstack((elm_val_pred, dnn_val_pred))
meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_val_pred, y_val)

# Predict on test set
elm_test_pred = elm_model.predict_proba(X_test_scaled)
dnn_test_pred = dnn_model.predict(X_test_scaled)
stacked_test_pred = np.hstack((elm_test_pred, dnn_test_pred))
final_predictions = meta_model.predict(stacked_test_pred)

# Evaluate ensemble
test_accuracy = accuracy_score(y_test, final_predictions)
print(f"Ensemble (ELM + DNN) Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, final_predictions))

# Predict probabilities for validation set
xgb_val_pred = xgb_model.predict_proba(X_val_scaled)
elm_val_pred = elm_model.predict_proba(X_val_scaled)
dnn_val_pred = dnn_model.predict(X_val_scaled)

# Stack predictions
stacked_val_pred = np.hstack((xgb_val_pred, elm_val_pred, dnn_val_pred))

# Train meta-model
meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_val_pred, y_val)

# Predict on test set
xgb_test_pred = xgb_model.predict_proba(X_test_scaled)
elm_test_pred = elm_model.predict_proba(X_test_scaled)
dnn_test_pred = dnn_model.predict(X_test_scaled)
stacked_test_pred = np.hstack((xgb_test_pred, elm_test_pred, dnn_test_pred))
final_predictions = meta_model.predict(stacked_test_pred)

# Evaluate ensemble
test_accuracy = accuracy_score(y_test, final_predictions)
print(f"Ensemble (XGBoost + ELM + DNN) Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, final_predictions))

# XGBoost model
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)
xgb_model.fit(X_train_scaled, y_train)

# Predictions
xgb_val_predictions = xgb_model.predict(X_val_scaled)
xgb_test_predictions = xgb_model.predict(X_test_scaled)

# Evaluate performance
val_accuracy = accuracy_score(y_val, xgb_val_predictions)
test_accuracy = accuracy_score(y_test, xgb_test_predictions)

print(f"XGBoost Validation Accuracy: {val_accuracy * 100:.2f}%")
print(f"XGBoost Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report (Test):\n", classification_report(y_test, xgb_test_predictions))

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd

# Create a directory to save the plots if it doesn't exist
output_dir_xgb = 'xgboost_output_plots'
os.makedirs(output_dir_xgb, exist_ok=True)

# Calculate Metrics for XGBoost
xgb_test_predictions = xgb_model.predict(X_test_scaled)
xgb_test_probabilities = xgb_model.predict_proba(X_test_scaled)

xgb_accuracy = accuracy_score(y_test, xgb_test_predictions)
xgb_precision = precision_score(y_test, xgb_test_predictions, average='weighted')
xgb_recall = recall_score(y_test, xgb_test_predictions, average='weighted')
xgb_f1 = f1_score(y_test, xgb_test_predictions, average='weighted')
xgb_roc_auc = roc_auc_score(pd.get_dummies(y_test), xgb_test_probabilities, multi_class='ovr')

print("\nXGBoost Metrics")
print(f"• Accuracy: {xgb_accuracy:.4f}")
print(f"• Precision: {xgb_precision:.4f}")
print(f"• Recall: {xgb_recall:.4f}")
print(f"• F1 Score: {xgb_f1:.4f}")
print(f"• ROC AUC: {xgb_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_xgb = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [xgb_accuracy, xgb_precision, xgb_recall, xgb_f1, xgb_roc_auc]
}
metrics_df_xgb = pd.DataFrame(metrics_data_xgb)
metrics_file = os.path.join(output_dir_xgb, 'xgboost_metrics_results.xlsx')
metrics_df_xgb.to_excel(metrics_file, index=False)
print(f"\nMetrics saved to '{metrics_file}'.")

# Confusion Matrix
conf_matrix_xgb = confusion_matrix(y_test, xgb_test_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix_xgb,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=np.unique(y_test),
    yticklabels=np.unique(y_test)
)
plt.title('Confusion Matrix - XGBoost')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
conf_matrix_file = os.path.join(output_dir_xgb, 'confusion_matrix_xgb.png')
plt.savefig(conf_matrix_file)
plt.show()

# ROC Curve
fpr_xgb = {}
tpr_xgb = {}
roc_auc_xgb = {}
classes_xgb = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_xgb):
    fpr_xgb[class_label], tpr_xgb[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], xgb_test_probabilities[:, i])
    roc_auc_xgb[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], xgb_test_probabilities[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_xgb:
    plt.plot(fpr_xgb[class_label], tpr_xgb[class_label], label=f'Class {class_label} (AUC = {roc_auc_xgb[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - XGBoost')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
roc_curve_file = os.path.join(output_dir_xgb, 'roc_curve_xgb.png')
plt.savefig(roc_curve_file)
plt.show()

# Precision-Recall Curve
precision_xgb = {}
recall_xgb = {}

for i, class_label in enumerate(classes_xgb):
    precision_xgb[class_label], recall_xgb[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], xgb_test_probabilities[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_xgb:
    plt.plot(recall_xgb[class_label], precision_xgb[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - XGBoost')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
pr_curve_file = os.path.join(output_dir_xgb, 'precision_recall_curve_xgb.png')
plt.savefig(pr_curve_file)
plt.show()

print(f"All plots and metrics have been saved to the directory: {output_dir_xgb}")

import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin

# Define the ELM class
class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):
    def __init__(self, hidden_size=100, activation_function='relu'):
        self.hidden_size = hidden_size
        self.activation_function = activation_function
        self.input_weights = None
        self.biases = None
        self.output_weights = None

    def _activation(self, x):
        if self.activation_function == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation_function == 'tanh':
            return np.tanh(x)
        elif self.activation_function == 'relu':
            return np.maximum(0, x)
        else:
            raise ValueError("Unsupported activation function.")

    def fit(self, X, y):
        num_classes = len(np.unique(y))
        y_one_hot = np.zeros((len(y), num_classes))
        y_one_hot[np.arange(len(y)), y] = 1

        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))
        self.biases = np.random.normal(size=(self.hidden_size,))
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)

    def predict(self, X):
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        output = np.dot(H, self.output_weights)
        return np.argmax(output, axis=1)

# Train and evaluate ELM
elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')
elm_model.fit(X_train_scaled, y_train)

elm_val_predictions = elm_model.predict(X_val_scaled)
elm_test_predictions = elm_model.predict(X_test_scaled)

val_accuracy = accuracy_score(y_val, elm_val_predictions)
test_accuracy = accuracy_score(y_test, elm_test_predictions)

print(f"ELM Validation Accuracy: {val_accuracy * 100:.2f}%")
print(f"ELM Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report (Test):\n", classification_report(y_test, elm_test_predictions))

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd
import numpy as np

# Create a directory to save the plots if it doesn't exist
output_dir_elm = 'elm_output_plots'
os.makedirs(output_dir_elm, exist_ok=True)

# Predict using ELM
elm_test_predictions = elm_model.predict(X_test_scaled)

# Convert predictions to probabilities-like output
elm_test_probabilities = np.zeros((len(elm_test_predictions), len(np.unique(y_test))))
for idx, pred in enumerate(elm_test_predictions):
    elm_test_probabilities[idx, pred] = 1

# Calculate Metrics for ELM
elm_accuracy = accuracy_score(y_test, elm_test_predictions)
elm_precision = precision_score(y_test, elm_test_predictions, average='weighted')
elm_recall = recall_score(y_test, elm_test_predictions, average='weighted')
elm_f1 = f1_score(y_test, elm_test_predictions, average='weighted')
elm_roc_auc = roc_auc_score(pd.get_dummies(y_test), elm_test_probabilities, multi_class="ovr")

print("\nELM Metrics")
print(f"• Accuracy: {elm_accuracy:.4f}")
print(f"• Precision: {elm_precision:.4f}")
print(f"• Recall: {elm_recall:.4f}")
print(f"• F1 Score: {elm_f1:.4f}")
print(f"• ROC AUC: {elm_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_elm = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [elm_accuracy, elm_precision, elm_recall, elm_f1, elm_roc_auc]
}
metrics_df_elm = pd.DataFrame(metrics_data_elm)
metrics_file = os.path.join(output_dir_elm, 'elm_metrics_results.xlsx')
metrics_df_elm.to_excel(metrics_file, index=False)
print(f"\nMetrics saved to '{metrics_file}'.")

# Confusion Matrix
conf_matrix_elm = confusion_matrix(y_test, elm_test_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix_elm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=np.unique(y_test),
    yticklabels=np.unique(y_test),
)
plt.title("Confusion Matrix - ELM")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
conf_matrix_file = os.path.join(output_dir_elm, 'confusion_matrix_elm.png')
plt.savefig(conf_matrix_file)
plt.show()

# ROC Curve
fpr_elm = {}
tpr_elm = {}
roc_auc_elm = {}
classes_elm = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_elm):
    fpr_elm[class_label], tpr_elm[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], elm_test_probabilities[:, i])
    roc_auc_elm[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], elm_test_probabilities[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_elm:
    plt.plot(fpr_elm[class_label], tpr_elm[class_label], label=f"Class {class_label} (AUC = {roc_auc_elm[class_label]:.2f})")
plt.plot([0, 1], [0, 1], "k--")  # Diagonal line
plt.title("ROC Curve - ELM")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
roc_curve_file = os.path.join(output_dir_elm, 'roc_curve_elm.png')
plt.savefig(roc_curve_file)
plt.show()

# Precision-Recall Curve
precision_elm = {}
recall_elm = {}

for i, class_label in enumerate(classes_elm):
    precision_elm[class_label], recall_elm[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], elm_test_probabilities[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_elm:
    plt.plot(recall_elm[class_label], precision_elm[class_label], label=f"Class {class_label}")
plt.title("Precision-Recall Curve - ELM")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()
pr_curve_file = os.path.join(output_dir_elm, 'precision_recall_curve_elm.png')
plt.savefig(pr_curve_file)
plt.show()

print(f"All plots and metrics have been saved to the directory: {output_dir_elm}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
import pandas as pd
import numpy as np

# Assuming you have the predicted probabilities for each model and true labels
models = {
    "ELM": elm_test_proba,
    "XGBoost": xgb_test_predictions_proba,
    "Random Forest": rf_test_proba,
    "KNN": knn_test_proba,
    "Bayesian": bayesian_test_proba,
    "DNN": dnn_test_proba
}

true_labels = pd.get_dummies(y_test).values  # One-hot encoded true labels

# Initialize plot
plt.figure(figsize=(10, 8))

# Loop through models to calculate ROC curves and AUC
for model_name, model_proba in models.items():
    fpr = {}
    tpr = {}
    roc_auc = {}

    for i in range(true_labels.shape[1]):  # Iterate through each class
        fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], model_proba[:, i])
        roc_auc[i] = roc_auc_score(true_labels[:, i], model_proba[:, i])

    # Average across all classes
    mean_fpr = np.linspace(0, 1, 100)
    mean_tpr = np.mean([np.interp(mean_fpr, fpr[i], tpr[i]) for i in range(true_labels.shape[1])], axis=0)
    mean_auc = np.mean(list(roc_auc.values()))

    # Plot the ROC curve for this model
    plt.plot(mean_fpr, mean_tpr, label=f"{model_name} (AUC = {mean_auc:.2f})")

# Plot random guess line
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")

# Finalize plot
plt.title("ROC Curve Comparison Across Models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid()
plt.savefig("combined_roc_curves.png")
plt.show()

import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.utils import plot_model
from tensorflow.keras.backend import count_params
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input
import tensorflow as tf

# Function to calculate FLOPs for TensorFlow models
def calculate_flops(model):
    session = tf.compat.v1.Session()
    graph = tf.compat.v1.get_default_graph()
    with session.as_default():
        with graph.as_default():
            run_meta = tf.compat.v1.RunMetadata()
            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
            flops = tf.compat.v1.profiler.profile(
                graph=graph, run_meta=run_meta, cmd="op", options=opts
            ).total_float_ops
    return flops

# Function to calculate the number of parameters for ELM
def calculate_elm_params(model):
    input_params = model.input_weights.size
    bias_params = model.biases.size
    output_params = model.output_weights.size
    return input_params + bias_params + output_params

# Efficiency Analysis
def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, model_params=None):
    # Measure execution time
    start_time = time.time()
    if model_name == "DNN":
        model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=0)
    else:
        model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # Predict
    start_time = time.time()
    predictions = model.predict(X_test)
    inference_time = time.time() - start_time

    # Calculate accuracy
    test_accuracy = accuracy_score(y_test, np.argmax(predictions, axis=1) if model_name == "DNN" else predictions)

    # Calculate number of parameters
    if model_name == "DNN":
        params = model.count_params()
        flops = calculate_flops(model)
    elif model_name == "ELM":
        params = calculate_elm_params(model)
        flops = "Not Applicable"
    else:  # XGBoost
        params = model.n_estimators * model.max_depth
        flops = "Not Applicable"

    return {
        "Model": model_name,
        "Train Time (s)": train_time,
        "Inference Time (s)": inference_time,
        "Accuracy": test_accuracy,
        "Parameters": params,
        "FLOPs": flops,
    }

# Prepare analysis
results = []

# DNN Analysis
dnn_model = create_model(
    hidden_layers=2,
    hidden_nodes=20,
    input_shape=(X_train_scaled.shape[1],),
    learning_rate=0.001,
    optimizer="adam",
    activation="tanh",
)
dnn_result = efficiency_analysis(dnn_model, "DNN", X_train_scaled, y_train, X_test_scaled, y_test)
results.append(dnn_result)

# XGBoost Analysis
xgb_result = efficiency_analysis(xgb_model, "XGBoost", X_train_scaled, y_train, X_test_scaled, y_test)
results.append(xgb_result)

# ELM Analysis
elm_result = efficiency_analysis(elm_model, "ELM", X_train_scaled, y_train, X_test_scaled, y_test)
results.append(elm_result)

# Convert results to DataFrame
efficiency_df = pd.DataFrame(results)

# Save results to an Excel file
efficiency_df.to_excel("efficiency_analysis_results.xlsx", index=False)

# Print Results
print("\nEfficiency Analysis Results:")
print(efficiency_df)

# Visualization
metrics = ["Train Time (s)", "Inference Time (s)", "Accuracy", "Parameters"]
for metric in metrics:
    plt.figure(figsize=(8, 6))
    plt.bar(efficiency_df["Model"], efficiency_df[metric], color=["blue", "green", "orange"])
    plt.title(f"{metric} Comparison")
    plt.ylabel(metric)
    plt.xlabel("Models")
    plt.show()

from sklearn.metrics import accuracy_score, classification_report

# Adjust model predictions for classification
def adjust_predictions_for_classification(model, X_test):
    predictions = model.predict(X_test)
    if len(predictions.shape) > 1:  # Probabilistic outputs
        return predictions.argmax(axis=1)  # Convert probabilities to class labels
    return predictions

# Efficiency analysis function with adjusted predictions
def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, flops=None):
    memory_before = get_memory_usage()
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    memory_after = get_memory_usage()

    start_time = time.time()
    predictions = adjust_predictions_for_classification(model, X_test)
    inference_time = time.time() - start_time

    throughput = measure_throughput(model, X_test)

    return {
        "Model": model_name,
        "Train Time (s)": train_time,
        "Inference Time (s)": inference_time,
        "Accuracy": accuracy_score(y_test, predictions),
        "Parameters": sum([np.prod(v.shape) for v in model.get_weights()]) if hasattr(model, "get_weights") else "N/A",
        "Memory (MB)": memory_after - memory_before,
        "Throughput (Pred/s)": throughput,
        "FLOPs": flops or "Not Applicable"
    }

# Perform efficiency analysis for DNN
dnn_flops = calculate_flops_dnn(
    hidden_layers=dnn_hidden_layers,
    hidden_nodes=dnn_hidden_nodes,
    input_size=X_train.shape[1],
    output_classes=len(np.unique(y_train))
)

dnn_result = efficiency_analysis(
    best_model,
    "DNN",
    X_train_scaled,
    y_train,
    X_test_scaled,
    y_test,
    flops=dnn_flops
)

# Perform efficiency analysis for XGBoost
xgb_result = efficiency_analysis(
    xgb_model,
    "XGBoost",
    X_train_scaled,
    y_train,
    X_test_scaled,
    y_test
)

# Perform efficiency analysis for ELM
elm_flops = calculate_flops_elm(
    input_size=X_train.shape[1],
    hidden_neurons=100,
    output_classes=len(np.unique(y_train))
)

elm_result = efficiency_analysis(
    elm_model,
    "ELM",
    X_train_scaled,
    y_train,
    X_test_scaled,
    y_test,
    flops=elm_flops
)

# Compile and save results
results = [dnn_result, xgb_result, elm_result]
results_df = pd.DataFrame(results)

# Save results to Excel
results_df.to_excel("efficiency_analysis.xlsx", index=False)

# Print results
print(results_df)

# Plot Comparative Histogram
results_df.set_index("Model")[["Train Time (s)", "Inference Time (s)", "Memory (MB)", "Throughput (Pred/s)"]].plot(kind="bar", figsize=(10, 6))
plt.title("Efficiency Analysis")
plt.ylabel("Metrics")
plt.show()

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.base import BaseEstimator, ClassifierMixin
from imblearn.over_sampling import SMOTE

# Ensure feature columns are numeric
def preprocess_features(data):
    for column in data.columns:
        if isinstance(data[column].iloc[0], (list, np.ndarray)):
            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
    return data

# Preprocess the features
X = preprocess_features(data.drop('Label', axis=1))
y = data['Label']

# Map string labels to integers
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = np.array([label_mapping[label] for label in y])

# Normalize features
X = X.values.astype(np.float32)

# Apply SMOTE for class imbalance
smote = SMOTE(random_state=1)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the data into 70% training, 20% validation, and 10% testing
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.1, random_state=1
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2222, random_state=1
)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Define base models
rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=1)
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)

# Define a custom ELM class
class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):
    def __init__(self, hidden_size=100, activation_function='relu'):
        self.hidden_size = hidden_size
        self.activation_function = activation_function
        self.input_weights = None
        self.biases = None
        self.output_weights = None

    def _activation(self, x):
        if self.activation_function == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation_function == 'tanh':
            return np.tanh(x)
        elif self.activation_function == 'relu':
            return np.maximum(0, x)
        else:
            raise ValueError("Unsupported activation function.")

    def fit(self, X, y):
        num_classes = len(np.unique(y))
        y_one_hot = np.zeros((len(y), num_classes))
        y_one_hot[np.arange(len(y)), y] = 1

        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))
        self.biases = np.random.normal(size=(self.hidden_size,))
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)

    def predict(self, X):
        H = self._activation(np.dot(X, self.input_weights) + self.biases)
        output = np.dot(H, self.output_weights)
        return np.argmax(output, axis=1)

elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')

# Fit base models
rf_model.fit(X_train_scaled, y_train)
xgb_model.fit(X_train_scaled, y_train)
elm_model.fit(X_train_scaled, y_train)

# Predict using base models
rf_predictions = rf_model.predict_proba(X_val_scaled)
xgb_predictions = xgb_model.predict_proba(X_val_scaled)
elm_predictions = elm_model.predict(X_val_scaled)
elm_proba = np.zeros((len(elm_predictions), len(np.unique(y_train))))  # Convert ELM to probability-like output
for idx, pred in enumerate(elm_predictions):
    elm_proba[idx, pred] = 1

# Stack predictions
stacked_predictions = np.hstack((rf_predictions, xgb_predictions, elm_proba))

# Train meta-model
meta_model = LogisticRegression(random_state=42)
meta_model.fit(stacked_predictions, y_val)

# Evaluate the ensemble on test data
rf_test_predictions = rf_model.predict_proba(X_test_scaled)
xgb_test_predictions = xgb_model.predict_proba(X_test_scaled)
elm_test_predictions = elm_model.predict(X_test_scaled)
elm_test_proba = np.zeros((len(elm_test_predictions), len(np.unique(y_test))))
for idx, pred in enumerate(elm_test_predictions):
    elm_test_proba[idx, pred] = 1

stacked_test_predictions = np.hstack((rf_test_predictions, xgb_test_predictions, elm_test_proba))
final_predictions = meta_model.predict(stacked_test_predictions)

# Evaluate performance
test_accuracy = accuracy_score(y_test, final_predictions)
print(f"Ensemble Test Accuracy: {test_accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, final_predictions))

from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_curve,
)
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Verify dimensions of predictions
print(f"Shape of stacked_test_predictions: {stacked_test_predictions.shape}")
print(f"Number of unique classes in y_test: {len(np.unique(y_test))}")

# Ensure the probabilities align with 8 classes
n_classes = 8
if stacked_test_predictions.shape[1] != n_classes:
    print("Adjusting probabilities to match 8 classes.")
    adjusted_proba = np.zeros((stacked_test_predictions.shape[0], n_classes))
    for i in range(stacked_test_predictions.shape[0]):
        for j in range(min(stacked_test_predictions.shape[1], n_classes)):
            adjusted_proba[i, j] = stacked_test_predictions[i, j]
    stacked_test_predictions = adjusted_proba

# Calculate metrics
test_accuracy = accuracy_score(y_test, final_predictions)
test_precision = precision_score(y_test, final_predictions, average="weighted")
test_recall = recall_score(y_test, final_predictions, average="weighted")
test_f1 = f1_score(y_test, final_predictions, average="weighted")
test_roc_auc = roc_auc_score(pd.get_dummies(y_test), stacked_test_predictions, multi_class="ovr")

# Print Metrics
print("\nEnsemble Model Metrics")
print(f"• Accuracy: {test_accuracy:.4f}")
print(f"• Precision: {test_precision:.4f}")
print(f"• Recall: {test_recall:.4f}")
print(f"• F1 Score: {test_f1:.4f}")
print(f"• ROC AUC: {test_roc_auc:.4f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, final_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=np.arange(n_classes),
    yticklabels=np.arange(n_classes),
)
plt.title("Confusion Matrix - Ensemble Model")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# ROC Curve
fpr = {}
tpr = {}
roc_auc = {}

classes = range(n_classes)

for i in classes:
    fpr[i], tpr[i], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])
    roc_auc[i] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])

plt.figure(figsize=(10, 8))
for i in classes:
    plt.plot(fpr[i], tpr[i], label=f"Class {i} (AUC = {roc_auc[i]:.2f})")
plt.plot([0, 1], [0, 1], "k--")  # Diagonal line
plt.title("ROC Curve - Ensemble Model")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

# Precision-Recall Curve
precision = {}
recall = {}
average_precision = {}

# Calculate Precision-Recall for each class
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])
    average_precision[i] = average_precision_score(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])

# Plot Precision-Recall Curves
plt.figure(figsize=(10, 8))
for i in classes:
    plt.plot(recall[i], precision[i], label=f'Class {i} (AP = {average_precision[i]:.2f})')
plt.title("Precision-Recall Curve - Ensemble Model")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()
plt.grid()
plt.show()

# Print Average Precision Scores for each class
print("\nAverage Precision Scores:")
for i in classes:
    print(f"Class {i}: {average_precision[i]:.4f}")

# Evaluate the XGBoost Model
y_test_pred_xgb = xgb_model.predict(X_test_scaled)
y_test_proba_xgb = xgb_model.predict_proba(X_test_scaled)

# Calculate Metrics
xgb_accuracy = accuracy_score(y_test, y_test_pred_xgb)
xgb_precision = precision_score(y_test, y_test_pred_xgb, average='weighted')
xgb_recall = recall_score(y_test, y_test_pred_xgb, average='weighted')
xgb_f1 = f1_score(y_test, y_test_pred_xgb, average='weighted')
xgb_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba_xgb, multi_class='ovr')

print("\nXGBoost Classifier Metrics")
print(f"• Accuracy: {xgb_accuracy:.4f}")
print(f"• Precision: {xgb_precision:.4f}")
print(f"• Recall: {xgb_recall:.4f}")
print(f"• F1 Score: {xgb_f1:.4f}")
print(f"• ROC AUC: {xgb_roc_auc:.4f}")

# Confusion Matrix
conf_matrix_xgb = confusion_matrix(y_test, y_test_pred_xgb)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - XGBoost')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# ROC Curve
fpr_xgb, tpr_xgb, roc_auc_xgb = {}, {}, {}

plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    fpr_xgb[class_label], tpr_xgb[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_xgb[:, i])
    roc_auc_xgb[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba_xgb[:, i])
    plt.plot(fpr_xgb[class_label], tpr_xgb[class_label], label=f'Class {class_label} (AUC = {roc_auc_xgb[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve - XGBoost')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Precision-Recall Curve
precision_xgb, recall_xgb = {}, {}

plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    precision_xgb[class_label], recall_xgb[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_xgb[:, i])
    plt.plot(recall_xgb[class_label], precision_xgb[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - XGBoost')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    precision_recall_curve
)
import seaborn as sns

# Evaluate the ELM Model
y_test_pred_elm = elm_model.predict(X_test_scaled)

# Since ELM doesn't directly provide probabilities, convert predictions to probabilities-like output
elm_proba = np.zeros((len(y_test_pred_elm), len(classes)))
for idx, pred in enumerate(y_test_pred_elm):
    elm_proba[idx, pred] = 1

# Calculate Metrics
elm_accuracy = accuracy_score(y_test, y_test_pred_elm)
elm_precision = precision_score(y_test, y_test_pred_elm, average='weighted')
elm_recall = recall_score(y_test, y_test_pred_elm, average='weighted')
elm_f1 = f1_score(y_test, y_test_pred_elm, average='weighted')
elm_roc_auc = roc_auc_score(pd.get_dummies(y_test), elm_proba, multi_class='ovr')

print("\nELM Classifier Metrics")
print(f"• Accuracy: {elm_accuracy:.4f}")
print(f"• Precision: {elm_precision:.4f}")
print(f"• Recall: {elm_recall:.4f}")
print(f"• F1 Score: {elm_f1:.4f}")
print(f"• ROC AUC: {elm_roc_auc:.4f}")

# Confusion Matrix
conf_matrix_elm = confusion_matrix(y_test, y_test_pred_elm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_elm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - ELM')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# ROC Curve
fpr_elm, tpr_elm, roc_auc_elm = {}, {}, {}

plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    fpr_elm[class_label], tpr_elm[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], elm_proba[:, i])
    roc_auc_elm[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], elm_proba[:, i])
    plt.plot(fpr_elm[class_label], tpr_elm[class_label], label=f'Class {class_label} (AUC = {roc_auc_elm[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve - ELM')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Precision-Recall Curve
precision_elm, recall_elm = {}, {}

plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    precision_elm[class_label], recall_elm[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], elm_proba[:, i])
    plt.plot(recall_elm[class_label], precision_elm[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - ELM')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Prepare dictionaries to store FPR, TPR, and AUC for all models
models = {
    "Ensemble": stacked_test_predictions,
    "Random Forest": rf_metrics_results,
    "DNN": dnn_metrics_results,  # Assuming predictions are stored as dnn_test_predictions
    "KNN": knn_metrics_results,  # Assuming predictions are stored as knn_test_predictions
    "XGBoost": xgb_test_predictions,
    "Bayesian": nb_metrics_results,  # Assuming predictions are stored as bayesian_test_predictions
    "ELM": elm_test_proba
}

# Initialize ROC storage
roc_results = {}

# Generate ROC and AUC for each model
for model_name, model_proba in models.items():
    fpr = {}
    tpr = {}
    roc_auc = {}

    for i in range(n_classes):  # Iterate through each class
        fpr[i], tpr[i], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], model_proba[:, i])
        roc_auc[i] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], model_proba[:, i])

    roc_results[model_name] = {
        "fpr": fpr,
        "tpr": tpr,
        "roc_auc": roc_auc
    }

# Plot all ROC curves
plt.figure(figsize=(12, 10))

# Plot each model's macro-average ROC
for model_name, results in roc_results.items():
    macro_avg_auc = np.mean(list(results["roc_auc"].values()))
    plt.plot(
        results["fpr"][0],  # Use the first class for uniform visualization
        results["tpr"][0],
        label=f"{model_name} (Macro-AUC = {macro_avg_auc:.2f})"
    )

# Add a diagonal line for random classifier
plt.plot([0, 1], [0, 1], "k--", label="Random Classifier")

# Customize the plot
plt.title("ROC Curve Comparison Across All Models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid()
plt.show()

# Print AUC for each model
print("\nROC AUC Scores for All Models:")
for model_name, results in roc_results.items():
    macro_avg_auc = np.mean(list(results["roc_auc"].values()))
    print(f"{model_name}: Macro-AUC = {macro_avg_auc:.4f}")

# Address class imbalance using SMOTE
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
import cv2
from scipy.ndimage import gaussian_filter, median_filter

smote = SMOTE(random_state=1)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the data into 70% training, 20% validation, and 10% testing
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.1, random_state=1
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2222, random_state=1
)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Define the hyperparameters to test
n_estimators_list = [100, 200]
max_depth_list = [10, 20]
criterion_list = ['gini', 'entropy']

# Store results for comparison
rf_results = []

# Train and evaluate Random Forest

# Train and evaluate Random Forest models with different hyperparameters
for n_estimators in n_estimators_list:
    for max_depth in max_depth_list:
        for criterion in criterion_list:
            # Initialize the Random Forest model
            rf_model = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                criterion=criterion,
                random_state=1
            )

            # Train the model on the training set
            rf_model.fit(X_train_scaled, y_train)

            # Evaluate on the validation set
            y_val_pred = rf_model.predict(X_val_scaled)
            val_accuracy = accuracy_score(y_val, y_val_pred)

            # Store the results
            rf_results.append({
                'n_estimators': n_estimators,
                'max_depth': max_depth,
                'criterion': criterion,
                'Validation Accuracy': val_accuracy
            })

# Convert results to a DataFrame
rf_results_df = pd.DataFrame(rf_results)

# Find the best configuration based on validation accuracy
best_rf_result = rf_results_df.loc[rf_results_df['Validation Accuracy'].idxmax()]
print("\nBest Random Forest configuration found:")
print(best_rf_result)

# Train the final model with the best hyperparameters on combined training and validation data
best_rf_model = RandomForestClassifier(
    n_estimators=best_rf_result['n_estimators'],
    max_depth=best_rf_result['max_depth'],
    criterion=best_rf_result['criterion'],
    random_state=1
)
best_rf_model.fit(scaler.transform(X_train_val), y_train_val)

# Evaluate the final model on the test set
y_test_pred = best_rf_model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"\nTest Accuracy with the best configuration: {test_accuracy * 100:.2f}%")

# Save results to an Excel file
output_file = 'rf_model_results.xlsx'
rf_results_df.to_excel(output_file, index=False)
print(f"\nResults saved to '{output_file}' for download.")

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

# Scale combined training and validation data
from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Scale training and validation data
X_train_val_scaled = scaler.fit_transform(X_train_val)
X_test_scaled = scaler.transform(X_test)

# Load Random Forest results from Excel
rf_results_file = 'rf_model_results.xlsx'  # Update with your file path
rf_results_df = pd.read_excel(rf_results_file)

# Select the best configuration based on validation accuracy
best_rf_result = rf_results_df.loc[rf_results_df['Validation Accuracy'].idxmax()]
print("\nBest Random Forest configuration found:")
print(best_rf_result)

# Extract best hyperparameters
n_estimators = int(best_rf_result['n_estimators'])
max_depth = int(best_rf_result['max_depth']) if not pd.isnull(best_rf_result['max_depth']) else None
criterion = best_rf_result['criterion']

# Train the final model with the best configuration on combined training and validation data
best_rf_model = RandomForestClassifier(
    n_estimators=n_estimators,
    max_depth=max_depth,
    criterion=criterion,
    random_state=1
)
best_rf_model.fit(X_train_val_scaled, y_train_val)

# Evaluate the final model on the test set
y_test_pred = best_rf_model.predict(X_test_scaled)
y_test_proba = best_rf_model.predict_proba(X_test_scaled)

# Calculate Metrics
rf_accuracy = accuracy_score(y_test, y_test_pred)
rf_precision = precision_score(y_test, y_test_pred, average='weighted')
rf_recall = recall_score(y_test, y_test_pred, average='weighted')
rf_f1 = f1_score(y_test, y_test_pred, average='weighted')
rf_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr')

print("\nRandom Forest Classifier Metrics")
print(f"• Accuracy: {rf_accuracy:.4f}")
print(f"• Precision: {rf_precision:.4f}")
print(f"• Recall: {rf_recall:.4f}")
print(f"• F1 Score: {rf_f1:.4f}")
print(f"• ROC AUC: {rf_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]
}
metrics_df = pd.DataFrame(metrics_data)
metrics_df.to_excel('rf_metrics_results.xlsx', index=False)
print("\nMetrics saved to 'rf_metrics_results.xlsx' for download.")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# ROC Curve
fpr = {}
tpr = {}
roc_auc = {}
classes = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes):
    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])
    roc_auc[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes:
    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Random Forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Precision-Recall Curve
precision = {}
recall = {}

for i, class_label in enumerate(classes):
    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes:
    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Random Forest')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# Load hyperparameter tuning results from the Excel file
results_file = 'hyperparameter_tuning_results.xlsx'  # Replace with your actual file path
results_df = pd.read_excel(results_file)

# Select the best configuration based on mean validation accuracy
best_result = results_df.loc[results_df['mean_val_accuracy'].idxmax()]
print("\nBest Deep Neural Network Configuration Found:")
print(best_result)

# Extract hyperparameters from the best configuration
hidden_layers = int(best_result['hidden_layers'])
hidden_nodes = int(best_result['hidden_nodes'])
learning_rate = best_result['learning_rate']
optimizer = best_result['optimizer']
activation = best_result['activation']

# Assuming predictions are already available from the best model
# Replace `y_test_dl_pred_proba` and `y_test_dl_pred` with actual data
# Example placeholders for predictions:
# y_test_dl_pred_proba = np.load('y_test_dl_pred_proba.npy')  # Load saved predicted probabilities
# y_test_dl_pred = np.argmax(y_test_dl_pred_proba, axis=1)   # Predicted labels

# Calculate Metrics
dnn_accuracy = accuracy_score(y_test, y_test_dl_pred)
dnn_precision = precision_score(y_test, y_test_dl_pred, average='weighted')
dnn_recall = recall_score(y_test, y_test_dl_pred, average='weighted')
dnn_f1 = f1_score(y_test, y_test_dl_pred, average='weighted')
dnn_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_dl_pred_proba, multi_class='ovr')

# Print Metrics
print("\nDeep Neural Network Metrics")
print(f"• Accuracy: {dnn_accuracy:.4f}")
print(f"• Precision: {dnn_precision:.4f}")
print(f"• Recall: {dnn_recall:.4f}")
print(f"• F1 Score: {dnn_f1:.4f}")
print(f"• ROC AUC: {dnn_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_dnn = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [dnn_accuracy, dnn_precision, dnn_recall, dnn_f1, dnn_roc_auc]
}
metrics_df_dnn = pd.DataFrame(metrics_data_dnn)
metrics_df_dnn.to_excel('dnn_metrics_results.xlsx', index=False)
print("\nMetrics saved to 'dnn_metrics_results.xlsx' for download.")

# Confusion Matrix
conf_matrix_dnn = confusion_matrix(y_test, y_test_dl_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - Deep Neural Network')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.savefig('confusion_matrix_dnn.png')
plt.show()

# ROC Curve
fpr_dnn = {}
tpr_dnn = {}
roc_auc_dnn = {}
classes_dnn = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_dnn):
    fpr_dnn[class_label], tpr_dnn[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])
    roc_auc_dnn[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_dnn:
    plt.plot(fpr_dnn[class_label], tpr_dnn[class_label], label=f'Class {class_label} (AUC = {roc_auc_dnn[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Deep Neural Network')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.savefig('roc_curve_dnn.png')
plt.show()

# Precision-Recall Curve
precision_dnn = {}
recall_dnn = {}

for i, class_label in enumerate(classes_dnn):
    precision_dnn[class_label], recall_dnn[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_dnn:
    plt.plot(recall_dnn[class_label], precision_dnn[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Deep Neural Network')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.savefig('precision_recall_curve_dnn.png')
plt.show()

print("\nPlots and metrics saved successfully.")

import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import pandas as pd

# Create a directory to save the plots if it doesn't exist
output_dir_rf = 'rf_output_plots'
os.makedirs(output_dir_rf, exist_ok=True)

# # Calculate Metrics
# rf_accuracy = accuracy_score(y_test, y_test_pred)
# rf_precision = precision_score(y_test, y_test_pred, average='weighted')
# rf_recall = recall_score(y_test, y_test_pred, average='weighted')
# rf_f1 = f1_score(y_test, y_test_pred, average='weighted')
# rf_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr')

# print("\nRandom Forest Classifier Metrics")
# print(f"• Accuracy: {rf_accuracy:.4f}")
# print(f"• Precision: {rf_precision:.4f}")
# print(f"• Recall: {rf_recall:.4f}")
# print(f"• F1 Score: {rf_f1:.4f}")
# print(f"• ROC AUC: {rf_roc_auc:.4f}")

# # Save Metrics to Excel
# metrics_data = {
#     'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
#     'Value': [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]
# }
# metrics_df = pd.DataFrame(metrics_data)
# metrics_file = os.path.join(output_dir_rf, 'rf_metrics_results.xlsx')
# metrics_df.to_excel(metrics_file, index=False)
# print(f"\nMetrics saved to '{metrics_file}' for download.")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
conf_matrix_file = os.path.join(output_dir_rf, 'confusion_matrix_rf.png')
plt.savefig(conf_matrix_file)
plt.show()

# ROC Curve
fpr = {}
tpr = {}
roc_auc = {}
classes = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes):
    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])
    roc_auc[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes:
    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Random Forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
roc_curve_file = os.path.join(output_dir_rf, 'roc_curve_rf.png')
plt.savefig(roc_curve_file)
plt.show()

# Precision-Recall Curve
precision = {}
recall = {}

for i, class_label in enumerate(classes):
    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes:
    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Random Forest')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
pr_curve_file = os.path.join(output_dir_rf, 'precision_recall_curve_rf.png')
plt.savefig(pr_curve_file)
plt.show()

print(f"All plots and metrics have been saved to the directory: {output_dir_rf}")

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    roc_curve, precision_recall_curve
)
import matplotlib.pyplot as plt

# Load DNN results
dnn_results_file = 'hyperparameter_tuning_results.xlsx'  # Update the file path if needed
dnn_results_df = pd.read_excel(dnn_results_file)

# Find the best DNN model configuration based on validation accuracy
best_dnn_result = dnn_results_df.loc[dnn_results_df['mean_val_accuracy'].idxmax()]
print("\nBest DNN Configuration Found:")
print(best_dnn_result)

# Load Random Forest results
rf_results_file = 'rf_model_results.xlsx'  # Update the file path if needed
rf_results_df = pd.read_excel(rf_results_file)

# Find the best Random Forest model configuration based on validation accuracy
best_rf_result = rf_results_df.loc[rf_results_df['Validation Accuracy'].idxmax()]
print("\nBest Random Forest Configuration Found:")
print(best_rf_result)

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# Load hyperparameter tuning results from the Excel file
results_file = 'hyperparameter_tuning_results.xlsx'  # Replace with your actual file path
results_df = pd.read_excel(results_file)

# Select the best configuration based on mean validation accuracy
best_result = results_df.loc[results_df['mean_val_accuracy'].idxmax()]
print("\nBest Deep Neural Network Configuration Found:")
print(best_result)

# # Extract hyperparameters from the best configuration
# hidden_layers = int(best_result['hidden_layers'])
# hidden_nodes = int(best_result['hidden_nodes'])
# learning_rate = best_result['learning_rate']
# optimizer = best_result['optimizer']
# activation = best_result['activation']

# # Create and train the best model with the selected configuration
# best_model = create_model(
#     hidden_layers=hidden_layers,
#     hidden_nodes=hidden_nodes,
#     input_shape=(X.shape[1],),
#     learning_rate=learning_rate,
#     optimizer=optimizer,
#     activation=activation
# )

# # Train the best model on the combined training and validation data
# history = best_model.fit(X_train_val, y_train_val, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)

# # Evaluate the best model on the test set


# Train the best model with optimal parameters on training and validation data
best_model = create_model(
    best_result['hidden_layers'],
    best_result['hidden_nodes'],
    (X.shape[1],),
    best_result['learning_rate'],
    best_result['optimizer'],
    best_result['activation']
)

best_model.fit(X_train_val, y_train_val, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)

# Evaluate the model on the test set
test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy with best configuration: {test_accuracy * 100:.2f}%")
y_test_dl_pred_proba = best_model.predict(X_test)
y_test_dl_pred = np.argmax(y_test_dl_pred_proba, axis=1)

# Calculate Metrics
dnn_accuracy = accuracy_score(y_test, y_test_dl_pred)
dnn_precision = precision_score(y_test, y_test_dl_pred, average='weighted')
dnn_recall = recall_score(y_test, y_test_dl_pred, average='weighted')
dnn_f1 = f1_score(y_test, y_test_dl_pred, average='weighted')
dnn_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_dl_pred_proba, multi_class='ovr')

# Print Metrics
print("\nDeep Neural Network Metrics")
print(f"• Accuracy: {dnn_accuracy:.4f}")
print(f"• Precision: {dnn_precision:.4f}")
print(f"• Recall: {dnn_recall:.4f}")
print(f"• F1 Score: {dnn_f1:.4f}")
print(f"• ROC AUC: {dnn_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_dnn = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [dnn_accuracy, dnn_precision, dnn_recall, dnn_f1, dnn_roc_auc]
}
metrics_df_dnn = pd.DataFrame(metrics_data_dnn)
metrics_df_dnn.to_excel('dnn_metrics_results.xlsx', index=False)
print("\nMetrics saved to 'dnn_metrics_results.xlsx' for download.")

# Confusion Matrix
conf_matrix_dnn = confusion_matrix(y_test, y_test_dl_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - Deep Neural Network')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# ROC Curve
fpr_dnn = {}
tpr_dnn = {}
roc_auc_dnn = {}
classes_dnn = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_dnn):
    fpr_dnn[class_label], tpr_dnn[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])
    roc_auc_dnn[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_dnn:
    plt.plot(fpr_dnn[class_label], tpr_dnn[class_label], label=f'Class {class_label} (AUC = {roc_auc_dnn[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Deep Neural Network')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Precision-Recall Curve
precision_dnn = {}
recall_dnn = {}

for i, class_label in enumerate(classes_dnn):
    precision_dnn[class_label], recall_dnn[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_dnn:
    plt.plot(recall_dnn[class_label], precision_dnn[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Deep Neural Network')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()



import os

# Create a directory to save the plots if it doesn't exist
output_dir = 'dnn_output_plots'
os.makedirs(output_dir, exist_ok=True)

# Confusion Matrix
conf_matrix_dnn = confusion_matrix(y_test, y_test_dl_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - Deep Neural Network')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.savefig(os.path.join(output_dir, 'confusion_matrix_dnn.png'))
plt.show()

# ROC Curve
plt.figure(figsize=(10, 8))
for class_label in classes_dnn:
    plt.plot(fpr_dnn[class_label], tpr_dnn[class_label], label=f'Class {class_label} (AUC = {roc_auc_dnn[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Deep Neural Network')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.savefig(os.path.join(output_dir, 'roc_curve_dnn.png'))
plt.show()

# Precision-Recall Curve
plt.figure(figsize=(10, 8))
for class_label in classes_dnn:
    plt.plot(recall_dnn[class_label], precision_dnn[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Deep Neural Network')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.savefig(os.path.join(output_dir, 'precision_recall_curve_dnn.png'))
plt.show()

print(f"All plots have been saved to the directory: {output_dir}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


# Define hyperparameters for tuning
n_neighbors_list = [3, 5, 7, 9]
weights_list = ['uniform', 'distance']
metric_list = ['euclidean', 'manhattan']

# Store results for comparison
knn_results = []

# Train and evaluate KNN
for n_neighbors in n_neighbors_list:
    for weights in weights_list:
        for metric in metric_list:
            # Initialize the KNN model
            knn_model = KNeighborsClassifier(
                n_neighbors=n_neighbors,
                weights=weights,
                metric=metric
            )

            # Train the model on the training set
            knn_model.fit(X_train_scaled, y_train)

            # Evaluate on the validation set
            y_val_pred = knn_model.predict(X_val_scaled)
            val_accuracy = accuracy_score(y_val, y_val_pred)

            # Store the results
            knn_results.append({
                'n_neighbors': n_neighbors,
                'weights': weights,
                'metric': metric,
                'Validation Accuracy': val_accuracy
            })

# Convert results to a DataFrame
knn_results_df = pd.DataFrame(knn_results)

# Find the best configuration based on validation accuracy
best_knn_result = knn_results_df.loc[knn_results_df['Validation Accuracy'].idxmax()]
print("\nBest KNN configuration found:")
print(best_knn_result)

# Train the final model with the best hyperparameters on combined training and validation data
best_knn_model = KNeighborsClassifier(
    n_neighbors=best_knn_result['n_neighbors'],
    weights=best_knn_result['weights'],
    metric=best_knn_result['metric']
)
best_knn_model.fit(scaler.transform(X_train_val), y_train_val)

# Evaluate the final model on the test set
y_test_pred = best_knn_model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"\nTest Accuracy with the best KNN configuration: {test_accuracy * 100:.2f}%")

# Save results to an Excel file
output_file = 'knn_model_results.xlsx'
knn_results_df.to_excel(output_file, index=False)
print(f"\nResults saved to '{output_file}' for download.")

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd

# Create a directory to save the plots if it doesn't exist
output_dir_knn = 'knn_output_plots'
os.makedirs(output_dir_knn, exist_ok=True)

# Calculate Metrics
knn_accuracy = accuracy_score(y_test, y_test_pred)
knn_precision = precision_score(y_test, y_test_pred, average='weighted')
knn_recall = recall_score(y_test, y_test_pred, average='weighted')
knn_f1 = f1_score(y_test, y_test_pred, average='weighted')
knn_roc_auc = roc_auc_score(pd.get_dummies(y_test), best_knn_model.predict_proba(X_test_scaled), multi_class='ovr')

print("\nKNN Metrics")
print(f"• Accuracy: {knn_accuracy:.4f}")
print(f"• Precision: {knn_precision:.4f}")
print(f"• Recall: {knn_recall:.4f}")
print(f"• F1 Score: {knn_f1:.4f}")
print(f"• ROC AUC: {knn_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_knn = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_roc_auc]
}
metrics_df_knn = pd.DataFrame(metrics_data_knn)
metrics_file = os.path.join(output_dir_knn, 'knn_metrics_results.xlsx')
metrics_df_knn.to_excel(metrics_file, index=False)
print(f"\nMetrics saved to '{metrics_file}'.")

# Confusion Matrix
conf_matrix_knn = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - KNN')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
conf_matrix_file = os.path.join(output_dir_knn, 'confusion_matrix_knn.png')
plt.savefig(conf_matrix_file)
plt.show()

# ROC Curve
fpr_knn = {}
tpr_knn = {}
roc_auc_knn = {}
classes_knn = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_knn):
    fpr_knn[class_label], tpr_knn[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], best_knn_model.predict_proba(X_test_scaled)[:, i])
    roc_auc_knn[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], best_knn_model.predict_proba(X_test_scaled)[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_knn:
    plt.plot(fpr_knn[class_label], tpr_knn[class_label], label=f'Class {class_label} (AUC = {roc_auc_knn[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - KNN')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
roc_curve_file = os.path.join(output_dir_knn, 'roc_curve_knn.png')
plt.savefig(roc_curve_file)
plt.show()

# Precision-Recall Curve
precision_knn = {}
recall_knn = {}

for i, class_label in enumerate(classes_knn):
    precision_knn[class_label], recall_knn[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], best_knn_model.predict_proba(X_test_scaled)[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_knn:
    plt.plot(recall_knn[class_label], precision_knn[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - KNN')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
pr_curve_file = os.path.join(output_dir_knn, 'precision_recall_curve_knn.png')
plt.savefig(pr_curve_file)
plt.show()

print(f"All plots and metrics have been saved to the directory: {output_dir_knn}")

from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd

# Feature scaling (Naive Bayes variants may require different scaling methods)
scalers = {'StandardScaler': StandardScaler(), 'MinMaxScaler': MinMaxScaler()}

# Store results for comparison
nb_results = []

# Define Naive Bayes variants
nb_variants = {
    'GaussianNB': GaussianNB()
}

# Train and evaluate Naive Bayes classifiers
for scaler_name, scaler in scalers.items():
    # Scale features
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    for variant_name, nb_model in nb_variants.items():
        # Train the model
        nb_model.fit(X_train_scaled, y_train)

        # Evaluate on the validation set
        y_val_pred = nb_model.predict(X_val_scaled)
        val_accuracy = accuracy_score(y_val, y_val_pred)

        # Store the results
        nb_results.append({
            'Scaler': scaler_name,
            'Variant': variant_name,
            'Validation Accuracy': val_accuracy
        })

# Convert results to a DataFrame
nb_results_df = pd.DataFrame(nb_results)

# Find the best configuration based on validation accuracy
best_nb_result = nb_results_df.loc[nb_results_df['Validation Accuracy'].idxmax()]
print("\nBest Naive Bayes configuration found:")
print(best_nb_result)

# Train the final model with the best configuration on combined training and validation data
best_scaler = scalers[best_nb_result['Scaler']]
X_train_val_scaled = best_scaler.fit_transform(X_train_val)
X_test_scaled = best_scaler.transform(X_test)

best_nb_model = nb_variants[best_nb_result['Variant']]
best_nb_model.fit(X_train_val_scaled, y_train_val)

# Evaluate the final model on the test set
y_test_pred = best_nb_model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"\nTest Accuracy with the best Naive Bayes configuration: {test_accuracy * 100:.2f}%")

# Save results to an Excel file
output_file = 'nb_model_results.xlsx'
nb_results_df.to_excel(output_file, index=False)
print(f"\nResults saved to '{output_file}' for download.")

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd

# Create a directory to save the plots if it doesn't exist
output_dir_bayes = 'bayes_output_plots'
os.makedirs(output_dir_bayes, exist_ok=True)

# Predictions for Naive Bayes
y_test_proba_bayes = best_nb_model.predict_proba(X_test_scaled)  # Predicted probabilities
y_test_pred_bayes = best_nb_model.predict(X_test_scaled)        # Predicted labels

# Calculate Metrics
bayes_accuracy = accuracy_score(y_test, y_test_pred_bayes)
bayes_precision = precision_score(y_test, y_test_pred_bayes, average='weighted')
bayes_recall = recall_score(y_test, y_test_pred_bayes, average='weighted')
bayes_f1 = f1_score(y_test, y_test_pred_bayes, average='weighted')
bayes_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba_bayes, multi_class='ovr')

print("\nBayesian Classifier Metrics")
print(f"• Accuracy: {bayes_accuracy:.4f}")
print(f"• Precision: {bayes_precision:.4f}")
print(f"• Recall: {bayes_recall:.4f}")
print(f"• F1 Score: {bayes_f1:.4f}")
print(f"• ROC AUC: {bayes_roc_auc:.4f}")

# Save Metrics to Excel
metrics_data_bayes = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],
    'Value': [bayes_accuracy, bayes_precision, bayes_recall, bayes_f1, bayes_roc_auc]
}
metrics_df_bayes = pd.DataFrame(metrics_data_bayes)
metrics_file_bayes = os.path.join(output_dir_bayes, 'bayes_metrics_results.xlsx')
metrics_df_bayes.to_excel(metrics_file_bayes, index=False)
print(f"\nMetrics saved to '{metrics_file_bayes}'.")

# Confusion Matrix
conf_matrix_bayes = confusion_matrix(y_test, y_test_pred_bayes)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_bayes, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix - Bayesian Classifier')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
conf_matrix_file_bayes = os.path.join(output_dir_bayes, 'confusion_matrix_bayes.png')
plt.savefig(conf_matrix_file_bayes)
plt.show()

# ROC Curve
fpr_bayes = {}
tpr_bayes = {}
roc_auc_bayes = {}
classes_bayes = pd.get_dummies(y_test).columns

for i, class_label in enumerate(classes_bayes):
    fpr_bayes[class_label], tpr_bayes[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_bayes[:, i])
    roc_auc_bayes[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba_bayes[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_bayes:
    plt.plot(fpr_bayes[class_label], tpr_bayes[class_label], label=f'Class {class_label} (AUC = {roc_auc_bayes[class_label]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title('ROC Curve - Bayesian Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
roc_curve_file_bayes = os.path.join(output_dir_bayes, 'roc_curve_bayes.png')
plt.savefig(roc_curve_file_bayes)
plt.show()

# Precision-Recall Curve
precision_bayes = {}
recall_bayes = {}

for i, class_label in enumerate(classes_bayes):
    precision_bayes[class_label], recall_bayes[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_bayes[:, i])

plt.figure(figsize=(10, 8))
for class_label in classes_bayes:
    plt.plot(recall_bayes[class_label], precision_bayes[class_label], label=f'Class {class_label}')
plt.title('Precision-Recall Curve - Bayesian Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
pr_curve_file_bayes = os.path.join(output_dir_bayes, 'precision_recall_curve_bayes.png')
plt.savefig(pr_curve_file_bayes)
plt.show()

print(f"All plots and metrics have been saved to the directory: {output_dir_bayes}")

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd

# Assuming you already have X and y loaded in your environment
# Example: X = data.drop('Label', axis=1); y = data['Label']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the base estimator
base_estimator = DecisionTreeClassifier(max_depth=10)

# Create bagging classifier
bagging_model = BaggingClassifier(estimator=base_estimator, n_estimators=50, random_state=1)

# Train the model
bagging_model.fit(X_train_scaled, y_train)

# Evaluate the model on the test set
y_test_pred = bagging_model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Bagging Test Accuracy: {test_accuracy * 100:.2f}%")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Define the Extreme Learning Machine class
class ExtremeLearningMachine:
    def __init__(self, input_size, hidden_size, activation_function='sigmoid'):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.activation_function = activation_function
        self.input_weights = np.random.normal(size=(self.input_size, self.hidden_size))
        self.biases = np.random.normal(size=(self.hidden_size,))
        self.output_weights = None

    def _activation(self, x):
        if self.activation_function == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation_function == 'tanh':
            return np.tanh(x)
        elif self.activation_function == 'relu':
            return np.maximum(0, x)
        else:
            raise ValueError("Unsupported activation function. Choose 'sigmoid', 'tanh', or 'relu'.")

    def fit(self, X, y):
        # Compute hidden layer outputs
        H = self._activation(np.dot(X, self.input_weights) + self.biases)

        # Compute output weights using Moore-Penrose pseudoinverse
        self.output_weights = np.dot(np.linalg.pinv(H), y)

    def predict(self, X):
        # Compute hidden layer outputs
        H = self._activation(np.dot(X, self.input_weights) + self.biases)

        # Compute predictions
        output = np.dot(H, self.output_weights)
        return np.argmax(output, axis=1)

# Load your preprocessed dataset
# Assuming 'data' is your preprocessed DataFrame
X = data.drop('Label', axis=1).values
y = data['Label'].values

# One-hot encode the labels
num_classes = len(np.unique(y))
y_one_hot = np.zeros((len(y), num_classes))
y_one_hot[np.arange(len(y)), y] = 1

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the ELM
input_size = X_train_scaled.shape[1]
hidden_size = 100  # Number of hidden nodes (hyperparameter)
elm = ExtremeLearningMachine(input_size=input_size, hidden_size=hidden_size, activation_function='relu')
elm.fit(X_train_scaled, y_train)

# Predict on the test set
y_test_pred = elm.predict(X_test_scaled)
y_test_true = np.argmax(y_test, axis=1)

# Evaluate accuracy
test_accuracy = accuracy_score(y_test_true, y_test_pred)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Random Forest results from the Excel file
results_file = 'rf_model_results.xlsx'  # Update the file path if needed
rf_results_df = pd.read_excel(results_file)

# Plot 1: Validation Accuracy vs. Number of Estimators
plt.figure(figsize=(12, 6))
sns.barplot(
    data=rf_results_df,
    x='n_estimators',
    y='Validation Accuracy',
    hue='max_depth',
    palette='viridis'
)
plt.title('Validation Accuracy for Different n_estimators and max_depth')
plt.xlabel('Number of Estimators')
plt.ylabel('Validation Accuracy')
plt.legend(title='Max Depth')
plt.tight_layout()
plt.show()

# Plot 2: Validation Accuracy vs. Max Depth
plt.figure(figsize=(10, 6))
sns.barplot(
    data=rf_results_df,
    x='max_depth',
    y='Validation Accuracy',
    hue='criterion',
    palette='plasma'
)
plt.title('Validation Accuracy by Max Depth and Criterion')
plt.xlabel('Max Depth')
plt.ylabel('Validation Accuracy')
plt.legend(title='Criterion')
plt.tight_layout()
plt.show()

# Plot 3: Validation Accuracy vs. Criterion
plt.figure(figsize=(10, 6))
sns.boxplot(
    data=rf_results_df,
    x='criterion',
    y='Validation Accuracy',
    palette='coolwarm'
)
plt.title('Validation Accuracy Distribution by Criterion')
plt.xlabel('Criterion')
plt.ylabel('Validation Accuracy')
plt.tight_layout()
plt.show()

# Plot 4: Overall Mean Validation Accuracy Distribution
plt.figure(figsize=(12, 6))
sns.boxplot(
    data=rf_results_df,
    x='max_depth',
    y='Validation Accuracy',
    hue='n_estimators',
    palette='Set2'
)
plt.title('Validation Accuracy Distribution by Max Depth and n_estimators')
plt.xlabel('Max Depth')
plt.ylabel('Validation Accuracy')
plt.legend(title='Number of Estimators')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Random Forest results from the Excel file
results_file = 'rf_model_results.xlsx'  # Update the file path if needed
rf_results_df = pd.read_excel(results_file)

# Plot 1: Validation Accuracy vs. Number of Estimators for Different Max Depths
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=rf_results_df,
    x='n_estimators',
    y='Validation Accuracy',
    hue='max_depth',
    style='criterion',
    markers=True,
    dashes=False,
    palette='viridis'
)
plt.title('Validation Accuracy vs. Number of Estimators for Different Max Depths')
plt.xlabel('Number of Estimators')
plt.ylabel('Validation Accuracy')
plt.legend(title='Max Depth / Criterion', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot 2: Validation Accuracy vs. Max Depth for Different Criteria
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=rf_results_df,
    x='max_depth',
    y='Validation Accuracy',
    hue='criterion',
    style='n_estimators',
    markers=True,
    dashes=False,
    palette='plasma'
)
plt.title('Validation Accuracy vs. Max Depth for Different Criteria')
plt.xlabel('Max Depth')
plt.ylabel('Validation Accuracy')
plt.legend(title='Criterion / Estimators', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot 3: Validation Accuracy vs. Criterion for Different Estimators and Max Depth
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=rf_results_df,
    x='criterion',
    y='Validation Accuracy',
    hue='n_estimators',
    style='max_depth',
    markers=True,
    dashes=False,
    palette='coolwarm'
)
plt.title('Validation Accuracy vs. Criterion for Different Estimators and Max Depth')
plt.xlabel('Criterion')
plt.ylabel('Validation Accuracy')
plt.legend(title='Estimators / Max Depth', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Define features and target
X = data.drop(columns=['Label'])  # Use all columns except 'Label'
y = data['Label']

# Map string labels to integers if needed
if y.dtype == 'object':
    label_mapping = {label: idx for idx, label in enumerate(y.unique())}
    y = y.map(label_mapping)

# Flatten columns with sequences by extracting statistical features
def flatten_sequences(df):
    for column in df.columns:
        if df[column].apply(lambda x: isinstance(x, (list, np.ndarray))).any():
            df[column + '_mean'] = df[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)
            df[column + '_std'] = df[column].apply(lambda x: np.std(x) if isinstance(x, (list, np.ndarray)) else 0)
            df[column + '_median'] = df[column].apply(lambda x: np.median(x) if isinstance(x, (list, np.ndarray)) else x)
            df.drop(column, axis=1, inplace=True)

# Apply the flattening function to the features
flatten_sequences(X)

# Convert DataFrame to numpy array
X = X.values

# Split the data into 70% training, 20% validation, and 10% testing
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.1, random_state=1
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2222, random_state=1  # 0.2222 of 90% to get 20% validation
)

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Define the SVM model with RBF kernel
svm_model = SVC(kernel='rbf', random_state=1)

# Define hyperparameter ranges for grid search
param_grid = {
    'C': [ 1, 10, 100],  # Regularization parameter
    'gamma': [0.001, 0.01, 0.1]  # RBF kernel coefficient
}

# Perform grid search with cross-validation on training + validation sets
grid_search = GridSearchCV(
    estimator=svm_model,
    param_grid=param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

# Fit the grid search model
grid_search.fit(X_train_scaled, y_train)

# Get the best hyperparameters and validation accuracy
best_params = grid_search.best_params_
best_val_accuracy = grid_search.best_score_

print("\nBest Hyperparameters found:")
print(f"C: {best_params['C']}, Gamma: {best_params['gamma']}")
print(f"Best Validation Accuracy: {best_val_accuracy * 100:.2f}%")

# Train the final model with the best hyperparameters on combined training + validation data
final_svm_model = SVC(
    kernel='rbf',
    C=best_params['C'],
    gamma=best_params['gamma'],
    random_state=1
)
final_svm_model.fit(scaler.transform(X_train_val), y_train_val)

# Evaluate the final model on the test set
y_test_pred = final_svm_model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"\nTest Accuracy with the best configuration: {test_accuracy * 100:.2f}%")
print("\nClassification Report on Test Set:")
print(classification_report(y_test, y_test_pred))
