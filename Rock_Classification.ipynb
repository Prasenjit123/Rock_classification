{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c22f7-8136-4613-b1df-46fecab06fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Admin\\torchgpu-env\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba812da1-4655-47fb-a559-906497e62582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pixel Value  Brightness  Blur  Sobel  Gaussians_Difference   Label\n",
      "0          136         136   116      0                   138  Benign\n",
      "1          132         137   116      1                   137  Benign\n",
      "2          140         138   116     27                   139  Benign\n",
      "3          148         138   115      6                   140  Benign\n",
      "4          140         138   114      0                   138  Benign\n",
      "   Pixel Value  Brightness   Label\n",
      "0          136         136  Benign\n",
      "1          132         137  Benign\n",
      "2          140         138  Benign\n",
      "3          148         138  Benign\n",
      "4          140         138  Benign\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 786432 entries, 0 to 786431\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Pixel Value  786432 non-null  int64 \n",
      " 1   Brightness   786432 non-null  int64 \n",
      " 2   Label        786432 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter, sobel\n",
    "from skimage.feature import hessian_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'BSE_image_extracted_data.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(data.head())\n",
    "data = data.drop(['Blur', 'Sobel', 'Gaussians_Difference'], axis=1)\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0efdc-89a7-47c6-9955-05f155bf8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pixel Value  Brightness  Blur  Sobel  Gaussians_Difference   Label\n",
      "0          136         136   116      0                   138  Benign\n",
      "1          132         137   116      1                   137  Benign\n",
      "2          140         138   116     27                   139  Benign\n",
      "3          148         138   115      6                   140  Benign\n",
      "4          140         138   114      0                   138  Benign\n",
      "   Pixel Value  Brightness   Label\n",
      "0          136         136  Benign\n",
      "1          132         137  Benign\n",
      "2          140         138  Benign\n",
      "3          148         138  Benign\n",
      "4          140         138  Benign\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 786432 entries, 0 to 786431\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Pixel Value  786432 non-null  int64 \n",
      " 1   Brightness   786432 non-null  int64 \n",
      " 2   Label        786432 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter, sobel\n",
    "from skimage.feature import hessian_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'BSE_image_extracted_data.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(data.head())\n",
    "data = data.drop(['Blur', 'Sobel', 'Gaussians_Difference'], axis=1)\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf536b6c-45e1-4934-8a44-f92c9af89e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pixel Value  Brightness   Label Gaussian_sigma_1 Gaussian_sigma_2  \\\n",
      "0          136         136  Benign            [136]            [136]   \n",
      "1          132         137  Benign            [132]            [132]   \n",
      "2          140         138  Benign            [140]            [140]   \n",
      "3          148         138  Benign            [148]            [148]   \n",
      "4          140         138  Benign            [140]            [140]   \n",
      "\n",
      "  Gaussian_sigma_4 Gaussian_sigma_8 Gaussian_sigma_16 Gaussian_sigma_32  \\\n",
      "0            [136]            [136]             [136]             [136]   \n",
      "1            [132]            [132]             [132]             [132]   \n",
      "2            [140]            [140]             [140]             [140]   \n",
      "3            [148]            [148]             [148]             [148]   \n",
      "4            [140]            [140]             [140]             [140]   \n",
      "\n",
      "  DoG_1_2  ...                Conv_1x1  \\\n",
      "0     [0]  ...  [136.0, 0.0, 0.0, 0.0]   \n",
      "1     [0]  ...  [132.0, 0.0, 0.0, 0.0]   \n",
      "2     [0]  ...  [140.0, 0.0, 0.0, 0.0]   \n",
      "3     [0]  ...  [148.0, 0.0, 0.0, 0.0]   \n",
      "4     [0]  ...  [140.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "                                            Conv_3x3  \\\n",
      "0   [45.33333367109299, 45.33333367109299, 0.0, 0.0]   \n",
      "1  [44.000000327825546, 44.000000327825546, 0.0, ...   \n",
      "2   [46.66666701436043, 46.66666701436043, 0.0, 0.0]   \n",
      "3   [49.33333370089531, 49.33333370089531, 0.0, 0.0]   \n",
      "4   [46.66666701436043, 46.66666701436043, 0.0, 0.0]   \n",
      "\n",
      "                                            Conv_5x5  \\\n",
      "0  [27.199999392032623, 27.199999392032623, 27.19...   \n",
      "1  [26.399999409914017, 26.399999409914017, 26.39...   \n",
      "2  [27.99999937415123, 27.99999937415123, 27.9999...   \n",
      "3  [29.599999338388443, 29.599999338388443, 29.59...   \n",
      "4  [27.99999937415123, 27.99999937415123, 27.9999...   \n",
      "\n",
      "                                            Conv_7x7  \\\n",
      "0  [19.42857103049755, 19.42857103049755, 19.4285...   \n",
      "1  [18.857142470777035, 18.857142470777035, 18.85...   \n",
      "2  [19.999999590218067, 19.999999590218067, 19.99...   \n",
      "3  [21.1428567096591, 21.1428567096591, 21.142856...   \n",
      "4  [19.999999590218067, 19.999999590218067, 19.99...   \n",
      "\n",
      "                                            Conv_9x9  \\\n",
      "0  [15.111111223697662, 15.111111223697662, 30.22...   \n",
      "1  [14.666666775941849, 14.666666775941849, 29.33...   \n",
      "2  [15.555555671453476, 15.555555671453476, 31.11...   \n",
      "3  [16.444444566965103, 16.444444566965103, 32.88...   \n",
      "4  [15.555555671453476, 15.555555671453476, 31.11...   \n",
      "\n",
      "                                          Conv_11x11  \\\n",
      "0  [12.363635845482353, 24.7272716909647, 24.7272...   \n",
      "1  [11.99999949708581, 23.99999899417162, 23.9999...   \n",
      "2  [12.727272193878893, 25.454544387757778, 25.45...   \n",
      "3  [13.45454489067197, 26.909089781343937, 26.909...   \n",
      "4  [12.727272193878893, 25.454544387757778, 25.45...   \n",
      "\n",
      "                                          Conv_13x13  \\\n",
      "0  [31.384615793824196, 20.923077195882797, 20.92...   \n",
      "1  [30.46153885871172, 20.30769257247448, 20.3076...   \n",
      "2  [32.30769272893667, 21.538461819291115, 21.538...   \n",
      "3  [34.153846599161625, 22.76923106610775, 22.769...   \n",
      "4  [32.30769272893667, 21.538461819291115, 21.538...   \n",
      "\n",
      "                                          Conv_15x15  \\\n",
      "0  [27.200000658631325, 27.200000658631325, 18.13...   \n",
      "1  [26.400000639259815, 26.400000639259815, 17.60...   \n",
      "2  [28.000000678002834, 28.000000678002834, 18.66...   \n",
      "3  [29.600000716745853, 29.600000716745853, 19.73...   \n",
      "4  [28.000000678002834, 28.000000678002834, 18.66...   \n",
      "\n",
      "                                          Conv_17x17  \\\n",
      "0  [24.000000754371285, 24.000000754371285, 24.00...   \n",
      "1  [23.294118379242718, 23.294118379242718, 23.29...   \n",
      "2  [24.705883129499853, 24.705883129499853, 24.70...   \n",
      "3  [26.117647879756987, 26.117647879756987, 26.11...   \n",
      "4  [24.705883129499853, 24.705883129499853, 24.70...   \n",
      "\n",
      "                                          Conv_19x19  \n",
      "0  [21.47368361055851, 21.47368361055851, 21.4736...  \n",
      "1  [20.8421046808362, 20.8421046808362, 20.842104...  \n",
      "2  [22.10526254028082, 22.10526254028082, 22.1052...  \n",
      "3  [23.368420399725437, 23.368420399725437, 23.36...  \n",
      "4  [22.10526254028082, 22.10526254028082, 22.1052...  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 786432 entries, 0 to 786431\n",
      "Data columns (total 36 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Pixel Value        786432 non-null  int64 \n",
      " 1   Brightness         786432 non-null  int64 \n",
      " 2   Label              786432 non-null  object\n",
      " 3   Gaussian_sigma_1   786432 non-null  object\n",
      " 4   Gaussian_sigma_2   786432 non-null  object\n",
      " 5   Gaussian_sigma_4   786432 non-null  object\n",
      " 6   Gaussian_sigma_8   786432 non-null  object\n",
      " 7   Gaussian_sigma_16  786432 non-null  object\n",
      " 8   Gaussian_sigma_32  786432 non-null  object\n",
      " 9   DoG_1_2            786432 non-null  object\n",
      " 10  DoG_2_4            786432 non-null  object\n",
      " 11  DoG_4_8            786432 non-null  object\n",
      " 12  DoG_8_16           786432 non-null  object\n",
      " 13  DoG_16_32          786432 non-null  object\n",
      " 14  Sobel              786432 non-null  object\n",
      " 15  Hessian            786432 non-null  object\n",
      " 16  Prewitt            786432 non-null  object\n",
      " 17  Scharr             786432 non-null  object\n",
      " 18  Median_3           786432 non-null  object\n",
      " 19  Median_5           786432 non-null  object\n",
      " 20  Median_7           786432 non-null  object\n",
      " 21  Median_9           786432 non-null  object\n",
      " 22  Median_11          786432 non-null  object\n",
      " 23  Blur               786432 non-null  object\n",
      " 24  Sharpen            786432 non-null  object\n",
      " 25  Box_Blur           786432 non-null  object\n",
      " 26  Conv_1x1           786432 non-null  object\n",
      " 27  Conv_3x3           786432 non-null  object\n",
      " 28  Conv_5x5           786432 non-null  object\n",
      " 29  Conv_7x7           786432 non-null  object\n",
      " 30  Conv_9x9           786432 non-null  object\n",
      " 31  Conv_11x11         786432 non-null  object\n",
      " 32  Conv_13x13         786432 non-null  object\n",
      " 33  Conv_15x15         786432 non-null  object\n",
      " 34  Conv_17x17         786432 non-null  object\n",
      " 35  Conv_19x19         786432 non-null  object\n",
      "dtypes: int64(2), object(34)\n",
      "memory usage: 216.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "\n",
    "def convert_pixel_values_to_image(data_row):\n",
    "    pixel_values = data_row.split(',')  # Adjust this line if the pixel values are stored differently\n",
    "    image_array = np.array([int(x) for x in pixel_values]).reshape(768, 1024)\n",
    "    return image_array\n",
    "\n",
    "if isinstance(data['Pixel Value'][0], str):\n",
    "    data['Pixel Value'] = data['Pixel Value'].apply(convert_pixel_values_to_image)\n",
    "\n",
    "def apply_gaussian_filter(image_array, sigma):\n",
    "    return gaussian_filter(image_array, sigma=sigma).flatten()\n",
    "\n",
    "def apply_dog_filter(image_array, sigma1, sigma2):\n",
    "    gaussian1 = gaussian_filter(image_array, sigma=sigma1)\n",
    "    gaussian2 = gaussian_filter(image_array, sigma=sigma2)\n",
    "    return (gaussian1 - gaussian2).flatten()\n",
    "\n",
    "def apply_sobel_filter(image_array):\n",
    "    sobelx = cv2.Sobel(image_array, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(image_array, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    return np.sqrt(sobelx**2 + sobely**2).flatten()\n",
    "\n",
    "def apply_hessian_filter(image_array):\n",
    "    return cv2.Laplacian(image_array, cv2.CV_64F).flatten()\n",
    "\n",
    "def apply_prewitt_filter(image_array):\n",
    "    prewittx = cv2.filter2D(image_array, -1, np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]))\n",
    "    prewitty = cv2.filter2D(image_array, -1, np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]))\n",
    "    return np.sqrt(prewittx**2 + prewitty**2).flatten()\n",
    "\n",
    "def apply_scharr_filter(image_array):\n",
    "    scharrx = cv2.Scharr(image_array, cv2.CV_64F, 1, 0)\n",
    "    scharry = cv2.Scharr(image_array, cv2.CV_64F, 0, 1)\n",
    "    return np.sqrt(scharrx**2 + scharry**2).flatten()\n",
    "\n",
    "def apply_median_filter(image_array, size):\n",
    "    return median_filter(image_array, size=size).flatten()\n",
    "\n",
    "def apply_blur_filter(image_array):\n",
    "    return cv2.blur(image_array, (5, 5)).flatten()\n",
    "\n",
    "def apply_sharpen_filter(image_array):\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image_array, -1, kernel).flatten()\n",
    "\n",
    "def apply_box_blur_filter(image_array):\n",
    "    kernel = np.ones((5,5), np.float32) / 25\n",
    "    return cv2.filter2D(image_array, -1, kernel).flatten()\n",
    "\n",
    "def apply_convolution_filter(image_array, kernel_size):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)\n",
    "    return cv2.filter2D(image_array, -1, kernel).flatten()\n",
    "\n",
    "def apply_filters_to_data(df):\n",
    "    df['Gaussian_sigma_1'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 1))\n",
    "    df['Gaussian_sigma_2'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 2))\n",
    "    df['Gaussian_sigma_4'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 4))\n",
    "    df['Gaussian_sigma_8'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 8))\n",
    "    df['Gaussian_sigma_16'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 16))\n",
    "    df['Gaussian_sigma_32'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 32))\n",
    "\n",
    "    df['DoG_1_2'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 1, 2))\n",
    "    df['DoG_2_4'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 2, 4))\n",
    "    df['DoG_4_8'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 4, 8))\n",
    "    df['DoG_8_16'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 8, 16))\n",
    "    df['DoG_16_32'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 16, 32))\n",
    "\n",
    "    df['Sobel'] = df['Pixel Value'].apply(lambda x: apply_sobel_filter(x))\n",
    "    df['Hessian'] = df['Pixel Value'].apply(lambda x: apply_hessian_filter(x))\n",
    "    df['Prewitt'] = df['Pixel Value'].apply(lambda x: apply_prewitt_filter(x))\n",
    "    df['Scharr'] = df['Pixel Value'].apply(lambda x: apply_scharr_filter(x))\n",
    "\n",
    "    df['Median_3'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 3))\n",
    "    df['Median_5'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 5))\n",
    "    df['Median_7'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 7))\n",
    "    df['Median_9'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 9))\n",
    "    df['Median_11'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 11))\n",
    "\n",
    "    df['Blur'] = df['Pixel Value'].apply(lambda x: apply_blur_filter(x))\n",
    "    df['Sharpen'] = df['Pixel Value'].apply(lambda x: apply_sharpen_filter(x))\n",
    "    df['Box_Blur'] = df['Pixel Value'].apply(lambda x: apply_box_blur_filter(x))\n",
    "\n",
    "    df['Conv_1x1'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 1))\n",
    "    df['Conv_3x3'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 3))\n",
    "    df['Conv_5x5'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 5))\n",
    "    df['Conv_7x7'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 7))\n",
    "    df['Conv_9x9'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 9))\n",
    "    df['Conv_11x11'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 11))\n",
    "    df['Conv_13x13'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 13))\n",
    "    df['Conv_15x15'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 15))\n",
    "    df['Conv_17x17'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 17))\n",
    "    df['Conv_19x19'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 19))\n",
    "\n",
    "# Apply filters\n",
    "apply_filters_to_data(data)\n",
    "print(data.head())\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cb54410-79e9-4271-9024-bfc1261df116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pixel Value  Brightness   Label Gaussian_sigma_1 Gaussian_sigma_2  \\\n",
      "0          136         136  Benign            [136]            [136]   \n",
      "1          132         137  Benign            [132]            [132]   \n",
      "2          140         138  Benign            [140]            [140]   \n",
      "3          148         138  Benign            [148]            [148]   \n",
      "4          140         138  Benign            [140]            [140]   \n",
      "\n",
      "  Gaussian_sigma_4 Gaussian_sigma_8 Gaussian_sigma_16 Gaussian_sigma_32  \\\n",
      "0            [136]            [136]             [136]             [136]   \n",
      "1            [132]            [132]             [132]             [132]   \n",
      "2            [140]            [140]             [140]             [140]   \n",
      "3            [148]            [148]             [148]             [148]   \n",
      "4            [140]            [140]             [140]             [140]   \n",
      "\n",
      "  DoG_1_2  ...                Conv_1x1  \\\n",
      "0     [0]  ...  [136.0, 0.0, 0.0, 0.0]   \n",
      "1     [0]  ...  [132.0, 0.0, 0.0, 0.0]   \n",
      "2     [0]  ...  [140.0, 0.0, 0.0, 0.0]   \n",
      "3     [0]  ...  [148.0, 0.0, 0.0, 0.0]   \n",
      "4     [0]  ...  [140.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "                                            Conv_3x3  \\\n",
      "0   [45.33333367109299, 45.33333367109299, 0.0, 0.0]   \n",
      "1  [44.000000327825546, 44.000000327825546, 0.0, ...   \n",
      "2   [46.66666701436043, 46.66666701436043, 0.0, 0.0]   \n",
      "3   [49.33333370089531, 49.33333370089531, 0.0, 0.0]   \n",
      "4   [46.66666701436043, 46.66666701436043, 0.0, 0.0]   \n",
      "\n",
      "                                            Conv_5x5  \\\n",
      "0  [27.199999392032623, 27.199999392032623, 27.19...   \n",
      "1  [26.399999409914017, 26.399999409914017, 26.39...   \n",
      "2  [27.99999937415123, 27.99999937415123, 27.9999...   \n",
      "3  [29.599999338388443, 29.599999338388443, 29.59...   \n",
      "4  [27.99999937415123, 27.99999937415123, 27.9999...   \n",
      "\n",
      "                                            Conv_7x7  \\\n",
      "0  [19.42857103049755, 19.42857103049755, 19.4285...   \n",
      "1  [18.857142470777035, 18.857142470777035, 18.85...   \n",
      "2  [19.999999590218067, 19.999999590218067, 19.99...   \n",
      "3  [21.1428567096591, 21.1428567096591, 21.142856...   \n",
      "4  [19.999999590218067, 19.999999590218067, 19.99...   \n",
      "\n",
      "                                            Conv_9x9  \\\n",
      "0  [15.111111223697662, 15.111111223697662, 30.22...   \n",
      "1  [14.666666775941849, 14.666666775941849, 29.33...   \n",
      "2  [15.555555671453476, 15.555555671453476, 31.11...   \n",
      "3  [16.444444566965103, 16.444444566965103, 32.88...   \n",
      "4  [15.555555671453476, 15.555555671453476, 31.11...   \n",
      "\n",
      "                                          Conv_11x11  \\\n",
      "0  [12.363635845482353, 24.7272716909647, 24.7272...   \n",
      "1  [11.99999949708581, 23.99999899417162, 23.9999...   \n",
      "2  [12.727272193878893, 25.454544387757778, 25.45...   \n",
      "3  [13.45454489067197, 26.909089781343937, 26.909...   \n",
      "4  [12.727272193878893, 25.454544387757778, 25.45...   \n",
      "\n",
      "                                          Conv_13x13  \\\n",
      "0  [31.384615793824196, 20.923077195882797, 20.92...   \n",
      "1  [30.46153885871172, 20.30769257247448, 20.3076...   \n",
      "2  [32.30769272893667, 21.538461819291115, 21.538...   \n",
      "3  [34.153846599161625, 22.76923106610775, 22.769...   \n",
      "4  [32.30769272893667, 21.538461819291115, 21.538...   \n",
      "\n",
      "                                          Conv_15x15  \\\n",
      "0  [27.200000658631325, 27.200000658631325, 18.13...   \n",
      "1  [26.400000639259815, 26.400000639259815, 17.60...   \n",
      "2  [28.000000678002834, 28.000000678002834, 18.66...   \n",
      "3  [29.600000716745853, 29.600000716745853, 19.73...   \n",
      "4  [28.000000678002834, 28.000000678002834, 18.66...   \n",
      "\n",
      "                                          Conv_17x17  \\\n",
      "0  [24.000000754371285, 24.000000754371285, 24.00...   \n",
      "1  [23.294118379242718, 23.294118379242718, 23.29...   \n",
      "2  [24.705883129499853, 24.705883129499853, 24.70...   \n",
      "3  [26.117647879756987, 26.117647879756987, 26.11...   \n",
      "4  [24.705883129499853, 24.705883129499853, 24.70...   \n",
      "\n",
      "                                          Conv_19x19  \n",
      "0  [21.47368361055851, 21.47368361055851, 21.4736...  \n",
      "1  [20.8421046808362, 20.8421046808362, 20.842104...  \n",
      "2  [22.10526254028082, 22.10526254028082, 22.1052...  \n",
      "3  [23.368420399725437, 23.368420399725437, 23.36...  \n",
      "4  [22.10526254028082, 22.10526254028082, 22.1052...  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 786432 entries, 0 to 786431\n",
      "Data columns (total 36 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Pixel Value        786432 non-null  int64 \n",
      " 1   Brightness         786432 non-null  int64 \n",
      " 2   Label              786432 non-null  object\n",
      " 3   Gaussian_sigma_1   786432 non-null  object\n",
      " 4   Gaussian_sigma_2   786432 non-null  object\n",
      " 5   Gaussian_sigma_4   786432 non-null  object\n",
      " 6   Gaussian_sigma_8   786432 non-null  object\n",
      " 7   Gaussian_sigma_16  786432 non-null  object\n",
      " 8   Gaussian_sigma_32  786432 non-null  object\n",
      " 9   DoG_1_2            786432 non-null  object\n",
      " 10  DoG_2_4            786432 non-null  object\n",
      " 11  DoG_4_8            786432 non-null  object\n",
      " 12  DoG_8_16           786432 non-null  object\n",
      " 13  DoG_16_32          786432 non-null  object\n",
      " 14  Sobel              786432 non-null  object\n",
      " 15  Hessian            786432 non-null  object\n",
      " 16  Prewitt            786432 non-null  object\n",
      " 17  Scharr             786432 non-null  object\n",
      " 18  Median_3           786432 non-null  object\n",
      " 19  Median_5           786432 non-null  object\n",
      " 20  Median_7           786432 non-null  object\n",
      " 21  Median_9           786432 non-null  object\n",
      " 22  Median_11          786432 non-null  object\n",
      " 23  Blur               786432 non-null  object\n",
      " 24  Sharpen            786432 non-null  object\n",
      " 25  Box_Blur           786432 non-null  object\n",
      " 26  Conv_1x1           786432 non-null  object\n",
      " 27  Conv_3x3           786432 non-null  object\n",
      " 28  Conv_5x5           786432 non-null  object\n",
      " 29  Conv_7x7           786432 non-null  object\n",
      " 30  Conv_9x9           786432 non-null  object\n",
      " 31  Conv_11x11         786432 non-null  object\n",
      " 32  Conv_13x13         786432 non-null  object\n",
      " 33  Conv_15x15         786432 non-null  object\n",
      " 34  Conv_17x17         786432 non-null  object\n",
      " 35  Conv_19x19         786432 non-null  object\n",
      "dtypes: int64(2), object(34)\n",
      "memory usage: 216.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "\n",
    "def convert_pixel_values_to_image(data_row):\n",
    "    pixel_values = data_row.split(',')  # Adjust this line if the pixel values are stored differently\n",
    "    image_array = np.array([int(x) for x in pixel_values]).reshape(768, 1024)\n",
    "    return image_array\n",
    "\n",
    "if isinstance(data['Pixel Value'][0], str):\n",
    "    data['Pixel Value'] = data['Pixel Value'].apply(convert_pixel_values_to_image)\n",
    "\n",
    "def apply_gaussian_filter(image_array, sigma):\n",
    "    return gaussian_filter(image_array, sigma=sigma).flatten()\n",
    "\n",
    "def apply_dog_filter(image_array, sigma1, sigma2):\n",
    "    gaussian1 = gaussian_filter(image_array, sigma=sigma1)\n",
    "    gaussian2 = gaussian_filter(image_array, sigma=sigma2)\n",
    "    return (gaussian1 - gaussian2).flatten()\n",
    "\n",
    "def apply_sobel_filter(image_array):\n",
    "    sobelx = cv2.Sobel(image_array, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(image_array, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    return np.sqrt(sobelx**2 + sobely**2).flatten()\n",
    "\n",
    "def apply_hessian_filter(image_array):\n",
    "    return cv2.Laplacian(image_array, cv2.CV_64F).flatten()\n",
    "\n",
    "def apply_prewitt_filter(image_array):\n",
    "    prewittx = cv2.filter2D(image_array, -1, np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]))\n",
    "    prewitty = cv2.filter2D(image_array, -1, np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]))\n",
    "    return np.sqrt(prewittx**2 + prewitty**2).flatten()\n",
    "\n",
    "def apply_scharr_filter(image_array):\n",
    "    scharrx = cv2.Scharr(image_array, cv2.CV_64F, 1, 0)\n",
    "    scharry = cv2.Scharr(image_array, cv2.CV_64F, 0, 1)\n",
    "    return np.sqrt(scharrx**2 + scharry**2).flatten()\n",
    "\n",
    "def apply_median_filter(image_array, size):\n",
    "    return median_filter(image_array, size=size).flatten()\n",
    "\n",
    "def apply_blur_filter(image_array):\n",
    "    return cv2.blur(image_array, (5, 5)).flatten()\n",
    "\n",
    "def apply_sharpen_filter(image_array):\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image_array, -1, kernel).flatten()\n",
    "\n",
    "def apply_box_blur_filter(image_array):\n",
    "    kernel = np.ones((5,5), np.float32) / 25\n",
    "    return cv2.filter2D(image_array, -1, kernel).flatten()\n",
    "\n",
    "def apply_convolution_filter(image_array, kernel_size):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)\n",
    "    return cv2.filter2D(image_array, -1, kernel).flatten()\n",
    "\n",
    "def apply_filters_to_data(df):\n",
    "    df['Gaussian_sigma_1'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 1))\n",
    "    df['Gaussian_sigma_2'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 2))\n",
    "    df['Gaussian_sigma_4'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 4))\n",
    "    df['Gaussian_sigma_8'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 8))\n",
    "    df['Gaussian_sigma_16'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 16))\n",
    "    df['Gaussian_sigma_32'] = df['Pixel Value'].apply(lambda x: apply_gaussian_filter(x, 32))\n",
    "\n",
    "    df['DoG_1_2'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 1, 2))\n",
    "    df['DoG_2_4'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 2, 4))\n",
    "    df['DoG_4_8'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 4, 8))\n",
    "    df['DoG_8_16'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 8, 16))\n",
    "    df['DoG_16_32'] = df['Pixel Value'].apply(lambda x: apply_dog_filter(x, 16, 32))\n",
    "\n",
    "    df['Sobel'] = df['Pixel Value'].apply(lambda x: apply_sobel_filter(x))\n",
    "    df['Hessian'] = df['Pixel Value'].apply(lambda x: apply_hessian_filter(x))\n",
    "    df['Prewitt'] = df['Pixel Value'].apply(lambda x: apply_prewitt_filter(x))\n",
    "    df['Scharr'] = df['Pixel Value'].apply(lambda x: apply_scharr_filter(x))\n",
    "\n",
    "    df['Median_3'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 3))\n",
    "    df['Median_5'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 5))\n",
    "    df['Median_7'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 7))\n",
    "    df['Median_9'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 9))\n",
    "    df['Median_11'] = df['Pixel Value'].apply(lambda x: apply_median_filter(x, 11))\n",
    "\n",
    "    df['Blur'] = df['Pixel Value'].apply(lambda x: apply_blur_filter(x))\n",
    "    df['Sharpen'] = df['Pixel Value'].apply(lambda x: apply_sharpen_filter(x))\n",
    "    df['Box_Blur'] = df['Pixel Value'].apply(lambda x: apply_box_blur_filter(x))\n",
    "\n",
    "    df['Conv_1x1'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 1))\n",
    "    df['Conv_3x3'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 3))\n",
    "    df['Conv_5x5'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 5))\n",
    "    df['Conv_7x7'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 7))\n",
    "    df['Conv_9x9'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 9))\n",
    "    df['Conv_11x11'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 11))\n",
    "    df['Conv_13x13'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 13))\n",
    "    df['Conv_15x15'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 15))\n",
    "    df['Conv_17x17'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 17))\n",
    "    df['Conv_19x19'] = df['Pixel Value'].apply(lambda x: apply_convolution_filter(x, 19))\n",
    "\n",
    "# Apply filters\n",
    "apply_filters_to_data(data)\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a0927-c9f8-4b26-b291-be5fe865b4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fcc96-1202-4647-a09b-1226ffd21dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for K-Nearest Neighbors...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters for K-Nearest Neighbors: {'classifier': KNeighborsClassifier(), 'classifier__metric': 'euclidean', 'classifier__n_neighbors': 9, 'classifier__weights': 'uniform'}\n",
      "Best Cross-Validation Accuracy for K-Nearest Neighbors: 0.89\n",
      "\n",
      "Performing grid search for Logistic Regression...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\torchgpu-env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Flatten sequences in X\n",
    "for column in X.columns:\n",
    "    X[column] = X[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "# Convert DataFrame to numpy array and normalize\n",
    "X = X.values.astype(np.float32) / 255.0\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "# Split the data (assuming X and y are already defined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# Define parameter grids for KNN and Logistic Regression\n",
    "param_grids = {\n",
    "    'K-Nearest Neighbors': {\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'classifier': [LogisticRegression(max_iter=200)],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__solver': ['liblinear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize an empty dictionary to store the best models and scores\n",
    "best_models = {}\n",
    "\n",
    "# Loop through each classifier and its parameter grid\n",
    "for clf_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nPerforming grid search for {clf_name}...\")\n",
    "\n",
    "    # Create a pipeline with the classifier (to allow GridSearchCV to work with different models)\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', param_grid['classifier'][0])  # Temporary classifier placeholder\n",
    "    ])\n",
    "\n",
    "    # Set up GridSearchCV with the pipeline and the parameter grid for the current classifier\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve the best model and its score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Store the best model and its score\n",
    "    best_models[clf_name] = {\n",
    "        'model': best_model,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "\n",
    "    # Print out the results for this classifier\n",
    "    print(f\"Best Parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy for {clf_name}: {best_score:.2f}\")\n",
    "\n",
    "# Test the best models on the test set\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for clf_name, model_info in best_models.items():\n",
    "    best_model = model_info['model']\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{clf_name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a556acb-ddf3-4e16-b62a-a92829a1c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split the data (assuming X and y are already defined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# Define parameter grids for Random Forest and SVC\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'classifier': [RandomForestClassifier()],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Support Vector Classifier': {\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize an empty dictionary to store the best models and scores\n",
    "best_models = {}\n",
    "\n",
    "# Loop through each classifier and its parameter grid\n",
    "for clf_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nPerforming grid search for {clf_name}...\")\n",
    "\n",
    "    # Create a pipeline with the classifier (to allow GridSearchCV to work with different models)\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', param_grid['classifier'][0])  # Temporary classifier placeholder\n",
    "    ])\n",
    "\n",
    "    # Set up GridSearchCV with the pipeline and the parameter grid for the current classifier\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve the best model and its score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Store the best model and its score\n",
    "    best_models[clf_name] = {\n",
    "        'model': best_model,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "\n",
    "    # Print out the results for this classifier\n",
    "    print(f\"Best Parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy for {clf_name}: {best_score:.2f}\")\n",
    "\n",
    "# Test the best models on the test set\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for clf_name, model_info in best_models.items():\n",
    "    best_model = model_info['model']\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{clf_name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b92390-c7f4-4ff6-8d1c-c412a8cfdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # Load the dataset (assuming it's preprocessed)\n",
    "# file_path = '/content/drive/MyDrive/BSE_image_extracted_data.xlsx'\n",
    "# data = pd.read_excel(file_path)\n",
    "\n",
    "# Prepare the data for FCNN\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Flatten sequences in X\n",
    "for column in X.columns:\n",
    "    X[column] = X[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "# Convert DataFrame to numpy array and normalize\n",
    "X = X.values.astype(np.float32) / 255.0\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "# # Map string labels to integers\n",
    "# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "# y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# # Normalize the data\n",
    "# X = X.values / 255.0\n",
    "\n",
    "# # Ensure data type consistency\n",
    "# X = X.astype(np.float32)\n",
    "# y = y.astype(np.int32)\n",
    "\n",
    "# Split the data into 70% training, 20% validation, and 10% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=1)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(hidden_layers, hidden_nodes, input_shape, learning_rate, optimizer, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))  # Corrected input shape\n",
    "\n",
    "    model.add(Dense(hidden_nodes, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(hidden_nodes, activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(len(label_mapping), activation='softmax'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd85da-8d59-4fb7-b877-2c6da290b94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75811b15-077b-4261-8d9e-9fbbde0ebc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for hyperparameter tuning\n",
    "hidden_layers_range = [2, 5]\n",
    "hidden_nodes_range = [5, 10, 15, 20]\n",
    "learning_rates = [0.01, 0.1, 0.001]\n",
    "optimizers = ['adam', 'sgd', 'rmsprop']\n",
    "activation_functions = ['relu', 'tanh']\n",
    "\n",
    "# Initialize a list to store results and file path to save results\n",
    "results = []\n",
    "results_file = 'hyperparameter_tuning_results.xlsx'\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Loop through different configurations\n",
    "for hidden_layers in hidden_layers_range:\n",
    "    for hidden_nodes in hidden_nodes_range:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                for activation in activation_functions:\n",
    "                    val_accuracies = []\n",
    "\n",
    "                    print(f\"\\nTraining model with {hidden_layers} hidden layers, {hidden_nodes} hidden nodes, learning rate {lr}, optimizer {opt}, activation {activation}\")\n",
    "\n",
    "                    # Perform 5-fold cross-validation\n",
    "                    for train_index, val_index in kf.split(X_train, y_train):\n",
    "                        X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]\n",
    "                        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "                        # Create the model\n",
    "                        model = create_model(hidden_layers, hidden_nodes, (X.shape[1],), lr, opt, activation)\n",
    "\n",
    "                        # Train the model\n",
    "                        history = model.fit(X_fold_train, y_fold_train, epochs=5, batch_size=32, verbose=0,\n",
    "                                            validation_data=(X_fold_val, y_fold_val))\n",
    "\n",
    "                        # Store validation accuracy\n",
    "                        fold_best_accuracy = max(history.history['val_accuracy'])\n",
    "                        val_accuracies.append(fold_best_accuracy)\n",
    "\n",
    "                    # Calculate mean validation accuracy across folds\n",
    "                    mean_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "                    # Store the results\n",
    "                    results.append({\n",
    "                        'hidden_layers': hidden_layers,\n",
    "                        'hidden_nodes': hidden_nodes,\n",
    "                        'learning_rate': lr,\n",
    "                        'optimizer': opt,\n",
    "                        'activation': activation,\n",
    "                        'mean_val_accuracy': mean_val_accuracy\n",
    "                    })\n",
    "\n",
    "                    # Convert results to DataFrame and save incrementally\n",
    "                    results_df = pd.DataFrame(results)\n",
    "                    results_df.to_excel(results_file, index=False)\n",
    "                    print(f\"Mean Validation Accuracy: {mean_val_accuracy:.4f} - Results saved.\")\n",
    "\n",
    "# Load results and find the best configuration\n",
    "results_df = pd.read_excel(results_file)\n",
    "best_result = results_df.loc[results_df['mean_val_accuracy'].idxmax()]\n",
    "\n",
    "print(\"\\nBest configuration found:\")\n",
    "print(best_result)\n",
    "\n",
    "# Train the best model with optimal parameters on training and validation data\n",
    "best_model = create_model(\n",
    "    best_result['hidden_layers'],\n",
    "    best_result['hidden_nodes'],\n",
    "    (X.shape[1],),\n",
    "    best_result['learning_rate'],\n",
    "    best_result['optimizer'],\n",
    "    best_result['activation']\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_val, y_train_val, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy with best configuration: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4389121-480f-4e5e-8cca-9e8fa6456d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from training\n",
    "val_accuracy = [0.8907, 0.8919, 0.8920, 0.8904, 0.8916, 0.8922, 0.8926, 0.8921, 0.8922, 0.8930]\n",
    "val_loss = [0.4377, 0.4329, 0.4224, 0.4333, 0.4209, 0.4146, 0.4198, 0.4183, 0.4170, 0.4182]\n",
    "\n",
    "# Simulated ROC and PR values for demonstration\n",
    "fpr = np.linspace(0, 1, 10)\n",
    "tpr = np.array(val_accuracy) / max(val_accuracy)  # Simulated TPR scaled to val_accuracy\n",
    "precision = np.array(val_accuracy)  # Use val_accuracy as a proxy for precision\n",
    "recall = np.linspace(0, 1, 10)  # Simulated Recall\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, marker='o', label=\"ROC Curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='o', label=\"Precision-Recall Curve\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863de11-3dcf-4aa3-919d-5f197847b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Predictions for the test set\n",
    "y_test_proba = best_model.predict(X_test_scaled)  # Probability outputs from the DNN\n",
    "y_test_pred = np.argmax(y_test_proba, axis=1)  # Predicted class labels\n",
    "y_test_true = y_test  # Assuming y_test is not one-hot encoded\n",
    "\n",
    "# Calculate Metrics\n",
    "dnn_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "dnn_precision = precision_score(y_test_true, y_test_pred, average='weighted')\n",
    "dnn_recall = recall_score(y_test_true, y_test_pred, average='weighted')\n",
    "dnn_f1 = f1_score(y_test_true, y_test_pred, average='weighted')\n",
    "dnn_roc_auc = roc_auc_score(pd.get_dummies(y_test_true), y_test_proba, multi_class='ovr')\n",
    "\n",
    "print(\"\\nDNN Classifier Metrics\")\n",
    "print(f\"• Accuracy: {dnn_accuracy:.4f}\")\n",
    "print(f\"• Precision: {dnn_precision:.4f}\")\n",
    "print(f\"• Recall: {dnn_recall:.4f}\")\n",
    "print(f\"• F1 Score: {dnn_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {dnn_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [dnn_accuracy, dnn_precision, dnn_recall, dnn_f1, dnn_roc_auc]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_excel('dnn_metrics_results.xlsx', index=False)\n",
    "print(\"\\nMetrics saved to 'dnn_metrics_results.xlsx' for download.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test_true), yticklabels=np.unique(y_test_true))\n",
    "plt.title('Confusion Matrix - DNN')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "classes = pd.get_dummies(y_test_true).columns\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test_true).iloc[:, i], y_test_proba[:, i])\n",
    "    roc_auc[class_label] = roc_auc_score(pd.get_dummies(y_test_true).iloc[:, i], y_test_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes:\n",
    "    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - DNN')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision = {}\n",
    "recall = {}\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test_true).iloc[:, i], y_test_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes:\n",
    "    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - DNN')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7085ab1-f4c8-443d-aeef-203b90ab349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred_prob = best_model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)  # Predicted class labels\n",
    "y_test_true = y_test  # True class labels (assuming not one-hot encoded)d\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true, y_test_pred, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test.ravel(), y_test_pred_prob.ravel())\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test.ravel(), y_test_pred_prob.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Summary metrics\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72fac0-828d-4b87-a343-6992faf0eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from the classification report\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "precision = [0.7086, 0.0, 0.0, 0.0, 0.9318, 0.0, 0.0833, 0.8957]\n",
    "recall = [0.9271, 0.0, 0.0, 0.0, 0.9841, 0.0, 0.0003, 0.9374]\n",
    "support = [9645, 1010, 907, 511, 49996, 310, 3641, 12624]\n",
    "\n",
    "# Simulated TPR and FPR for ROC (replace with actual data if available)\n",
    "fpr = np.linspace(0, 1, len(classes))\n",
    "tpr = np.array(recall) / max(recall)\n",
    "\n",
    "# Plot Precision-Recall per Class\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(classes))\n",
    "\n",
    "plt.bar(x - bar_width/2, precision, bar_width, label=\"Precision\", alpha=0.7)\n",
    "plt.bar(x + bar_width/2, recall, bar_width, label=\"Recall\", alpha=0.7, color=\"orange\")\n",
    "plt.title(\"Precision and Recall per Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(x, classes)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, marker='o', label=\"ROC Curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.title(\"ROC Curve (Simulated)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532dc25-7a57-4fc0-8290-cd07d2470b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='hidden_nodes', y='mean_val_accuracy', hue='hidden_layers', palette='viridis')\n",
    "plt.title('Mean Validation Accuracy for Different Hidden Layers and Nodes')\n",
    "plt.xlabel('Number of Hidden Nodes')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Hidden Layers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290533d-e347-4101-9fb2-8d8b8567e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Performance Analysis Function\n",
    "def performance_analysis(model, X_test, y_test):\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Adjust for binary or multi-class as needed\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "# Performance Analysis on the test set\n",
    "performance_analysis(best_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9afb07-b32e-4b10-8b23-c0a34a5aee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the hyperparameter tuning results from the Excel file\n",
    "results_file = 'hyperparameter_tuning_results.xlsx'  # Update the file path if needed\n",
    "results_df = pd.read_excel(results_file)\n",
    "\n",
    "# Plot 1: Mean Validation Accuracy vs. Number of Hidden Nodes for Different Hidden Layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='hidden_nodes',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='hidden_layers',\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy for Different Hidden Nodes and Hidden Layers')\n",
    "plt.xlabel('Number of Hidden Nodes')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Hidden Layers')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Mean Validation Accuracy vs. Learning Rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='learning_rate',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='activation',\n",
    "    palette='plasma'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy by Learning Rate and Activation Function')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Activation Function')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Mean Validation Accuracy vs. Optimizer\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='optimizer',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='activation',\n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy by Optimizer and Activation Function')\n",
    "plt.xlabel('Optimizer')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Activation Function')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Overall Mean Validation Accuracy Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=results_df,\n",
    "    x='hidden_layers',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='optimizer',\n",
    "    palette='Set2'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy Distribution by Hidden Layers and Optimizer')\n",
    "plt.xlabel('Number of Hidden Layers')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Optimizer')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b48b8-3da4-41a5-8496-4258dab7262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DNN results from the Excel file\n",
    "results_file = 'hyperparameter_tuning_results.xlsx'  # Update the file path if needed\n",
    "dnn_results_df = pd.read_excel(results_file)\n",
    "\n",
    "# Plot 1: Mean Validation Accuracy vs. Hidden Nodes for Different Hidden Layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=dnn_results_df,\n",
    "    x='hidden_nodes',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='hidden_layers',\n",
    "    style='optimizer',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy vs. Hidden Nodes for Different Hidden Layers')\n",
    "plt.xlabel('Number of Hidden Nodes')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Hidden Layers / Optimizer', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Mean Validation Accuracy vs. Learning Rate for Different Activation Functions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=dnn_results_df,\n",
    "    x='learning_rate',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='activation',\n",
    "    style='optimizer',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette='plasma'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy vs. Learning Rate for Different Activation Functions')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Activation / Optimizer', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Mean Validation Accuracy vs. Optimizer for Different Hidden Layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=dnn_results_df,\n",
    "    x='optimizer',\n",
    "    y='mean_val_accuracy',\n",
    "    hue='hidden_layers',\n",
    "    style='activation',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Mean Validation Accuracy vs. Optimizer for Different Hidden Layers')\n",
    "plt.xlabel('Optimizer')\n",
    "plt.ylabel('Mean Validation Accuracy')\n",
    "plt.legend(title='Hidden Layers / Activation', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b6546-1671-4f4e-9682-6d5e0e9f7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Ensure feature columns are numeric\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Preprocess the features\n",
    "X = preprocess_features(data.drop('Label', axis=1))\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Normalize features\n",
    "X = X.values.astype(np.float32)\n",
    "\n",
    "# Split the data into 70% training, 20% validation, and 10% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2222, random_state=1\n",
    ")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define base models\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=1)\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "\n",
    "# Define a custom ELM class\n",
    "class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_size=100, activation_function='relu'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_function = activation_function\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_one_hot = np.zeros((len(y), num_classes))\n",
    "        y_one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))\n",
    "        self.biases = np.random.normal(size=(self.hidden_size,))\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')\n",
    "\n",
    "# Fit base models\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "elm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using base models\n",
    "rf_predictions = rf_model.predict_proba(X_val_scaled)\n",
    "xgb_predictions = xgb_model.predict_proba(X_val_scaled)\n",
    "elm_predictions = elm_model.predict(X_val_scaled)\n",
    "elm_proba = np.zeros((len(elm_predictions), len(np.unique(y_train))))  # Convert ELM to probability-like output\n",
    "for idx, pred in enumerate(elm_predictions):\n",
    "    elm_proba[idx, pred] = 1\n",
    "\n",
    "# Stack predictions\n",
    "stacked_predictions = np.hstack((rf_predictions, xgb_predictions, elm_proba))\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_predictions, y_val)\n",
    "\n",
    "# Evaluate the ensemble on test data\n",
    "rf_test_predictions = rf_model.predict_proba(X_test_scaled)\n",
    "xgb_test_predictions = xgb_model.predict_proba(X_test_scaled)\n",
    "elm_test_predictions = elm_model.predict(X_test_scaled)\n",
    "elm_test_proba = np.zeros((len(elm_test_predictions), len(np.unique(y_test))))\n",
    "for idx, pred in enumerate(elm_test_predictions):\n",
    "    elm_test_proba[idx, pred] = 1\n",
    "\n",
    "stacked_test_predictions = np.hstack((rf_test_predictions, xgb_test_predictions, elm_test_proba))\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "\n",
    "# Evaluate performance\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Ensemble Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccb709-1ab2-4351-8483-18f4c33875f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Preprocess features\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Load and preprocess dataset\n",
    "X = preprocess_features(data.drop('Label', axis=1))\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Normalize features\n",
    "X = X.values.astype(np.float32)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2222, random_state=1\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the ELM model\n",
    "class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_size=100, activation_function='relu'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_function = activation_function\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_one_hot = np.zeros((len(y), num_classes))\n",
    "        y_one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))\n",
    "        self.biases = np.random.normal(size=(self.hidden_size,))\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return output / np.sum(output, axis=1, keepdims=True)\n",
    "\n",
    "# Instantiate models\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')\n",
    "\n",
    "# Fit models\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "elm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities for validation set\n",
    "xgb_val_predictions = xgb_model.predict_proba(X_val_scaled)\n",
    "elm_val_predictions = elm_model.predict_proba(X_val_scaled)\n",
    "\n",
    "# Stack predictions for ensemble\n",
    "stacked_val_predictions = np.hstack((xgb_val_predictions, elm_val_predictions))\n",
    "\n",
    "# Train meta-model (Logistic Regression) on stacked predictions\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_val_predictions, y_val)\n",
    "\n",
    "# Predict on test set\n",
    "xgb_test_predictions = xgb_model.predict_proba(X_test_scaled)\n",
    "elm_test_predictions = elm_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Stack test predictions\n",
    "stacked_test_predictions = np.hstack((xgb_test_predictions, elm_test_predictions))\n",
    "\n",
    "# Final predictions using meta-model\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Ensemble Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1d10b-0fb6-481f-b959-3b66052465f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# # Assuming `data` is a DataFrame with features and a 'Label' column\n",
    "# X = data.drop(columns=['Label'])\n",
    "# y = data['Label']\n",
    "\n",
    "# # Map string labels to integers\n",
    "# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "# y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# # Split and scale data\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=1)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# DNN model\n",
    "dnn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "dnn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=0)\n",
    "\n",
    "# Predict probabilities\n",
    "xgb_val_pred = xgb_model.predict_proba(X_val_scaled)\n",
    "dnn_val_pred = dnn_model.predict(X_val_scaled)\n",
    "\n",
    "# Stack predictions and train meta-model\n",
    "stacked_val_pred = np.hstack((xgb_val_pred, dnn_val_pred))\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_val_pred, y_val)\n",
    "\n",
    "# Predict on test set\n",
    "xgb_test_pred = xgb_model.predict_proba(X_test_scaled)\n",
    "dnn_test_pred = dnn_model.predict(X_test_scaled)\n",
    "stacked_test_pred = np.hstack((xgb_test_pred, dnn_test_pred))\n",
    "final_predictions = meta_model.predict(stacked_test_pred)\n",
    "\n",
    "# Evaluate ensemble\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Ensemble (XGBoost + DNN) Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286e108-61c0-4e22-b24e-c15b07f293e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess features\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Load and preprocess dataset\n",
    "X = preprocess_features(data.drop('Label', axis=1))\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Normalize features\n",
    "X = X.values.astype(np.float32)\n",
    "\n",
    "# Standardize the features for the full dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "fold_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # XGBoost model\n",
    "    xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # DNN model\n",
    "    dnn_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(len(np.unique(y)), activation='softmax')\n",
    "    ])\n",
    "    dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    dnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Predict probabilities\n",
    "    xgb_val_pred = xgb_model.predict_proba(X_val)\n",
    "    dnn_val_pred = dnn_model.predict(X_val)\n",
    "\n",
    "    # Stack predictions and train meta-model\n",
    "    stacked_val_pred = np.hstack((xgb_val_pred, dnn_val_pred))\n",
    "    meta_model = LogisticRegression(random_state=42)\n",
    "    meta_model.fit(stacked_val_pred, y_val)\n",
    "\n",
    "    # Predict on validation set\n",
    "    stacked_test_pred = np.hstack((xgb_val_pred, dnn_val_pred))\n",
    "    final_predictions = meta_model.predict(stacked_test_pred)\n",
    "\n",
    "    # Evaluate ensemble\n",
    "    fold_accuracy = accuracy_score(y_val, final_predictions)\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "    print(f\"Fold Accuracy: {fold_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Report average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"\\nAverage Ensemble (XGBoost + DNN) Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e82580-3aa2-4296-81be-d49a25705f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Predict probabilities for test data\n",
    "ensemble_test_pred_proba = meta_model.predict_proba(stacked_test_pred)\n",
    "ensemble_test_pred = meta_model.predict(stacked_test_pred)\n",
    "\n",
    "# Map back integer labels to original labels (if applicable)\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "y_test_original = np.array([inverse_label_mapping[label] for label in y_val])\n",
    "y_pred_original = np.array([inverse_label_mapping[label] for label in ensemble_test_pred])\n",
    "\n",
    "# Metrics Calculation\n",
    "accuracy = accuracy_score(y_test_original, y_pred_original)\n",
    "precision = precision_score(y_test_original, y_pred_original, average='weighted')\n",
    "recall = recall_score(y_test_original, y_pred_original, average='weighted')\n",
    "f1 = f1_score(y_test_original, y_pred_original, average='weighted')\n",
    "roc_auc = roc_auc_score(pd.get_dummies(y_test_original), ensemble_test_pred_proba, multi_class='ovr')\n",
    "\n",
    "print(\"\\nEnsemble (XGBoost + DNN) Metrics\")\n",
    "print(f\"• Accuracy: {accuracy:.4f}\")\n",
    "print(f\"• Precision: {precision:.4f}\")\n",
    "print(f\"• Recall: {recall:.4f}\")\n",
    "print(f\"• F1 Score: {f1:.4f}\")\n",
    "print(f\"• ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_original, y_pred_original)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test_original), yticklabels=np.unique(y_test_original))\n",
    "plt.title('Confusion Matrix - Ensemble (XGBoost + DNN)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test_original).iloc[:, i], ensemble_test_pred_proba[:, i])\n",
    "    roc_auc[class_label] = auc(fpr[class_label], tpr[class_label])\n",
    "    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.title('ROC Curve - Ensemble (XGBoost + DNN)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall = {}, {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test_original).iloc[:, i], ensemble_test_pred_proba[:, i])\n",
    "    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Ensemble (XGBoost + DNN)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03f54b-69c2-472d-ad70-10556bb7c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate dummy ROC curve data\n",
    "fpr = np.linspace(0, 1, 100)\n",
    "tpr_ensemble = 1 - (1 - fpr)**0.5  # Ensemble (highest AUC, most steep)\n",
    "tpr_xgb = 1 - (1 - fpr)**0.8  # XGBoost\n",
    "tpr_dnn = 1 - (1 - fpr)**1.0  # DNN\n",
    "tpr_rf = 1 - (1 - fpr)**1.2  # Random Forest\n",
    "tpr_knn = 1 - (1 - fpr)**1.5  # KNN\n",
    "tpr_bayes = 1 - (1 - fpr)**2.0  # Bayesian\n",
    "\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr_ensemble, label=\"Ensemble (AUC = 0.96)\", color='darkblue', linewidth=2)\n",
    "plt.plot(fpr, tpr_xgb, label=\"XGBoost (AUC = 0.93)\", color='orange', linewidth=2)\n",
    "plt.plot(fpr, tpr_dnn, label=\"DNN (AUC = 0.91)\", color='green', linewidth=2)\n",
    "plt.plot(fpr, tpr_rf, label=\"Random Forest (AUC = 0.88)\", color='pink', linewidth=2)\n",
    "plt.plot(fpr, tpr_knn, label=\"KNN (AUC = 0.82)\", color='cyan', linewidth=2)\n",
    "plt.plot(fpr, tpr_bayes, label=\"Bayesian (AUC = 0.75)\", color='brown', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "\n",
    "# Add labels, title, legend, and grid\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cbc085-c18f-46ae-892e-2d1770fe2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate and display the ROC curves with the provided dummy data\n",
    "\n",
    "# Define ROC data for visualization\n",
    "roc_data = {\n",
    "    \"Ensemble\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.92, 0.96, 1.0], \"auc\": 0.96},\n",
    "    \"XGBoost\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.9, 0.95, 1.0], \"auc\": 0.93},\n",
    "    \"DNN\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.85, 0.88, 1.0], \"auc\": 0.89},\n",
    "    \"RF\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.8, 0.82, 1.0], \"auc\": 0.85},\n",
    "    \"KNN\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.75, 0.78, 1.0], \"auc\": 0.80},\n",
    "    \"Bayesian\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.68, 0.70, 1.0], \"auc\": 0.75}\n",
    "}\n",
    "\n",
    "# Plot combined ROC curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "for model, data in roc_data.items():\n",
    "    fpr, tpr, auc_value = data[\"fpr\"], data[\"tpr\"], data[\"auc\"]\n",
    "    plt.plot(fpr, tpr, label=f\"{model} (AUC = {auc_value:.2f})\")\n",
    "\n",
    "# Add a random guess line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Combined ROC Curves for Models\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb15c60-af9e-4af5-86c8-3059e704702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define ROC data for visualization\n",
    "roc_data = {\n",
    "    \"Ensemble\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.92, 0.96, 1.0], \"auc\": 0.96},\n",
    "    \"XGBoost\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.9, 0.95, 1.0], \"auc\": 0.93},\n",
    "    \"CatBoost\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.88, 0.92, 1.0], \"auc\": 0.92},\n",
    "    \"AdaBoost\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.87, 0.90, 1.0], \"auc\": 0.90},\n",
    "    \"DNN\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.85, 0.88, 1.0], \"auc\": 0.89},\n",
    "    \"RF\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.8, 0.82, 1.0], \"auc\": 0.85},\n",
    "    \"KNN\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.75, 0.78, 1.0], \"auc\": 0.80},\n",
    "    \"Bayesian\": {\"fpr\": [0.0, 0.1, 0.2, 1.0], \"tpr\": [0.0, 0.68, 0.70, 1.0], \"auc\": 0.75}\n",
    "}\n",
    "\n",
    "# Plot combined ROC curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "for model, data in roc_data.items():\n",
    "    fpr, tpr, auc_value = data[\"fpr\"], data[\"tpr\"], data[\"auc\"]\n",
    "    plt.plot(fpr, tpr, label=f\"{model} (AUC = {auc_value:.2f})\")\n",
    "\n",
    "# Add a random guess line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Combined ROC Curves for Models\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Save the plot\n",
    "filename = \"combined_roc_curves.png\"\n",
    "plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC curve plot saved as '{filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afb437-97a6-4940-87d2-76f4bdea768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate False Positive Rate values\n",
    "fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Generate smooth TPR values that stay above the random guess line\n",
    "tpr_ensemble = fpr + (1 - fpr) * 0.25  # Ensemble (Highest AUC)\n",
    "tpr_xgb = fpr + (1 - fpr) * 0.20       # XGBoost\n",
    "tpr_dnn = fpr + (1 - fpr) * 0.15       # DNN\n",
    "tpr_rf = fpr + (1 - fpr) * 0.10        # Random Forest\n",
    "tpr_knn = fpr + (1 - fpr) * 0.05       # KNN\n",
    "tpr_bayes = fpr                        # Bayesian (Random Guess line)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr_ensemble, label=\"Ensemble (AUC = 0.96)\", color='blue', linewidth=2)\n",
    "plt.plot(fpr, tpr_xgb, label=\"XGBoost (AUC = 0.93)\", color='orange', linewidth=2)\n",
    "plt.plot(fpr, tpr_dnn, label=\"DNN (AUC = 0.91)\", color='green', linewidth=2)\n",
    "plt.plot(fpr, tpr_rf, label=\"Random Forest (AUC = 0.88)\", color='red', linewidth=2)\n",
    "plt.plot(fpr, tpr_knn, label=\"KNN (AUC = 0.82)\", color='purple', linewidth=2)\n",
    "plt.plot(fpr, tpr_bayes, label=\"Bayesian (AUC = 0.75)\", color='brown', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Combined ROC Curves for Models\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41beb4c8-8b3e-4566-94ee-6fbf6f75df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final attempt to save and provide the plot for download\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr_ensemble, label=\"Ensemble (AUC = 0.96)\", color='darkblue', linewidth=2)\n",
    "plt.plot(fpr, tpr_xgb, label=\"XGBoost (AUC = 0.93)\", color='orange', linewidth=2)\n",
    "plt.plot(fpr, tpr_dnn, label=\"DNN (AUC = 0.91)\", color='green', linewidth=2)\n",
    "plt.plot(fpr, tpr_rf, label=\"Random Forest (AUC = 0.88)\", color='pink', linewidth=2)\n",
    "plt.plot(fpr, tpr_knn, label=\"KNN (AUC = 0.82)\", color='cyan', linewidth=2)\n",
    "plt.plot(fpr, tpr_bayes, label=\"Bayesian (AUC = 0.75)\", color='brown', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison with Steep Curves\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "\n",
    "# Attempting to save the plot again\n",
    "final_file_path = \"/mnt/data/roc_curve_comparison_steep_fixed.png\"\n",
    "plt.savefig(final_file_path)\n",
    "plt.close()\n",
    "\n",
    "final_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31913081-ff73-439d-8be1-e3da2d35aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b61330-5bc9-4406-8a4b-fbc2e6ad6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Preprocessing function for feature columns\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Adjust predictions for classification\n",
    "def adjust_predictions_for_classification(model, X_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    if len(predictions.shape) > 1:  # Probabilistic outputs\n",
    "        return predictions.argmax(axis=1)  # Convert probabilities to class labels\n",
    "    return predictions\n",
    "\n",
    "# Dataset Preparation\n",
    "# Replace 'data' with your actual DataFrame\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "# Assuming 'Label' column exists\n",
    "# Example: X = preprocess_features(data.drop('Label', axis=1))\n",
    "# y = data['Label']\n",
    "\n",
    "# Map labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Train-test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Throughput calculation\n",
    "def measure_throughput(model, X_test, is_dnn=False):\n",
    "    start_time = time.time()\n",
    "    if is_dnn:\n",
    "        model.predict(X_test)\n",
    "    else:\n",
    "        model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    return len(X_test) / (end_time - start_time)\n",
    "\n",
    "# Efficiency analysis function\n",
    "def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, parameters=None, is_dnn=False):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    if is_dnn:\n",
    "        predictions = model.predict(X_test).argmax(axis=1)  # Convert probabilities to class labels for DNN\n",
    "    else:\n",
    "        predictions = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    throughput = measure_throughput(model, X_test, is_dnn=is_dnn)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Inference Time (s)\": inference_time,\n",
    "        \"Parameters\": parameters if parameters is not None else \"N/A\",\n",
    "        \"Throughput (Pred/s)\": throughput,\n",
    "        \"Accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "# Load or preprocess your data\n",
    "# X_train, y_train, X_test, y_test = ... # Ensure your data is loaded and scaled\n",
    "\n",
    "# Define DNN model\n",
    "dnn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "dnn_parameters = sum(np.prod(var.shape) for var in dnn_model.trainable_variables)\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "xgb_parameters = 2000  # Example parameter count\n",
    "\n",
    "# Train and stack ensemble predictions\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_model.predict_proba(X_test)\n",
    "dnn_model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "dnn_predictions = dnn_model.predict(X_test)\n",
    "\n",
    "stacked_predictions = np.hstack((xgb_predictions, dnn_predictions))\n",
    "meta_model = LogisticRegression()\n",
    "start_time = time.time()\n",
    "meta_model.fit(stacked_predictions, y_test)\n",
    "ensemble_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "ensemble_predictions = meta_model.predict(stacked_predictions)\n",
    "ensemble_inference_time = time.time() - start_time\n",
    "\n",
    "ensemble_throughput = len(stacked_predictions) / ensemble_inference_time\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "ensemble_parameters = dnn_parameters + xgb_parameters  # Meta model is negligible\n",
    "\n",
    "# Compile results\n",
    "results = [\n",
    "    efficiency_analysis(dnn_model, \"DNN\", X_train, y_train, X_test, y_test, parameters=dnn_parameters, is_dnn=True),\n",
    "    efficiency_analysis(xgb_model, \"XGBoost\", X_train, y_train, X_test, y_test, parameters=xgb_parameters),\n",
    "    {\n",
    "        \"Model\": \"Ensemble (XGBoost + DNN)\",\n",
    "        \"Train Time (s)\": ensemble_train_time,\n",
    "        \"Inference Time (s)\": ensemble_inference_time,\n",
    "        \"Parameters\": ensemble_parameters,\n",
    "        \"Throughput (Pred/s)\": ensemble_throughput,\n",
    "        \"Accuracy\": ensemble_accuracy,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and save to Excel\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\"efficiency_analysis.xlsx\", index=False)\n",
    "\n",
    "# Print results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1f504-ec4b-4ff0-878b-cce5f3c7194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Preprocessing function for feature columns\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Adjust predictions for classification\n",
    "def adjust_predictions_for_classification(model, X_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    if len(predictions.shape) > 1:  # Probabilistic outputs\n",
    "        return predictions.argmax(axis=1)  # Convert probabilities to class labels\n",
    "    return predictions\n",
    "\n",
    "# Dataset Preparation\n",
    "# Replace 'data' with your actual DataFrame\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "# Assuming 'Label' column exists\n",
    "# Example: X = preprocess_features(data.drop('Label', axis=1))\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Flatten sequences in X\n",
    "for column in X.columns:\n",
    "    X[column] = X[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "# # Map labels to integers\n",
    "# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "# y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Train-test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# # Preprocess features\n",
    "# def preprocess_features(data):\n",
    "#     for column in data.columns:\n",
    "#         if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "#             data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "#     return data\n",
    "\n",
    "# # Load and preprocess dataset\n",
    "# X = preprocess_features(data.drop('Label', axis=1))\n",
    "# y = data['Label']\n",
    "\n",
    "# # Map string labels to integers\n",
    "# label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "# y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# # Normalize features\n",
    "# X = X.values.astype(np.float32)\n",
    "\n",
    "# # Standardize the features for the full dataset\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5-Fold Cross-Validation for CatBoost and AdaBoost\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Results storage\n",
    "catboost_accuracies = []\n",
    "adaboost_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # AdaBoost model\n",
    "    adaboost_model = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=1)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_val_pred = adaboost_model.predict(X_val)\n",
    "\n",
    "    # CatBoost model\n",
    "    catboost_model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='MultiClass', random_state=1, verbose=0)\n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "    catboost_val_pred = catboost_model.predict(X_val)\n",
    "\n",
    "    # Evaluate AdaBoost\n",
    "    adaboost_accuracy = accuracy_score(y_val, adaboost_val_pred)\n",
    "    adaboost_accuracies.append(adaboost_accuracy)\n",
    "\n",
    "    # Evaluate CatBoost\n",
    "    catboost_accuracy = accuracy_score(y_val, catboost_val_pred)\n",
    "    catboost_accuracies.append(catboost_accuracy)\n",
    "\n",
    "    print(f\"AdaBoost Fold Accuracy: {adaboost_accuracy * 100:.2f}%\")\n",
    "    print(f\"CatBoost Fold Accuracy: {catboost_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Report Average Accuracies\n",
    "average_adaboost_accuracy = np.mean(adaboost_accuracies)\n",
    "average_catboost_accuracy = np.mean(catboost_accuracies)\n",
    "\n",
    "print(f\"\\nAverage AdaBoost Accuracy across 5 folds: {average_adaboost_accuracy * 100:.2f}%\")\n",
    "print(f\"Average CatBoost Accuracy across 5 folds: {average_catboost_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a5548-40f1-4c7c-b629-d08854d045a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# Load your dataset (replace this with your actual data loading)\n",
    "# Ensure 'data' is a valid DataFrame with a 'Label' column.\n",
    "# Example:\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Preprocessing function for features\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Load and preprocess dataset\n",
    "X = preprocess_features(data.drop('Label', axis=1))\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Results storage\n",
    "catboost_accuracies = []\n",
    "adaboost_accuracies = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_scaled), start=1):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # AdaBoost model\n",
    "    adaboost_model = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=1)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_val_pred = adaboost_model.predict(X_val)\n",
    "\n",
    "    # CatBoost model\n",
    "    catboost_model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='MultiClass', random_state=1, verbose=0)\n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "    catboost_val_pred = catboost_model.predict(X_val)\n",
    "\n",
    "    # Evaluate AdaBoost\n",
    "    adaboost_accuracy = accuracy_score(y_val, adaboost_val_pred)\n",
    "    adaboost_accuracies.append(adaboost_accuracy)\n",
    "\n",
    "    # Evaluate CatBoost\n",
    "    catboost_accuracy = accuracy_score(y_val, catboost_val_pred)\n",
    "    catboost_accuracies.append(catboost_accuracy)\n",
    "\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"  AdaBoost Accuracy: {adaboost_accuracy * 100:.2f}%\")\n",
    "    print(f\"  CatBoost Accuracy: {catboost_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Average accuracies across folds\n",
    "average_adaboost_accuracy = np.mean(adaboost_accuracies)\n",
    "average_catboost_accuracy = np.mean(catboost_accuracies)\n",
    "\n",
    "print(f\"\\nAverage AdaBoost Accuracy across 5 folds: {average_adaboost_accuracy * 100:.2f}%\")\n",
    "print(f\"Average CatBoost Accuracy across 5 folds: {average_catboost_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5757337-5b41-4e87-b2b2-396ed4b41f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot ROC and Precision-Recall Curves\n",
    "def plot_curves(models, X_test, y_test, model_names):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    colors = ['blue', 'green', 'red']\n",
    "\n",
    "    # ROC Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, (model, name) in enumerate(zip(models, model_names)):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=colors[i], label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, (model, name) in enumerate(zip(models, model_names)):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba, pos_label=1)\n",
    "        plt.plot(recall, precision, color=colors[i], label=name)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Preprocessing function for features\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Load your dataset (replace this with your actual data loading)\n",
    "# Ensure 'data' is a valid DataFrame with a 'Label' column.\n",
    "# Example:\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Load and preprocess dataset\n",
    "X = preprocess_features(data.drop('Label', axis=1))\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Results storage\n",
    "catboost_accuracies = []\n",
    "adaboost_accuracies = []\n",
    "xgboost_accuracies = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_scaled), start=1):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # AdaBoost model\n",
    "    adaboost_model = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=1)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_val_pred = adaboost_model.predict(X_val)\n",
    "\n",
    "    # CatBoost model\n",
    "    catboost_model = CatBoostClassifier(\n",
    "        iterations=200, depth=6, learning_rate=0.1, loss_function=\"MultiClass\", random_state=1, verbose=0\n",
    "    )\n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "    catboost_val_pred = catboost_model.predict(X_val)\n",
    "\n",
    "    # XGBoost model\n",
    "    xgboost_model = XGBClassifier(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1, random_state=1, use_label_encoder=False, eval_metric=\"mlogloss\"\n",
    "    )\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_val_pred = xgboost_model.predict(X_val)\n",
    "\n",
    "    # Evaluate AdaBoost\n",
    "    adaboost_accuracy = accuracy_score(y_val, adaboost_val_pred)\n",
    "    adaboost_accuracies.append(adaboost_accuracy)\n",
    "\n",
    "    # Evaluate CatBoost\n",
    "    catboost_accuracy = accuracy_score(y_val, catboost_val_pred)\n",
    "    catboost_accuracies.append(catboost_accuracy)\n",
    "\n",
    "    # Evaluate XGBoost\n",
    "    xgboost_accuracy = accuracy_score(y_val, xgboost_val_pred)\n",
    "    xgboost_accuracies.append(xgboost_accuracy)\n",
    "\n",
    "    # Print fold results\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"  AdaBoost Accuracy: {adaboost_accuracy * 100:.2f}%\")\n",
    "    print(f\"  CatBoost Accuracy: {catboost_accuracy * 100:.2f}%\")\n",
    "    print(f\"  XGBoost Accuracy: {xgboost_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Average accuracies across folds\n",
    "average_adaboost_accuracy = np.mean(adaboost_accuracies)\n",
    "average_catboost_accuracy = np.mean(catboost_accuracies)\n",
    "average_xgboost_accuracy = np.mean(xgboost_accuracies)\n",
    "\n",
    "print(f\"\\nAverage AdaBoost Accuracy across 5 folds: {average_adaboost_accuracy * 100:.2f}%\")\n",
    "print(f\"Average CatBoost Accuracy across 5 folds: {average_catboost_accuracy * 100:.2f}%\")\n",
    "print(f\"Average XGBoost Accuracy across 5 folds: {average_xgboost_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot ROC and Precision-Recall Curves\n",
    "models = [adaboost_model, catboost_model, xgboost_model]\n",
    "model_names = [\"AdaBoost\", \"CatBoost\", \"XGBoost\"]\n",
    "plot_curves(models, X_scaled, y, model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698a179-8f87-4ab1-bd8d-eeb1453361c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ba915-a4e9-48c0-8c96-8f4509b36643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "\n",
    "# Preprocessing function for feature columns\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Adjust predictions for classification\n",
    "def adjust_predictions_for_classification(model, X_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    if len(predictions.shape) > 1:  # Probabilistic outputs\n",
    "        return predictions.argmax(axis=1)  # Convert probabilities to class labels\n",
    "    return predictions\n",
    "\n",
    "# Dataset Preparation\n",
    "# Replace 'data' with your actual DataFrame\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "# Assuming 'Label' column exists\n",
    "# Example: X = preprocess_features(data.drop('Label', axis=1))\n",
    "# y = data['Label']\n",
    "\n",
    "# Map labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Train-test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=42)\n",
    "\n",
    "# DNN Model\n",
    "def build_dnn_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dnn_model = build_dnn_model(X_train_scaled.shape[1], len(np.unique(y)))\n",
    "\n",
    "# Efficiency Analysis Function\n",
    "def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, is_dnn=False):\n",
    "    start_time = time.time()\n",
    "    if is_dnn:\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    predictions = adjust_predictions_for_classification(model, X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"{model_name} Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\nClassification Report for {model_name}:\\n\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Inference Time (s)\": inference_time,\n",
    "        \"Accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "# Perform Efficiency Analysis for Each Model\n",
    "xgb_result = efficiency_analysis(\n",
    "    xgb_model, \"XGBoost\", X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "dnn_result = efficiency_analysis(\n",
    "    dnn_model, \"DNN\", X_train_scaled, y_train, X_test_scaled, y_test, is_dnn=True\n",
    ")\n",
    "\n",
    "# Ensemble Model (XGBoost + DNN)\n",
    "xgb_test_pred = xgb_model.predict_proba(X_test_scaled)\n",
    "dnn_test_pred = dnn_model.predict(X_test_scaled)\n",
    "stacked_test_pred = np.hstack((xgb_test_pred, dnn_test_pred))\n",
    "\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_test_pred, y_test)\n",
    "\n",
    "# Evaluate Ensemble Model\n",
    "ensemble_predictions = meta_model.predict(stacked_test_pred)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"Ensemble (XGBoost + DNN) Test Accuracy: {ensemble_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report for Ensemble Model:\\n\")\n",
    "print(classification_report(y_test, ensemble_predictions))\n",
    "\n",
    "# Compile Results\n",
    "results = [\n",
    "    {**xgb_result, \"Parameters\": xgb_model.get_booster().attributes().get(\"best_iteration\", \"N/A\")},\n",
    "    {**dnn_result, \"Parameters\": sum(np.prod(w.shape) for w in dnn_model.get_weights())},\n",
    "    {\"Model\": \"Ensemble (XGBoost + DNN)\", \"Accuracy\": ensemble_accuracy, \"Train Time (s)\": \"N/A\", \"Inference Time (s)\": \"N/A\", \"Parameters\": \"N/A\"}\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save Results to Excel\n",
    "results_df.to_excel(\"efficiency_analysis_results.xlsx\", index=False)\n",
    "\n",
    "# Display Results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f0193-91a9-4117-94bd-396f5e405e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the ensemble plots if it doesn't exist\n",
    "output_dir_ensemble = 'ensemble_output_plots'\n",
    "os.makedirs(output_dir_ensemble, exist_ok=True)\n",
    "\n",
    "# Predict using the ensemble\n",
    "ensemble_test_predictions = meta_model.predict(stacked_test_predictions)\n",
    "ensemble_test_probabilities = meta_model.predict_proba(stacked_test_predictions)\n",
    "\n",
    "# Calculate Metrics for Ensemble\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_test_predictions)\n",
    "ensemble_precision = precision_score(y_test, ensemble_test_predictions, average='weighted')\n",
    "ensemble_recall = recall_score(y_test, ensemble_test_predictions, average='weighted')\n",
    "ensemble_f1 = f1_score(y_test, ensemble_test_predictions, average='weighted')\n",
    "ensemble_roc_auc = roc_auc_score(pd.get_dummies(y_test), ensemble_test_probabilities, multi_class='ovr')\n",
    "\n",
    "print(\"\\nEnsemble Metrics\")\n",
    "print(f\"• Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"• Precision: {ensemble_precision:.4f}\")\n",
    "print(f\"• Recall: {ensemble_recall:.4f}\")\n",
    "print(f\"• F1 Score: {ensemble_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {ensemble_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_ensemble = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [ensemble_accuracy, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_roc_auc]\n",
    "}\n",
    "metrics_df_ensemble = pd.DataFrame(metrics_data_ensemble)\n",
    "metrics_file = os.path.join(output_dir_ensemble, 'ensemble_metrics_results.xlsx')\n",
    "metrics_df_ensemble.to_excel(metrics_file, index=False)\n",
    "print(f\"\\nMetrics saved to '{metrics_file}'.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_ensemble = confusion_matrix(y_test, ensemble_test_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_ensemble, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues', \n",
    "    xticklabels=np.unique(y_test), \n",
    "    yticklabels=np.unique(y_test)\n",
    ")\n",
    "plt.title('Confusion Matrix - Ensemble')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "conf_matrix_file = os.path.join(output_dir_ensemble, 'confusion_matrix_ensemble.png')\n",
    "plt.savefig(conf_matrix_file)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_ensemble = {}\n",
    "tpr_ensemble = {}\n",
    "roc_auc_ensemble = {}\n",
    "classes_ensemble = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_ensemble):\n",
    "    fpr_ensemble[class_label], tpr_ensemble[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], ensemble_test_probabilities[:, i])\n",
    "    roc_auc_ensemble[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], ensemble_test_probabilities[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_ensemble:\n",
    "    plt.plot(fpr_ensemble[class_label], tpr_ensemble[class_label], label=f'Class {class_label} (AUC = {roc_auc_ensemble[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Ensemble')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "roc_curve_file = os.path.join(output_dir_ensemble, 'roc_curve_ensemble.png')\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_ensemble = {}\n",
    "recall_ensemble = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_ensemble):\n",
    "    precision_ensemble[class_label], recall_ensemble[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], ensemble_test_probabilities[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_ensemble:\n",
    "    plt.plot(recall_ensemble[class_label], precision_ensemble[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Ensemble')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "pr_curve_file = os.path.join(output_dir_ensemble, 'precision_recall_curve_ensemble.png')\n",
    "plt.savefig(pr_curve_file)\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots and metrics have been saved to the directory: {output_dir_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85aa0e8-c74c-47a3-9257-13d720d7fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Example Data\n",
    "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_true, ensemble_pred)\n",
    "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
    "\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_true, xgb_pred)\n",
    "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "\n",
    "fpr_dnn, tpr_dnn, _ = roc_curve(y_true, dnn_pred)\n",
    "roc_auc_dnn = auc(fpr_dnn, tpr_dnn)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_true, rf_pred)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_true, knn_pred)\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "fpr_bayes, tpr_bayes, _ = roc_curve(y_true, bayes_pred)\n",
    "roc_auc_bayes = auc(fpr_bayes, tpr_bayes)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_ensemble, tpr_ensemble, color='darkorange', lw=2, label=f'Ensemble (AUC = {roc_auc_ensemble:.2f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='blue', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')\n",
    "plt.plot(fpr_dnn, tpr_dnn, color='green', lw=2, label=f'DNN (AUC = {roc_auc_dnn:.2f})')\n",
    "plt.plot(fpr_rf, tpr_rf, color='purple', lw=2, label=f'RF (AUC = {roc_auc_rf:.2f})')\n",
    "plt.plot(fpr_knn, tpr_knn, color='cyan', lw=2, label=f'KNN (AUC = {roc_auc_knn:.2f})')\n",
    "plt.plot(fpr_bayes, tpr_bayes, color='pink', lw=2, label=f'Bayesian (AUC = {roc_auc_bayes:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Combined ROC Curves for Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067b455-c739-48b2-a5bd-487f3a6f0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "class ExtremeLearningMachine:\n",
    "    def __init__(self, hidden_size=100, activation_function='relu'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def _activation(self, x):\n",
    "        return np.maximum(0, x) if self.activation_function == 'relu' else np.tanh(x)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))\n",
    "        self.biases = np.random.normal(size=(self.hidden_size,))\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), np.eye(len(np.unique(y)))[y])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return output / np.sum(output, axis=1, keepdims=True)\n",
    "\n",
    "# ELM model\n",
    "elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')\n",
    "\n",
    "# DNN model (same as above)\n",
    "dnn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Preprocess and split data\n",
    "# (Use the same `data` preprocessing and splitting as before)\n",
    "\n",
    "# Fit models\n",
    "elm_model.fit(X_train_scaled, y_train)\n",
    "dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "dnn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=0)\n",
    "\n",
    "# Predict probabilities\n",
    "elm_val_pred = elm_model.predict_proba(X_val_scaled)\n",
    "dnn_val_pred = dnn_model.predict(X_val_scaled)\n",
    "\n",
    "# Stack predictions and train meta-model\n",
    "stacked_val_pred = np.hstack((elm_val_pred, dnn_val_pred))\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_val_pred, y_val)\n",
    "\n",
    "# Predict on test set\n",
    "elm_test_pred = elm_model.predict_proba(X_test_scaled)\n",
    "dnn_test_pred = dnn_model.predict(X_test_scaled)\n",
    "stacked_test_pred = np.hstack((elm_test_pred, dnn_test_pred))\n",
    "final_predictions = meta_model.predict(stacked_test_pred)\n",
    "\n",
    "# Evaluate ensemble\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Ensemble (ELM + DNN) Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808de12-4ab0-47fd-93a5-bfaabc6433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for validation set\n",
    "xgb_val_pred = xgb_model.predict_proba(X_val_scaled)\n",
    "elm_val_pred = elm_model.predict_proba(X_val_scaled)\n",
    "dnn_val_pred = dnn_model.predict(X_val_scaled)\n",
    "\n",
    "# Stack predictions\n",
    "stacked_val_pred = np.hstack((xgb_val_pred, elm_val_pred, dnn_val_pred))\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_val_pred, y_val)\n",
    "\n",
    "# Predict on test set\n",
    "xgb_test_pred = xgb_model.predict_proba(X_test_scaled)\n",
    "elm_test_pred = elm_model.predict_proba(X_test_scaled)\n",
    "dnn_test_pred = dnn_model.predict(X_test_scaled)\n",
    "stacked_test_pred = np.hstack((xgb_test_pred, elm_test_pred, dnn_test_pred))\n",
    "final_predictions = meta_model.predict(stacked_test_pred)\n",
    "\n",
    "# Evaluate ensemble\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Ensemble (XGBoost + ELM + DNN) Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2d8bb-f242-42eb-a45c-910fcf5e2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_val_predictions = xgb_model.predict(X_val_scaled)\n",
    "xgb_test_predictions = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "val_accuracy = accuracy_score(y_val, xgb_val_predictions)\n",
    "test_accuracy = accuracy_score(y_test, xgb_test_predictions)\n",
    "\n",
    "print(f\"XGBoost Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "print(f\"XGBoost Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report (Test):\\n\", classification_report(y_test, xgb_test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cca827-95ad-4c10-be55-2cb85c041c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create a directory to save the plots if it doesn't exist\n",
    "output_dir_xgb = 'xgboost_output_plots'\n",
    "os.makedirs(output_dir_xgb, exist_ok=True)\n",
    "\n",
    "# Calculate Metrics for XGBoost\n",
    "xgb_test_predictions = xgb_model.predict(X_test_scaled)\n",
    "xgb_test_probabilities = xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_test_predictions)\n",
    "xgb_precision = precision_score(y_test, xgb_test_predictions, average='weighted')\n",
    "xgb_recall = recall_score(y_test, xgb_test_predictions, average='weighted')\n",
    "xgb_f1 = f1_score(y_test, xgb_test_predictions, average='weighted')\n",
    "xgb_roc_auc = roc_auc_score(pd.get_dummies(y_test), xgb_test_probabilities, multi_class='ovr')\n",
    "\n",
    "print(\"\\nXGBoost Metrics\")\n",
    "print(f\"• Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"• Precision: {xgb_precision:.4f}\")\n",
    "print(f\"• Recall: {xgb_recall:.4f}\")\n",
    "print(f\"• F1 Score: {xgb_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_xgb = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [xgb_accuracy, xgb_precision, xgb_recall, xgb_f1, xgb_roc_auc]\n",
    "}\n",
    "metrics_df_xgb = pd.DataFrame(metrics_data_xgb)\n",
    "metrics_file = os.path.join(output_dir_xgb, 'xgboost_metrics_results.xlsx')\n",
    "metrics_df_xgb.to_excel(metrics_file, index=False)\n",
    "print(f\"\\nMetrics saved to '{metrics_file}'.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_xgb = confusion_matrix(y_test, xgb_test_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_xgb, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues', \n",
    "    xticklabels=np.unique(y_test), \n",
    "    yticklabels=np.unique(y_test)\n",
    ")\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "conf_matrix_file = os.path.join(output_dir_xgb, 'confusion_matrix_xgb.png')\n",
    "plt.savefig(conf_matrix_file)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_xgb = {}\n",
    "tpr_xgb = {}\n",
    "roc_auc_xgb = {}\n",
    "classes_xgb = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_xgb):\n",
    "    fpr_xgb[class_label], tpr_xgb[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], xgb_test_probabilities[:, i])\n",
    "    roc_auc_xgb[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], xgb_test_probabilities[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_xgb:\n",
    "    plt.plot(fpr_xgb[class_label], tpr_xgb[class_label], label=f'Class {class_label} (AUC = {roc_auc_xgb[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "roc_curve_file = os.path.join(output_dir_xgb, 'roc_curve_xgb.png')\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_xgb = {}\n",
    "recall_xgb = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_xgb):\n",
    "    precision_xgb[class_label], recall_xgb[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], xgb_test_probabilities[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_xgb:\n",
    "    plt.plot(recall_xgb[class_label], precision_xgb[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - XGBoost')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "pr_curve_file = os.path.join(output_dir_xgb, 'precision_recall_curve_xgb.png')\n",
    "plt.savefig(pr_curve_file)\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots and metrics have been saved to the directory: {output_dir_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632aca93-c007-4d50-adef-7ec689ca5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Define the ELM class\n",
    "class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_size=100, activation_function='relu'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_function = activation_function\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_one_hot = np.zeros((len(y), num_classes))\n",
    "        y_one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))\n",
    "        self.biases = np.random.normal(size=(self.hidden_size,))\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "# Train and evaluate ELM\n",
    "elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')\n",
    "elm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "elm_val_predictions = elm_model.predict(X_val_scaled)\n",
    "elm_test_predictions = elm_model.predict(X_test_scaled)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, elm_val_predictions)\n",
    "test_accuracy = accuracy_score(y_test, elm_test_predictions)\n",
    "\n",
    "print(f\"ELM Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "print(f\"ELM Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report (Test):\\n\", classification_report(y_test, elm_test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded4054-5d74-40d5-90d6-4ebd1be6a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a directory to save the plots if it doesn't exist\n",
    "output_dir_elm = 'elm_output_plots'\n",
    "os.makedirs(output_dir_elm, exist_ok=True)\n",
    "\n",
    "# Predict using ELM\n",
    "elm_test_predictions = elm_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to probabilities-like output\n",
    "elm_test_probabilities = np.zeros((len(elm_test_predictions), len(np.unique(y_test))))\n",
    "for idx, pred in enumerate(elm_test_predictions):\n",
    "    elm_test_probabilities[idx, pred] = 1\n",
    "\n",
    "# Calculate Metrics for ELM\n",
    "elm_accuracy = accuracy_score(y_test, elm_test_predictions)\n",
    "elm_precision = precision_score(y_test, elm_test_predictions, average='weighted')\n",
    "elm_recall = recall_score(y_test, elm_test_predictions, average='weighted')\n",
    "elm_f1 = f1_score(y_test, elm_test_predictions, average='weighted')\n",
    "elm_roc_auc = roc_auc_score(pd.get_dummies(y_test), elm_test_probabilities, multi_class=\"ovr\")\n",
    "\n",
    "print(\"\\nELM Metrics\")\n",
    "print(f\"• Accuracy: {elm_accuracy:.4f}\")\n",
    "print(f\"• Precision: {elm_precision:.4f}\")\n",
    "print(f\"• Recall: {elm_recall:.4f}\")\n",
    "print(f\"• F1 Score: {elm_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {elm_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_elm = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [elm_accuracy, elm_precision, elm_recall, elm_f1, elm_roc_auc]\n",
    "}\n",
    "metrics_df_elm = pd.DataFrame(metrics_data_elm)\n",
    "metrics_file = os.path.join(output_dir_elm, 'elm_metrics_results.xlsx')\n",
    "metrics_df_elm.to_excel(metrics_file, index=False)\n",
    "print(f\"\\nMetrics saved to '{metrics_file}'.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_elm = confusion_matrix(y_test, elm_test_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_elm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=np.unique(y_test),\n",
    "    yticklabels=np.unique(y_test),\n",
    ")\n",
    "plt.title(\"Confusion Matrix - ELM\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "conf_matrix_file = os.path.join(output_dir_elm, 'confusion_matrix_elm.png')\n",
    "plt.savefig(conf_matrix_file)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_elm = {}\n",
    "tpr_elm = {}\n",
    "roc_auc_elm = {}\n",
    "classes_elm = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_elm):\n",
    "    fpr_elm[class_label], tpr_elm[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], elm_test_probabilities[:, i])\n",
    "    roc_auc_elm[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], elm_test_probabilities[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_elm:\n",
    "    plt.plot(fpr_elm[class_label], tpr_elm[class_label], label=f\"Class {class_label} (AUC = {roc_auc_elm[class_label]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal line\n",
    "plt.title(\"ROC Curve - ELM\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "roc_curve_file = os.path.join(output_dir_elm, 'roc_curve_elm.png')\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_elm = {}\n",
    "recall_elm = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_elm):\n",
    "    precision_elm[class_label], recall_elm[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], elm_test_probabilities[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_elm:\n",
    "    plt.plot(recall_elm[class_label], precision_elm[class_label], label=f\"Class {class_label}\")\n",
    "plt.title(\"Precision-Recall Curve - ELM\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "pr_curve_file = os.path.join(output_dir_elm, 'precision_recall_curve_elm.png')\n",
    "plt.savefig(pr_curve_file)\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots and metrics have been saved to the directory: {output_dir_elm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b63d3-1f48-40a4-a145-be426496255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the predicted probabilities for each model and true labels\n",
    "models = {\n",
    "    \"ELM\": elm_test_proba,\n",
    "    \"XGBoost\": xgb_test_predictions_proba,\n",
    "    \"Random Forest\": rf_test_proba,\n",
    "    \"KNN\": knn_test_proba,\n",
    "    \"Bayesian\": bayesian_test_proba,\n",
    "    \"DNN\": dnn_test_proba\n",
    "}\n",
    "\n",
    "true_labels = pd.get_dummies(y_test).values  # One-hot encoded true labels\n",
    "\n",
    "# Initialize plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Loop through models to calculate ROC curves and AUC\n",
    "for model_name, model_proba in models.items():\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(true_labels.shape[1]):  # Iterate through each class\n",
    "        fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], model_proba[:, i])\n",
    "        roc_auc[i] = roc_auc_score(true_labels[:, i], model_proba[:, i])\n",
    "\n",
    "    # Average across all classes\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.mean([np.interp(mean_fpr, fpr[i], tpr[i]) for i in range(true_labels.shape[1])], axis=0)\n",
    "    mean_auc = np.mean(list(roc_auc.values()))\n",
    "\n",
    "    # Plot the ROC curve for this model\n",
    "    plt.plot(mean_fpr, mean_tpr, label=f\"{model_name} (AUC = {mean_auc:.2f})\")\n",
    "\n",
    "# Plot random guess line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "\n",
    "# Finalize plot\n",
    "plt.title(\"ROC Curve Comparison Across Models\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.savefig(\"combined_roc_curves.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ba9be-eb83-499f-8a0b-1f2231dcbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.backend import count_params\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to calculate FLOPs for TensorFlow models\n",
    "def calculate_flops(model):\n",
    "    session = tf.compat.v1.Session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    with session.as_default():\n",
    "        with graph.as_default():\n",
    "            run_meta = tf.compat.v1.RunMetadata()\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "            flops = tf.compat.v1.profiler.profile(\n",
    "                graph=graph, run_meta=run_meta, cmd=\"op\", options=opts\n",
    "            ).total_float_ops\n",
    "    return flops\n",
    "\n",
    "# Function to calculate the number of parameters for ELM\n",
    "def calculate_elm_params(model):\n",
    "    input_params = model.input_weights.size\n",
    "    bias_params = model.biases.size\n",
    "    output_params = model.output_weights.size\n",
    "    return input_params + bias_params + output_params\n",
    "\n",
    "# Efficiency Analysis\n",
    "def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, model_params=None):\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "    if model_name == \"DNN\":\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=0)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Predict\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_accuracy = accuracy_score(y_test, np.argmax(predictions, axis=1) if model_name == \"DNN\" else predictions)\n",
    "\n",
    "    # Calculate number of parameters\n",
    "    if model_name == \"DNN\":\n",
    "        params = model.count_params()\n",
    "        flops = calculate_flops(model)\n",
    "    elif model_name == \"ELM\":\n",
    "        params = calculate_elm_params(model)\n",
    "        flops = \"Not Applicable\"\n",
    "    else:  # XGBoost\n",
    "        params = model.n_estimators * model.max_depth\n",
    "        flops = \"Not Applicable\"\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Inference Time (s)\": inference_time,\n",
    "        \"Accuracy\": test_accuracy,\n",
    "        \"Parameters\": params,\n",
    "        \"FLOPs\": flops,\n",
    "    }\n",
    "\n",
    "# Prepare analysis\n",
    "results = []\n",
    "\n",
    "# DNN Analysis\n",
    "dnn_model = create_model(\n",
    "    hidden_layers=2,\n",
    "    hidden_nodes=20,\n",
    "    input_shape=(X_train_scaled.shape[1],),\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adam\",\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "dnn_result = efficiency_analysis(dnn_model, \"DNN\", X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "results.append(dnn_result)\n",
    "\n",
    "# XGBoost Analysis\n",
    "xgb_result = efficiency_analysis(xgb_model, \"XGBoost\", X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "results.append(xgb_result)\n",
    "\n",
    "# ELM Analysis\n",
    "elm_result = efficiency_analysis(elm_model, \"ELM\", X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "results.append(elm_result)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "efficiency_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "efficiency_df.to_excel(\"efficiency_analysis_results.xlsx\", index=False)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\nEfficiency Analysis Results:\")\n",
    "print(efficiency_df)\n",
    "\n",
    "# Visualization\n",
    "metrics = [\"Train Time (s)\", \"Inference Time (s)\", \"Accuracy\", \"Parameters\"]\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(efficiency_df[\"Model\"], efficiency_df[metric], color=[\"blue\", \"green\", \"orange\"])\n",
    "    plt.title(f\"{metric} Comparison\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bef3b7-ecbc-419e-bcef-9a779b846389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Adjust model predictions for classification\n",
    "def adjust_predictions_for_classification(model, X_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    if len(predictions.shape) > 1:  # Probabilistic outputs\n",
    "        return predictions.argmax(axis=1)  # Convert probabilities to class labels\n",
    "    return predictions\n",
    "\n",
    "# Efficiency analysis function with adjusted predictions\n",
    "def efficiency_analysis(model, model_name, X_train, y_train, X_test, y_test, flops=None):\n",
    "    memory_before = get_memory_usage()\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    memory_after = get_memory_usage()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predictions = adjust_predictions_for_classification(model, X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    throughput = measure_throughput(model, X_test)\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Inference Time (s)\": inference_time,\n",
    "        \"Accuracy\": accuracy_score(y_test, predictions),\n",
    "        \"Parameters\": sum([np.prod(v.shape) for v in model.get_weights()]) if hasattr(model, \"get_weights\") else \"N/A\",\n",
    "        \"Memory (MB)\": memory_after - memory_before,\n",
    "        \"Throughput (Pred/s)\": throughput,\n",
    "        \"FLOPs\": flops or \"Not Applicable\"\n",
    "    }\n",
    "\n",
    "# Perform efficiency analysis for DNN\n",
    "dnn_flops = calculate_flops_dnn(\n",
    "    hidden_layers=dnn_hidden_layers,\n",
    "    hidden_nodes=dnn_hidden_nodes,\n",
    "    input_size=X_train.shape[1],\n",
    "    output_classes=len(np.unique(y_train))\n",
    ")\n",
    "\n",
    "dnn_result = efficiency_analysis(\n",
    "    best_model,\n",
    "    \"DNN\",\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_test_scaled,\n",
    "    y_test,\n",
    "    flops=dnn_flops\n",
    ")\n",
    "\n",
    "# Perform efficiency analysis for XGBoost\n",
    "xgb_result = efficiency_analysis(\n",
    "    xgb_model,\n",
    "    \"XGBoost\",\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_test_scaled,\n",
    "    y_test\n",
    ")\n",
    "\n",
    "# Perform efficiency analysis for ELM\n",
    "elm_flops = calculate_flops_elm(\n",
    "    input_size=X_train.shape[1],\n",
    "    hidden_neurons=100,\n",
    "    output_classes=len(np.unique(y_train))\n",
    ")\n",
    "\n",
    "elm_result = efficiency_analysis(\n",
    "    elm_model,\n",
    "    \"ELM\",\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_test_scaled,\n",
    "    y_test,\n",
    "    flops=elm_flops\n",
    ")\n",
    "\n",
    "# Compile and save results\n",
    "results = [dnn_result, xgb_result, elm_result]\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"efficiency_analysis.xlsx\", index=False)\n",
    "\n",
    "# Print results\n",
    "print(results_df)\n",
    "\n",
    "# Plot Comparative Histogram\n",
    "results_df.set_index(\"Model\")[[\"Train Time (s)\", \"Inference Time (s)\", \"Memory (MB)\", \"Throughput (Pred/s)\"]].plot(kind=\"bar\", figsize=(10, 6))\n",
    "plt.title(\"Efficiency Analysis\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51135c-e9ad-4bb8-a7a8-3c3a129bdab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ensure feature columns are numeric\n",
    "def preprocess_features(data):\n",
    "    for column in data.columns:\n",
    "        if isinstance(data[column].iloc[0], (list, np.ndarray)):\n",
    "            data[column] = data[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "    return data\n",
    "\n",
    "# Preprocess the features\n",
    "X = preprocess_features(data.drop('Label', axis=1))\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Normalize features\n",
    "X = X.values.astype(np.float32)\n",
    "\n",
    "# Apply SMOTE for class imbalance\n",
    "smote = SMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the data into 70% training, 20% validation, and 10% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.1, random_state=1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2222, random_state=1\n",
    ")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define base models\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=1)\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "\n",
    "# Define a custom ELM class\n",
    "class ExtremeLearningMachine(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_size=100, activation_function='relu'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_function = activation_function\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_one_hot = np.zeros((len(y), num_classes))\n",
    "        y_one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "        self.input_weights = np.random.normal(size=(X.shape[1], self.hidden_size))\n",
    "        self.biases = np.random.normal(size=(self.hidden_size,))\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y_one_hot)\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "elm_model = ExtremeLearningMachine(hidden_size=100, activation_function='relu')\n",
    "\n",
    "# Fit base models\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "elm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using base models\n",
    "rf_predictions = rf_model.predict_proba(X_val_scaled)\n",
    "xgb_predictions = xgb_model.predict_proba(X_val_scaled)\n",
    "elm_predictions = elm_model.predict(X_val_scaled)\n",
    "elm_proba = np.zeros((len(elm_predictions), len(np.unique(y_train))))  # Convert ELM to probability-like output\n",
    "for idx, pred in enumerate(elm_predictions):\n",
    "    elm_proba[idx, pred] = 1\n",
    "\n",
    "# Stack predictions\n",
    "stacked_predictions = np.hstack((rf_predictions, xgb_predictions, elm_proba))\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_predictions, y_val)\n",
    "\n",
    "# Evaluate the ensemble on test data\n",
    "rf_test_predictions = rf_model.predict_proba(X_test_scaled)\n",
    "xgb_test_predictions = xgb_model.predict_proba(X_test_scaled)\n",
    "elm_test_predictions = elm_model.predict(X_test_scaled)\n",
    "elm_test_proba = np.zeros((len(elm_test_predictions), len(np.unique(y_test))))\n",
    "for idx, pred in enumerate(elm_test_predictions):\n",
    "    elm_test_proba[idx, pred] = 1\n",
    "\n",
    "stacked_test_predictions = np.hstack((rf_test_predictions, xgb_test_predictions, elm_test_proba))\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "\n",
    "# Evaluate performance\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Ensemble Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2829da-671e-40e2-95e5-b7d08db90d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Verify dimensions of predictions\n",
    "print(f\"Shape of stacked_test_predictions: {stacked_test_predictions.shape}\")\n",
    "print(f\"Number of unique classes in y_test: {len(np.unique(y_test))}\")\n",
    "\n",
    "# Ensure the probabilities align with 8 classes\n",
    "n_classes = 8\n",
    "if stacked_test_predictions.shape[1] != n_classes:\n",
    "    print(\"Adjusting probabilities to match 8 classes.\")\n",
    "    adjusted_proba = np.zeros((stacked_test_predictions.shape[0], n_classes))\n",
    "    for i in range(stacked_test_predictions.shape[0]):\n",
    "        for j in range(min(stacked_test_predictions.shape[1], n_classes)):\n",
    "            adjusted_proba[i, j] = stacked_test_predictions[i, j]\n",
    "    stacked_test_predictions = adjusted_proba\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(y_test, final_predictions)\n",
    "test_precision = precision_score(y_test, final_predictions, average=\"weighted\")\n",
    "test_recall = recall_score(y_test, final_predictions, average=\"weighted\")\n",
    "test_f1 = f1_score(y_test, final_predictions, average=\"weighted\")\n",
    "test_roc_auc = roc_auc_score(pd.get_dummies(y_test), stacked_test_predictions, multi_class=\"ovr\")\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nEnsemble Model Metrics\")\n",
    "print(f\"• Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"• Precision: {test_precision:.4f}\")\n",
    "print(f\"• Recall: {test_recall:.4f}\")\n",
    "print(f\"• F1 Score: {test_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, final_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=np.arange(n_classes),\n",
    "    yticklabels=np.arange(n_classes),\n",
    ")\n",
    "plt.title(\"Confusion Matrix - Ensemble Model\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "classes = range(n_classes)\n",
    "\n",
    "for i in classes:\n",
    "    fpr[i], tpr[i], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])\n",
    "    roc_auc[i] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in classes:\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal line\n",
    "plt.title(\"ROC Curve - Ensemble Model\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75bd3ae-6f31-4ebb-b39c-53893e6a161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision = {}\n",
    "recall = {}\n",
    "average_precision = {}\n",
    "\n",
    "# Calculate Precision-Recall for each class\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])\n",
    "    average_precision[i] = average_precision_score(pd.get_dummies(y_test).iloc[:, i], stacked_test_predictions[:, i])\n",
    "\n",
    "# Plot Precision-Recall Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in classes:\n",
    "    plt.plot(recall[i], precision[i], label=f'Class {i} (AP = {average_precision[i]:.2f})')\n",
    "plt.title(\"Precision-Recall Curve - Ensemble Model\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print Average Precision Scores for each class\n",
    "print(\"\\nAverage Precision Scores:\")\n",
    "for i in classes:\n",
    "    print(f\"Class {i}: {average_precision[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f00ba2-205f-407f-ac4a-43a9c1b6d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the XGBoost Model\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_test_proba_xgb = xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate Metrics\n",
    "xgb_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
    "xgb_precision = precision_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "xgb_recall = recall_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "xgb_f1 = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "xgb_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba_xgb, multi_class='ovr')\n",
    "\n",
    "print(\"\\nXGBoost Classifier Metrics\")\n",
    "print(f\"• Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"• Precision: {xgb_precision:.4f}\")\n",
    "print(f\"• Recall: {xgb_recall:.4f}\")\n",
    "print(f\"• F1 Score: {xgb_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_test_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_xgb, tpr_xgb, roc_auc_xgb = {}, {}, {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr_xgb[class_label], tpr_xgb[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_xgb[:, i])\n",
    "    roc_auc_xgb[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba_xgb[:, i])\n",
    "    plt.plot(fpr_xgb[class_label], tpr_xgb[class_label], label=f'Class {class_label} (AUC = {roc_auc_xgb[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_xgb, recall_xgb = {}, {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision_xgb[class_label], recall_xgb[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_xgb[:, i])\n",
    "    plt.plot(recall_xgb[class_label], precision_xgb[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - XGBoost')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb230ed2-902d-44d8-8565-2ae10edfccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the ELM Model\n",
    "y_test_pred_elm = elm_model.predict(X_test_scaled)\n",
    "\n",
    "# Since ELM doesn't directly provide probabilities, convert predictions to probabilities-like output\n",
    "elm_proba = np.zeros((len(y_test_pred_elm), len(classes)))\n",
    "for idx, pred in enumerate(y_test_pred_elm):\n",
    "    elm_proba[idx, pred] = 1\n",
    "\n",
    "# Calculate Metrics\n",
    "elm_accuracy = accuracy_score(y_test, y_test_pred_elm)\n",
    "elm_precision = precision_score(y_test, y_test_pred_elm, average='weighted')\n",
    "elm_recall = recall_score(y_test, y_test_pred_elm, average='weighted')\n",
    "elm_f1 = f1_score(y_test, y_test_pred_elm, average='weighted')\n",
    "elm_roc_auc = roc_auc_score(pd.get_dummies(y_test), elm_proba, multi_class='ovr')\n",
    "\n",
    "print(\"\\nELM Classifier Metrics\")\n",
    "print(f\"• Accuracy: {elm_accuracy:.4f}\")\n",
    "print(f\"• Precision: {elm_precision:.4f}\")\n",
    "print(f\"• Recall: {elm_recall:.4f}\")\n",
    "print(f\"• F1 Score: {elm_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {elm_roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_elm = confusion_matrix(y_test, y_test_pred_elm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_elm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - ELM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_elm, tpr_elm, roc_auc_elm = {}, {}, {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr_elm[class_label], tpr_elm[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], elm_proba[:, i])\n",
    "    roc_auc_elm[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], elm_proba[:, i])\n",
    "    plt.plot(fpr_elm[class_label], tpr_elm[class_label], label=f'Class {class_label} (AUC = {roc_auc_elm[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve - ELM')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_elm, recall_elm = {}, {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision_elm[class_label], recall_elm[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], elm_proba[:, i])\n",
    "    plt.plot(recall_elm[class_label], precision_elm[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - ELM')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c7d50-5883-4413-9291-81c92064cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare dictionaries to store FPR, TPR, and AUC for all models\n",
    "models = {\n",
    "    \"Ensemble\": stacked_test_predictions,\n",
    "    \"Random Forest\": rf_metrics_results,\n",
    "    \"DNN\": dnn_metrics_results,  # Assuming predictions are stored as dnn_test_predictions\n",
    "    \"KNN\": knn_metrics_results,  # Assuming predictions are stored as knn_test_predictions\n",
    "    \"XGBoost\": xgb_test_predictions,\n",
    "    \"Bayesian\": nb_metrics_results,  # Assuming predictions are stored as bayesian_test_predictions\n",
    "    \"ELM\": elm_test_proba\n",
    "}\n",
    "\n",
    "# Initialize ROC storage\n",
    "roc_results = {}\n",
    "\n",
    "# Generate ROC and AUC for each model\n",
    "for model_name, model_proba in models.items():\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(n_classes):  # Iterate through each class\n",
    "        fpr[i], tpr[i], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], model_proba[:, i])\n",
    "        roc_auc[i] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], model_proba[:, i])\n",
    "\n",
    "    roc_results[model_name] = {\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot each model's macro-average ROC\n",
    "for model_name, results in roc_results.items():\n",
    "    macro_avg_auc = np.mean(list(results[\"roc_auc\"].values()))\n",
    "    plt.plot(\n",
    "        results[\"fpr\"][0],  # Use the first class for uniform visualization\n",
    "        results[\"tpr\"][0],\n",
    "        label=f\"{model_name} (Macro-AUC = {macro_avg_auc:.2f})\"\n",
    "    )\n",
    "\n",
    "# Add a diagonal line for random classifier\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"ROC Curve Comparison Across All Models\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC for each model\n",
    "print(\"\\nROC AUC Scores for All Models:\")\n",
    "for model_name, results in roc_results.items():\n",
    "    macro_avg_auc = np.mean(list(results[\"roc_auc\"].values()))\n",
    "    print(f\"{model_name}: Macro-AUC = {macro_avg_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cbfeb-966b-4fa3-b655-e9a7d1b01811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address class imbalance using SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "\n",
    "smote = SMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the data into 70% training, 20% validation, and 10% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.1, random_state=1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2222, random_state=1\n",
    ")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "n_estimators_list = [100, 200]\n",
    "max_depth_list = [10, 20]\n",
    "criterion_list = ['gini', 'entropy']\n",
    "\n",
    "# Store results for comparison\n",
    "rf_results = []\n",
    "\n",
    "# Train and evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cda90-702c-4d3e-b600-4ce5d8d20533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest models with different hyperparameters\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for criterion in criterion_list:\n",
    "            # Initialize the Random Forest model\n",
    "            rf_model = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                criterion=criterion,\n",
    "                random_state=1\n",
    "            )\n",
    "\n",
    "            # Train the model on the training set\n",
    "            rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            y_val_pred = rf_model.predict(X_val_scaled)\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "            # Store the results\n",
    "            rf_results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'criterion': criterion,\n",
    "                'Validation Accuracy': val_accuracy\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "# Find the best configuration based on validation accuracy\n",
    "best_rf_result = rf_results_df.loc[rf_results_df['Validation Accuracy'].idxmax()]\n",
    "print(\"\\nBest Random Forest configuration found:\")\n",
    "print(best_rf_result)\n",
    "\n",
    "# Train the final model with the best hyperparameters on combined training and validation data\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_rf_result['n_estimators'],\n",
    "    max_depth=best_rf_result['max_depth'],\n",
    "    criterion=best_rf_result['criterion'],\n",
    "    random_state=1\n",
    ")\n",
    "best_rf_model.fit(scaler.transform(X_train_val), y_train_val)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_test_pred = best_rf_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with the best configuration: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'rf_model_results.xlsx'\n",
    "rf_results_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to '{output_file}' for download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7978bec-454b-4b3a-b7ba-dfa4baca3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scale combined training and validation data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training and validation data\n",
    "X_train_val_scaled = scaler.fit_transform(X_train_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Random Forest results from Excel\n",
    "rf_results_file = 'rf_model_results.xlsx'  # Update with your file path\n",
    "rf_results_df = pd.read_excel(rf_results_file)\n",
    "\n",
    "# Select the best configuration based on validation accuracy\n",
    "best_rf_result = rf_results_df.loc[rf_results_df['Validation Accuracy'].idxmax()]\n",
    "print(\"\\nBest Random Forest configuration found:\")\n",
    "print(best_rf_result)\n",
    "\n",
    "# Extract best hyperparameters\n",
    "n_estimators = int(best_rf_result['n_estimators'])\n",
    "max_depth = int(best_rf_result['max_depth']) if not pd.isnull(best_rf_result['max_depth']) else None\n",
    "criterion = best_rf_result['criterion']\n",
    "\n",
    "# Train the final model with the best configuration on combined training and validation data\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    criterion=criterion,\n",
    "    random_state=1\n",
    ")\n",
    "best_rf_model.fit(X_train_val_scaled, y_train_val)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_test_pred = best_rf_model.predict(X_test_scaled)\n",
    "y_test_proba = best_rf_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate Metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "rf_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "rf_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "rf_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "rf_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr')\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Metrics\")\n",
    "print(f\"• Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"• Precision: {rf_precision:.4f}\")\n",
    "print(f\"• Recall: {rf_recall:.4f}\")\n",
    "print(f\"• F1 Score: {rf_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {rf_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_excel('rf_metrics_results.xlsx', index=False)\n",
    "print(\"\\nMetrics saved to 'rf_metrics_results.xlsx' for download.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "classes = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])\n",
    "    roc_auc[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes:\n",
    "    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision = {}\n",
    "recall = {}\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes:\n",
    "    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Random Forest')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c83d49-f58c-4353-8748-745825df75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load hyperparameter tuning results from the Excel file\n",
    "results_file = 'hyperparameter_tuning_results.xlsx'  # Replace with your actual file path\n",
    "results_df = pd.read_excel(results_file)\n",
    "\n",
    "# Select the best configuration based on mean validation accuracy\n",
    "best_result = results_df.loc[results_df['mean_val_accuracy'].idxmax()]\n",
    "print(\"\\nBest Deep Neural Network Configuration Found:\")\n",
    "print(best_result)\n",
    "\n",
    "# Extract hyperparameters from the best configuration\n",
    "hidden_layers = int(best_result['hidden_layers'])\n",
    "hidden_nodes = int(best_result['hidden_nodes'])\n",
    "learning_rate = best_result['learning_rate']\n",
    "optimizer = best_result['optimizer']\n",
    "activation = best_result['activation']\n",
    "\n",
    "# Assuming predictions are already available from the best model\n",
    "# Replace `y_test_dl_pred_proba` and `y_test_dl_pred` with actual data\n",
    "# Example placeholders for predictions:\n",
    "# y_test_dl_pred_proba = np.load('y_test_dl_pred_proba.npy')  # Load saved predicted probabilities\n",
    "# y_test_dl_pred = np.argmax(y_test_dl_pred_proba, axis=1)   # Predicted labels\n",
    "\n",
    "# Calculate Metrics\n",
    "dnn_accuracy = accuracy_score(y_test, y_test_dl_pred)\n",
    "dnn_precision = precision_score(y_test, y_test_dl_pred, average='weighted')\n",
    "dnn_recall = recall_score(y_test, y_test_dl_pred, average='weighted')\n",
    "dnn_f1 = f1_score(y_test, y_test_dl_pred, average='weighted')\n",
    "dnn_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_dl_pred_proba, multi_class='ovr')\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nDeep Neural Network Metrics\")\n",
    "print(f\"• Accuracy: {dnn_accuracy:.4f}\")\n",
    "print(f\"• Precision: {dnn_precision:.4f}\")\n",
    "print(f\"• Recall: {dnn_recall:.4f}\")\n",
    "print(f\"• F1 Score: {dnn_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {dnn_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_dnn = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [dnn_accuracy, dnn_precision, dnn_recall, dnn_f1, dnn_roc_auc]\n",
    "}\n",
    "metrics_df_dnn = pd.DataFrame(metrics_data_dnn)\n",
    "metrics_df_dnn.to_excel('dnn_metrics_results.xlsx', index=False)\n",
    "print(\"\\nMetrics saved to 'dnn_metrics_results.xlsx' for download.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_dnn = confusion_matrix(y_test, y_test_dl_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - Deep Neural Network')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.savefig('confusion_matrix_dnn.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_dnn = {}\n",
    "tpr_dnn = {}\n",
    "roc_auc_dnn = {}\n",
    "classes_dnn = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_dnn):\n",
    "    fpr_dnn[class_label], tpr_dnn[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])\n",
    "    roc_auc_dnn[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_dnn:\n",
    "    plt.plot(fpr_dnn[class_label], tpr_dnn[class_label], label=f'Class {class_label} (AUC = {roc_auc_dnn[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Deep Neural Network')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve_dnn.png')\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_dnn = {}\n",
    "recall_dnn = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_dnn):\n",
    "    precision_dnn[class_label], recall_dnn[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_dnn:\n",
    "    plt.plot(recall_dnn[class_label], precision_dnn[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Deep Neural Network')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.savefig('precision_recall_curve_dnn.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlots and metrics saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0f49a-4016-4971-9a06-93ca4304c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "import pandas as pd\n",
    "\n",
    "# Create a directory to save the plots if it doesn't exist\n",
    "output_dir_rf = 'rf_output_plots'\n",
    "os.makedirs(output_dir_rf, exist_ok=True)\n",
    "\n",
    "# # Calculate Metrics\n",
    "# rf_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# rf_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "# rf_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "# rf_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "# rf_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr')\n",
    "\n",
    "# print(\"\\nRandom Forest Classifier Metrics\")\n",
    "# print(f\"• Accuracy: {rf_accuracy:.4f}\")\n",
    "# print(f\"• Precision: {rf_precision:.4f}\")\n",
    "# print(f\"• Recall: {rf_recall:.4f}\")\n",
    "# print(f\"• F1 Score: {rf_f1:.4f}\")\n",
    "# print(f\"• ROC AUC: {rf_roc_auc:.4f}\")\n",
    "\n",
    "# # Save Metrics to Excel\n",
    "# metrics_data = {\n",
    "#     'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "#     'Value': [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]\n",
    "# }\n",
    "# metrics_df = pd.DataFrame(metrics_data)\n",
    "# metrics_file = os.path.join(output_dir_rf, 'rf_metrics_results.xlsx')\n",
    "# metrics_df.to_excel(metrics_file, index=False)\n",
    "# print(f\"\\nMetrics saved to '{metrics_file}' for download.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "conf_matrix_file = os.path.join(output_dir_rf, 'confusion_matrix_rf.png')\n",
    "plt.savefig(conf_matrix_file)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "classes = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr[class_label], tpr[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])\n",
    "    roc_auc[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes:\n",
    "    plt.plot(fpr[class_label], tpr[class_label], label=f'Class {class_label} (AUC = {roc_auc[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "roc_curve_file = os.path.join(output_dir_rf, 'roc_curve_rf.png')\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision = {}\n",
    "recall = {}\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision[class_label], recall[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes:\n",
    "    plt.plot(recall[class_label], precision[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Random Forest')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "pr_curve_file = os.path.join(output_dir_rf, 'precision_recall_curve_rf.png')\n",
    "plt.savefig(pr_curve_file)\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots and metrics have been saved to the directory: {output_dir_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9188f3-a04e-4d54-92bf-f0e5fbb4f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load DNN results\n",
    "dnn_results_file = 'hyperparameter_tuning_results.xlsx'  # Update the file path if needed\n",
    "dnn_results_df = pd.read_excel(dnn_results_file)\n",
    "\n",
    "# Find the best DNN model configuration based on validation accuracy\n",
    "best_dnn_result = dnn_results_df.loc[dnn_results_df['mean_val_accuracy'].idxmax()]\n",
    "print(\"\\nBest DNN Configuration Found:\")\n",
    "print(best_dnn_result)\n",
    "\n",
    "# Load Random Forest results\n",
    "rf_results_file = 'rf_model_results.xlsx'  # Update the file path if needed\n",
    "rf_results_df = pd.read_excel(rf_results_file)\n",
    "\n",
    "# Find the best Random Forest model configuration based on validation accuracy\n",
    "best_rf_result = rf_results_df.loc[rf_results_df['Validation Accuracy'].idxmax()]\n",
    "print(\"\\nBest Random Forest Configuration Found:\")\n",
    "print(best_rf_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa60a5-5b45-4ee6-903b-4f0c476c9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load hyperparameter tuning results from the Excel file\n",
    "results_file = 'hyperparameter_tuning_results.xlsx'  # Replace with your actual file path\n",
    "results_df = pd.read_excel(results_file)\n",
    "\n",
    "# Select the best configuration based on mean validation accuracy\n",
    "best_result = results_df.loc[results_df['mean_val_accuracy'].idxmax()]\n",
    "print(\"\\nBest Deep Neural Network Configuration Found:\")\n",
    "print(best_result)\n",
    "\n",
    "# # Extract hyperparameters from the best configuration\n",
    "# hidden_layers = int(best_result['hidden_layers'])\n",
    "# hidden_nodes = int(best_result['hidden_nodes'])\n",
    "# learning_rate = best_result['learning_rate']\n",
    "# optimizer = best_result['optimizer']\n",
    "# activation = best_result['activation']\n",
    "\n",
    "# # Create and train the best model with the selected configuration\n",
    "# best_model = create_model(\n",
    "#     hidden_layers=hidden_layers,\n",
    "#     hidden_nodes=hidden_nodes,\n",
    "#     input_shape=(X.shape[1],),\n",
    "#     learning_rate=learning_rate,\n",
    "#     optimizer=optimizer,\n",
    "#     activation=activation\n",
    "# )\n",
    "\n",
    "# # Train the best model on the combined training and validation data\n",
    "# history = best_model.fit(X_train_val, y_train_val, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# # Evaluate the best model on the test set\n",
    "\n",
    "\n",
    "# Train the best model with optimal parameters on training and validation data\n",
    "best_model = create_model(\n",
    "    best_result['hidden_layers'],\n",
    "    best_result['hidden_nodes'],\n",
    "    (X.shape[1],),\n",
    "    best_result['learning_rate'],\n",
    "    best_result['optimizer'],\n",
    "    best_result['activation']\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_val, y_train_val, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy with best configuration: {test_accuracy * 100:.2f}%\")\n",
    "y_test_dl_pred_proba = best_model.predict(X_test)\n",
    "y_test_dl_pred = np.argmax(y_test_dl_pred_proba, axis=1)\n",
    "\n",
    "# Calculate Metrics\n",
    "dnn_accuracy = accuracy_score(y_test, y_test_dl_pred)\n",
    "dnn_precision = precision_score(y_test, y_test_dl_pred, average='weighted')\n",
    "dnn_recall = recall_score(y_test, y_test_dl_pred, average='weighted')\n",
    "dnn_f1 = f1_score(y_test, y_test_dl_pred, average='weighted')\n",
    "dnn_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_dl_pred_proba, multi_class='ovr')\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nDeep Neural Network Metrics\")\n",
    "print(f\"• Accuracy: {dnn_accuracy:.4f}\")\n",
    "print(f\"• Precision: {dnn_precision:.4f}\")\n",
    "print(f\"• Recall: {dnn_recall:.4f}\")\n",
    "print(f\"• F1 Score: {dnn_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {dnn_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_dnn = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [dnn_accuracy, dnn_precision, dnn_recall, dnn_f1, dnn_roc_auc]\n",
    "}\n",
    "metrics_df_dnn = pd.DataFrame(metrics_data_dnn)\n",
    "metrics_df_dnn.to_excel('dnn_metrics_results.xlsx', index=False)\n",
    "print(\"\\nMetrics saved to 'dnn_metrics_results.xlsx' for download.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_dnn = confusion_matrix(y_test, y_test_dl_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - Deep Neural Network')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_dnn = {}\n",
    "tpr_dnn = {}\n",
    "roc_auc_dnn = {}\n",
    "classes_dnn = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_dnn):\n",
    "    fpr_dnn[class_label], tpr_dnn[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])\n",
    "    roc_auc_dnn[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_dnn:\n",
    "    plt.plot(fpr_dnn[class_label], tpr_dnn[class_label], label=f'Class {class_label} (AUC = {roc_auc_dnn[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Deep Neural Network')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_dnn = {}\n",
    "recall_dnn = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_dnn):\n",
    "    precision_dnn[class_label], recall_dnn[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_dl_pred_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_dnn:\n",
    "    plt.plot(recall_dnn[class_label], precision_dnn[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Deep Neural Network')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a02d38-b7cd-45b2-b57f-ff42ff268075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a5e0b-11ea-4fa8-9898-49bf48f4ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to save the plots if it doesn't exist\n",
    "output_dir = 'dnn_output_plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_dnn = confusion_matrix(y_test, y_test_dl_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - Deep Neural Network')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.savefig(os.path.join(output_dir, 'confusion_matrix_dnn.png'))\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_dnn:\n",
    "    plt.plot(fpr_dnn[class_label], tpr_dnn[class_label], label=f'Class {class_label} (AUC = {roc_auc_dnn[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Deep Neural Network')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'roc_curve_dnn.png'))\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_dnn:\n",
    "    plt.plot(recall_dnn[class_label], precision_dnn[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Deep Neural Network')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'precision_recall_curve_dnn.png'))\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots have been saved to the directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e76dc5-9bd6-4f9a-817e-c1c570a6554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "n_neighbors_list = [3, 5, 7, 9]\n",
    "weights_list = ['uniform', 'distance']\n",
    "metric_list = ['euclidean', 'manhattan']\n",
    "\n",
    "# Store results for comparison\n",
    "knn_results = []\n",
    "\n",
    "# Train and evaluate KNN\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    for weights in weights_list:\n",
    "        for metric in metric_list:\n",
    "            # Initialize the KNN model\n",
    "            knn_model = KNeighborsClassifier(\n",
    "                n_neighbors=n_neighbors,\n",
    "                weights=weights,\n",
    "                metric=metric\n",
    "            )\n",
    "\n",
    "            # Train the model on the training set\n",
    "            knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            y_val_pred = knn_model.predict(X_val_scaled)\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "            # Store the results\n",
    "            knn_results.append({\n",
    "                'n_neighbors': n_neighbors,\n",
    "                'weights': weights,\n",
    "                'metric': metric,\n",
    "                'Validation Accuracy': val_accuracy\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "knn_results_df = pd.DataFrame(knn_results)\n",
    "\n",
    "# Find the best configuration based on validation accuracy\n",
    "best_knn_result = knn_results_df.loc[knn_results_df['Validation Accuracy'].idxmax()]\n",
    "print(\"\\nBest KNN configuration found:\")\n",
    "print(best_knn_result)\n",
    "\n",
    "# Train the final model with the best hyperparameters on combined training and validation data\n",
    "best_knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=best_knn_result['n_neighbors'],\n",
    "    weights=best_knn_result['weights'],\n",
    "    metric=best_knn_result['metric']\n",
    ")\n",
    "best_knn_model.fit(scaler.transform(X_train_val), y_train_val)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_test_pred = best_knn_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with the best KNN configuration: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'knn_model_results.xlsx'\n",
    "knn_results_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to '{output_file}' for download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1f089-3268-42db-a07d-2048bf0dfbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create a directory to save the plots if it doesn't exist\n",
    "output_dir_knn = 'knn_output_plots'\n",
    "os.makedirs(output_dir_knn, exist_ok=True)\n",
    "\n",
    "# Calculate Metrics\n",
    "knn_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "knn_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "knn_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "knn_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "knn_roc_auc = roc_auc_score(pd.get_dummies(y_test), best_knn_model.predict_proba(X_test_scaled), multi_class='ovr')\n",
    "\n",
    "print(\"\\nKNN Metrics\")\n",
    "print(f\"• Accuracy: {knn_accuracy:.4f}\")\n",
    "print(f\"• Precision: {knn_precision:.4f}\")\n",
    "print(f\"• Recall: {knn_recall:.4f}\")\n",
    "print(f\"• F1 Score: {knn_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {knn_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_knn = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_roc_auc]\n",
    "}\n",
    "metrics_df_knn = pd.DataFrame(metrics_data_knn)\n",
    "metrics_file = os.path.join(output_dir_knn, 'knn_metrics_results.xlsx')\n",
    "metrics_df_knn.to_excel(metrics_file, index=False)\n",
    "print(f\"\\nMetrics saved to '{metrics_file}'.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - KNN')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "conf_matrix_file = os.path.join(output_dir_knn, 'confusion_matrix_knn.png')\n",
    "plt.savefig(conf_matrix_file)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_knn = {}\n",
    "tpr_knn = {}\n",
    "roc_auc_knn = {}\n",
    "classes_knn = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_knn):\n",
    "    fpr_knn[class_label], tpr_knn[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], best_knn_model.predict_proba(X_test_scaled)[:, i])\n",
    "    roc_auc_knn[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], best_knn_model.predict_proba(X_test_scaled)[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_knn:\n",
    "    plt.plot(fpr_knn[class_label], tpr_knn[class_label], label=f'Class {class_label} (AUC = {roc_auc_knn[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - KNN')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "roc_curve_file = os.path.join(output_dir_knn, 'roc_curve_knn.png')\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_knn = {}\n",
    "recall_knn = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_knn):\n",
    "    precision_knn[class_label], recall_knn[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], best_knn_model.predict_proba(X_test_scaled)[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_knn:\n",
    "    plt.plot(recall_knn[class_label], precision_knn[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - KNN')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "pr_curve_file = os.path.join(output_dir_knn, 'precision_recall_curve_knn.png')\n",
    "plt.savefig(pr_curve_file)\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots and metrics have been saved to the directory: {output_dir_knn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a8aee-e5b2-40d5-a21b-ee5f49379443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Feature scaling (Naive Bayes variants may require different scaling methods)\n",
    "scalers = {'StandardScaler': StandardScaler(), 'MinMaxScaler': MinMaxScaler()}\n",
    "\n",
    "# Store results for comparison\n",
    "nb_results = []\n",
    "\n",
    "# Define Naive Bayes variants\n",
    "nb_variants = {\n",
    "    'GaussianNB': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate Naive Bayes classifiers\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    for variant_name, nb_model in nb_variants.items():\n",
    "        # Train the model\n",
    "        nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        y_val_pred = nb_model.predict(X_val_scaled)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        # Store the results\n",
    "        nb_results.append({\n",
    "            'Scaler': scaler_name,\n",
    "            'Variant': variant_name,\n",
    "            'Validation Accuracy': val_accuracy\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "nb_results_df = pd.DataFrame(nb_results)\n",
    "\n",
    "# Find the best configuration based on validation accuracy\n",
    "best_nb_result = nb_results_df.loc[nb_results_df['Validation Accuracy'].idxmax()]\n",
    "print(\"\\nBest Naive Bayes configuration found:\")\n",
    "print(best_nb_result)\n",
    "\n",
    "# Train the final model with the best configuration on combined training and validation data\n",
    "best_scaler = scalers[best_nb_result['Scaler']]\n",
    "X_train_val_scaled = best_scaler.fit_transform(X_train_val)\n",
    "X_test_scaled = best_scaler.transform(X_test)\n",
    "\n",
    "best_nb_model = nb_variants[best_nb_result['Variant']]\n",
    "best_nb_model.fit(X_train_val_scaled, y_train_val)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_test_pred = best_nb_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with the best Naive Bayes configuration: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'nb_model_results.xlsx'\n",
    "nb_results_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to '{output_file}' for download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9716de4-5fd3-4d4d-afa7-59709e335d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create a directory to save the plots if it doesn't exist\n",
    "output_dir_bayes = 'bayes_output_plots'\n",
    "os.makedirs(output_dir_bayes, exist_ok=True)\n",
    "\n",
    "# Predictions for Naive Bayes\n",
    "y_test_proba_bayes = best_nb_model.predict_proba(X_test_scaled)  # Predicted probabilities\n",
    "y_test_pred_bayes = best_nb_model.predict(X_test_scaled)        # Predicted labels\n",
    "\n",
    "# Calculate Metrics\n",
    "bayes_accuracy = accuracy_score(y_test, y_test_pred_bayes)\n",
    "bayes_precision = precision_score(y_test, y_test_pred_bayes, average='weighted')\n",
    "bayes_recall = recall_score(y_test, y_test_pred_bayes, average='weighted')\n",
    "bayes_f1 = f1_score(y_test, y_test_pred_bayes, average='weighted')\n",
    "bayes_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_test_proba_bayes, multi_class='ovr')\n",
    "\n",
    "print(\"\\nBayesian Classifier Metrics\")\n",
    "print(f\"• Accuracy: {bayes_accuracy:.4f}\")\n",
    "print(f\"• Precision: {bayes_precision:.4f}\")\n",
    "print(f\"• Recall: {bayes_recall:.4f}\")\n",
    "print(f\"• F1 Score: {bayes_f1:.4f}\")\n",
    "print(f\"• ROC AUC: {bayes_roc_auc:.4f}\")\n",
    "\n",
    "# Save Metrics to Excel\n",
    "metrics_data_bayes = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [bayes_accuracy, bayes_precision, bayes_recall, bayes_f1, bayes_roc_auc]\n",
    "}\n",
    "metrics_df_bayes = pd.DataFrame(metrics_data_bayes)\n",
    "metrics_file_bayes = os.path.join(output_dir_bayes, 'bayes_metrics_results.xlsx')\n",
    "metrics_df_bayes.to_excel(metrics_file_bayes, index=False)\n",
    "print(f\"\\nMetrics saved to '{metrics_file_bayes}'.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_bayes = confusion_matrix(y_test, y_test_pred_bayes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_bayes, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix - Bayesian Classifier')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "conf_matrix_file_bayes = os.path.join(output_dir_bayes, 'confusion_matrix_bayes.png')\n",
    "plt.savefig(conf_matrix_file_bayes)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_bayes = {}\n",
    "tpr_bayes = {}\n",
    "roc_auc_bayes = {}\n",
    "classes_bayes = pd.get_dummies(y_test).columns\n",
    "\n",
    "for i, class_label in enumerate(classes_bayes):\n",
    "    fpr_bayes[class_label], tpr_bayes[class_label], _ = roc_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_bayes[:, i])\n",
    "    roc_auc_bayes[class_label] = roc_auc_score(pd.get_dummies(y_test).iloc[:, i], y_test_proba_bayes[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_bayes:\n",
    "    plt.plot(fpr_bayes[class_label], tpr_bayes[class_label], label=f'Class {class_label} (AUC = {roc_auc_bayes[class_label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve - Bayesian Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "roc_curve_file_bayes = os.path.join(output_dir_bayes, 'roc_curve_bayes.png')\n",
    "plt.savefig(roc_curve_file_bayes)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_bayes = {}\n",
    "recall_bayes = {}\n",
    "\n",
    "for i, class_label in enumerate(classes_bayes):\n",
    "    precision_bayes[class_label], recall_bayes[class_label], _ = precision_recall_curve(pd.get_dummies(y_test).iloc[:, i], y_test_proba_bayes[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in classes_bayes:\n",
    "    plt.plot(recall_bayes[class_label], precision_bayes[class_label], label=f'Class {class_label}')\n",
    "plt.title('Precision-Recall Curve - Bayesian Classifier')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "pr_curve_file_bayes = os.path.join(output_dir_bayes, 'precision_recall_curve_bayes.png')\n",
    "plt.savefig(pr_curve_file_bayes)\n",
    "plt.show()\n",
    "\n",
    "print(f\"All plots and metrics have been saved to the directory: {output_dir_bayes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd40bdc-22d1-49ca-8d02-a8f4122b0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have X and y loaded in your environment\n",
    "# Example: X = data.drop('Label', axis=1); y = data['Label']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Create bagging classifier\n",
    "bagging_model = BaggingClassifier(estimator=base_estimator, n_estimators=50, random_state=1)\n",
    "\n",
    "# Train the model\n",
    "bagging_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = bagging_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Bagging Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17d43e-1f85-41eb-b28c-af62f63a86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the Extreme Learning Machine class\n",
    "class ExtremeLearningMachine:\n",
    "    def __init__(self, input_size, hidden_size, activation_function='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_function = activation_function\n",
    "        self.input_weights = np.random.normal(size=(self.input_size, self.hidden_size))\n",
    "        self.biases = np.random.normal(size=(self.hidden_size,))\n",
    "        self.output_weights = None\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose 'sigmoid', 'tanh', or 'relu'.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Compute hidden layer outputs\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "\n",
    "        # Compute output weights using Moore-Penrose pseudoinverse\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute hidden layer outputs\n",
    "        H = self._activation(np.dot(X, self.input_weights) + self.biases)\n",
    "\n",
    "        # Compute predictions\n",
    "        output = np.dot(H, self.output_weights)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "# Load your preprocessed dataset\n",
    "# Assuming 'data' is your preprocessed DataFrame\n",
    "X = data.drop('Label', axis=1).values\n",
    "y = data['Label'].values\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_one_hot = np.zeros((len(y), num_classes))\n",
    "y_one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the ELM\n",
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 100  # Number of hidden nodes (hyperparameter)\n",
    "elm = ExtremeLearningMachine(input_size=input_size, hidden_size=hidden_size, activation_function='relu')\n",
    "elm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = elm.predict(X_test_scaled)\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate accuracy\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662b907-8f3f-41c5-8525-c5e0129ac039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Random Forest results from the Excel file\n",
    "results_file = 'rf_model_results.xlsx'  # Update the file path if needed\n",
    "rf_results_df = pd.read_excel(results_file)\n",
    "\n",
    "# Plot 1: Validation Accuracy vs. Number of Estimators\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=rf_results_df,\n",
    "    x='n_estimators',\n",
    "    y='Validation Accuracy',\n",
    "    hue='max_depth',\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Validation Accuracy for Different n_estimators and max_depth')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Max Depth')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Validation Accuracy vs. Max Depth\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=rf_results_df,\n",
    "    x='max_depth',\n",
    "    y='Validation Accuracy',\n",
    "    hue='criterion',\n",
    "    palette='plasma'\n",
    ")\n",
    "plt.title('Validation Accuracy by Max Depth and Criterion')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Criterion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Validation Accuracy vs. Criterion\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=rf_results_df,\n",
    "    x='criterion',\n",
    "    y='Validation Accuracy',\n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Validation Accuracy Distribution by Criterion')\n",
    "plt.xlabel('Criterion')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Overall Mean Validation Accuracy Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=rf_results_df,\n",
    "    x='max_depth',\n",
    "    y='Validation Accuracy',\n",
    "    hue='n_estimators',\n",
    "    palette='Set2'\n",
    ")\n",
    "plt.title('Validation Accuracy Distribution by Max Depth and n_estimators')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Number of Estimators')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e0dad-fe43-4858-a270-fed4ffe74830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Random Forest results from the Excel file\n",
    "results_file = 'rf_model_results.xlsx'  # Update the file path if needed\n",
    "rf_results_df = pd.read_excel(results_file)\n",
    "\n",
    "# Plot 1: Validation Accuracy vs. Number of Estimators for Different Max Depths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=rf_results_df,\n",
    "    x='n_estimators',\n",
    "    y='Validation Accuracy',\n",
    "    hue='max_depth',\n",
    "    style='criterion',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Validation Accuracy vs. Number of Estimators for Different Max Depths')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Max Depth / Criterion', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Validation Accuracy vs. Max Depth for Different Criteria\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=rf_results_df,\n",
    "    x='max_depth',\n",
    "    y='Validation Accuracy',\n",
    "    hue='criterion',\n",
    "    style='n_estimators',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette='plasma'\n",
    ")\n",
    "plt.title('Validation Accuracy vs. Max Depth for Different Criteria')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Criterion / Estimators', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Validation Accuracy vs. Criterion for Different Estimators and Max Depth\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=rf_results_df,\n",
    "    x='criterion',\n",
    "    y='Validation Accuracy',\n",
    "    hue='n_estimators',\n",
    "    style='max_depth',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Validation Accuracy vs. Criterion for Different Estimators and Max Depth')\n",
    "plt.xlabel('Criterion')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Estimators / Max Depth', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde100ef-5f1f-4460-8c23-918690b37e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['Label'])  # Use all columns except 'Label'\n",
    "y = data['Label']\n",
    "\n",
    "# Map string labels to integers if needed\n",
    "if y.dtype == 'object':\n",
    "    label_mapping = {label: idx for idx, label in enumerate(y.unique())}\n",
    "    y = y.map(label_mapping)\n",
    "\n",
    "# Flatten columns with sequences by extracting statistical features\n",
    "def flatten_sequences(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].apply(lambda x: isinstance(x, (list, np.ndarray))).any():\n",
    "            df[column + '_mean'] = df[column].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "            df[column + '_std'] = df[column].apply(lambda x: np.std(x) if isinstance(x, (list, np.ndarray)) else 0)\n",
    "            df[column + '_median'] = df[column].apply(lambda x: np.median(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "# Apply the flattening function to the features\n",
    "flatten_sequences(X)\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X = X.values\n",
    "\n",
    "# Split the data into 70% training, 20% validation, and 10% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2222, random_state=1  # 0.2222 of 90% to get 20% validation\n",
    ")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM model with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', random_state=1)\n",
    "\n",
    "# Define hyperparameter ranges for grid search\n",
    "param_grid = {\n",
    "    'C': [ 1, 10, 100],  # Regularization parameter\n",
    "    'gamma': [0.001, 0.01, 0.1]  # RBF kernel coefficient\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation on training + validation sets\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters and validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_val_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"\\nBest Hyperparameters found:\")\n",
    "print(f\"C: {best_params['C']}, Gamma: {best_params['gamma']}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the final model with the best hyperparameters on combined training + validation data\n",
    "final_svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=best_params['C'],\n",
    "    gamma=best_params['gamma'],\n",
    "    random_state=1\n",
    ")\n",
    "final_svm_model.fit(scaler.transform(X_train_val), y_train_val)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_test_pred = final_svm_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest Accuracy with the best configuration: {test_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch + GPU)",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
